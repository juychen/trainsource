{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "#import scipy.io as sio\n",
    "\n",
    "from models import VAE,AEBase\n",
    "from models import DNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import models\n",
    "import utils as ut\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "epochs = 500 #200,500,1000  \n",
    "#dim_au_in = 20049\n",
    "dim_au_out = 512 #8, 16, 32, 64, 128, 256,512\n",
    "dim_dnn_in = dim_au_out\n",
    "dim_dnn_out=1\n",
    "select_drug = 'Gefitinib'\n",
    "na = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r=pd.read_csv('data/GDSCexpression.csv',index_col=0)\n",
    "label_r=pd.read_csv('data/GDSClabel.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_r=label_r.fillna(na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
     ]
    }
   ],
   "source": [
    "hvg,adata = ut.highly_variable_genes(data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Erlotinib</th>\n",
       "      <th>AICAR</th>\n",
       "      <th>Camptothecin</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Cisplatin</th>\n",
       "      <th>Cytarabine</th>\n",
       "      <th>Docetaxel</th>\n",
       "      <th>Methotrexate</th>\n",
       "      <th>ATRA</th>\n",
       "      <th>Gefitinib</th>\n",
       "      <th>...</th>\n",
       "      <th>CMK</th>\n",
       "      <th>Pyrimethamine</th>\n",
       "      <th>JW-7-52-1</th>\n",
       "      <th>A-443654</th>\n",
       "      <th>GW843682X</th>\n",
       "      <th>MS-275</th>\n",
       "      <th>Parthenolide</th>\n",
       "      <th>MG-132</th>\n",
       "      <th>KIN001-135</th>\n",
       "      <th>TGX221</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BxPC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.159708</td>\n",
       "      <td>0.188393</td>\n",
       "      <td>0.436127</td>\n",
       "      <td>0.154274</td>\n",
       "      <td>0.113764</td>\n",
       "      <td>0.249987</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMOE-2</th>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.318206</td>\n",
       "      <td>0.281740</td>\n",
       "      <td>0.063808</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.101530</td>\n",
       "      <td>0.191210</td>\n",
       "      <td>0.031229</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022662</td>\n",
       "      <td>0.025217</td>\n",
       "      <td>0.270791</td>\n",
       "      <td>0.095608</td>\n",
       "      <td>0.139914</td>\n",
       "      <td>0.171747</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFM-223</th>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.334087</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.361438</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>0.186659</td>\n",
       "      <td>0.228797</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUGC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.416792</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.266409</td>\n",
       "      <td>0.237811</td>\n",
       "      <td>0.234952</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>0.153002</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OC-314</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.479915</td>\n",
       "      <td>0.390145</td>\n",
       "      <td>0.067738</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>0.110418</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.020309</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KP-N-S19s</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC-3</th>\n",
       "      <td>0.003515</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048227</td>\n",
       "      <td>0.146476</td>\n",
       "      <td>0.400087</td>\n",
       "      <td>0.205247</td>\n",
       "      <td>0.147090</td>\n",
       "      <td>0.298803</td>\n",
       "      <td>0.023796</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.069181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Panc 08.13</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.180653</td>\n",
       "      <td>0.090963</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EKVX</th>\n",
       "      <td>0.177238</td>\n",
       "      <td>0.078395</td>\n",
       "      <td>0.078938</td>\n",
       "      <td>0.072213</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>0.095642</td>\n",
       "      <td>0.111598</td>\n",
       "      <td>0.126364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.243635</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.177759</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>0.007216</td>\n",
       "      <td>0.063911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMS-114</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.127677</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.281940</td>\n",
       "      <td>0.092074</td>\n",
       "      <td>0.191972</td>\n",
       "      <td>0.185916</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.043920</td>\n",
       "      <td>0.285969</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.174841</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.080451</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.028733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Erlotinib     AICAR  Camptothecin  Vinblastine  Cisplatin  \\\n",
       "BxPC-3      -1.000000  0.159708      0.188393     0.436127   0.154274   \n",
       "KMOE-2       0.002108  0.339814      0.318206     0.281740   0.063808   \n",
       "MFM-223      0.002148  0.334087      0.122671     0.361438   0.051983   \n",
       "NUGC-3      -1.000000  0.095099      0.416792     0.422631   0.266409   \n",
       "OC-314      -1.000000  0.012740      0.479915     0.390145   0.067738   \n",
       "...               ...       ...           ...          ...        ...   \n",
       "KP-N-S19s   -1.000000 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "BC-3         0.003515 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "Panc 08.13  -1.000000  0.152818      0.196279     0.180653   0.090963   \n",
       "EKVX         0.177238  0.078395      0.078938     0.072213   0.007225   \n",
       "DMS-114      0.000322  0.127677      0.168317     0.281940   0.092074   \n",
       "\n",
       "            Cytarabine  Docetaxel  Methotrexate      ATRA  Gefitinib  ...  \\\n",
       "BxPC-3        0.113764   0.249987      0.015324  0.015324   0.020293  ...   \n",
       "KMOE-2        0.140681   0.101530      0.191210  0.031229   0.007419  ...   \n",
       "MFM-223       0.186659   0.228797      0.003639  0.023043   0.003639  ...   \n",
       "NUGC-3        0.237811   0.234952      0.011558  0.028838   0.153002  ...   \n",
       "OC-314        0.125869   0.249668      0.110418  0.002214   0.020309  ...   \n",
       "...                ...        ...           ...       ...        ...  ...   \n",
       "KP-N-S19s    -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "BC-3         -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "Panc 08.13    0.093197   0.097267      0.018269  0.010237   0.009405  ...   \n",
       "EKVX          0.003470   0.129184      0.095642  0.111598   0.126364  ...   \n",
       "DMS-114       0.191972   0.185916      0.007135  0.009329   0.020822  ...   \n",
       "\n",
       "                 CMK  Pyrimethamine  JW-7-52-1  A-443654  GW843682X    MS-275  \\\n",
       "BxPC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "KMOE-2      0.022662       0.025217   0.270791  0.095608   0.139914  0.171747   \n",
       "MFM-223     0.038997       0.006167   0.002185  0.102592   0.020338  0.010874   \n",
       "NUGC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "OC-314     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "...              ...            ...        ...       ...        ...       ...   \n",
       "KP-N-S19s  -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "BC-3        0.048227       0.146476   0.400087  0.205247   0.147090  0.298803   \n",
       "Panc 08.13 -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "EKVX        0.020472       0.044047   0.243635  0.079375   0.019859  0.177759   \n",
       "DMS-114     0.002137       0.043920   0.285969  0.075195   0.174841  0.008383   \n",
       "\n",
       "            Parthenolide    MG-132  KIN001-135    TGX221  \n",
       "BxPC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "KMOE-2          0.026670  0.013904    0.002116  0.000956  \n",
       "MFM-223         0.000970  0.035206    0.007051  0.000323  \n",
       "NUGC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "OC-314         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "...                  ...       ...         ...       ...  \n",
       "KP-N-S19s      -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "BC-3            0.023796  0.017219    0.002065  0.069181  \n",
       "Panc 08.13     -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "EKVX            0.014843  0.114431    0.007216  0.063911  \n",
       "DMS-114         0.002135  0.080451    0.002135  0.028733  \n",
       "\n",
       "[789 rows x 139 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = label_r.loc[:,select_drug]!=na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r.columns = adata.var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your is gene-cell, mine is cell-gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_r.loc[selected_idx,hvg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_r.loc[selected_idx,select_drug]\n",
    "#sscaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "mmscaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "data = mmscaler.fit_transform(data)\n",
    "#data = sscaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25383929199289434\n",
      "0.2727100504700379\n"
     ]
    }
   ],
   "source": [
    "print(np.std(data))\n",
    "print(np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49458949, 0.09860014, 0.21159545, ..., 0.21276445, 0.22738253,\n",
       "       0.27747885])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000004\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(data.max())\n",
    "print(data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 3462)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 139)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_all, X_test, Y_train_all, Y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_all, Y_train_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 3462)\n",
      "(675,)\n",
      "(432, 3462) (432,)\n",
      "(135, 3462) (135,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all data to AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainTensor = torch.FloatTensor(X_train).to(device)\n",
    "X_validTensor = torch.FloatTensor(X_valid).to(device)\n",
    "X_testTensor = torch.FloatTensor(X_test).to(device)\n",
    "X_allTensor = torch.FloatTensor(data).to(device)\n",
    "#X_alltrainTensor = torch.FloatTensor(X_train_all).to(device)\n",
    "\n",
    "\n",
    "Y_trainTensor = torch.FloatTensor(Y_train.values).to(device)\n",
    "Y_validTensor = torch.FloatTensor(Y_valid.values).to(device)\n",
    "\n",
    "# construct TensorDataset\n",
    "train_dataset = TensorDataset(X_trainTensor, X_trainTensor)\n",
    "valid_dataset = TensorDataset(X_validTensor, X_validTensor)\n",
    "test_dataset = TensorDataset(X_testTensor, X_testTensor)\n",
    "all_dataset = TensorDataset(X_allTensor, X_allTensor)\n",
    "\n",
    "X_trainDataLoader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "X_validDataLoader = DataLoader(dataset=valid_dataset, batch_size=200, shuffle=True)\n",
    "X_allDataLoader = DataLoader(dataset=all_dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = X_trainDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_train = {'train':X_trainDataLoader,'val':X_validDataLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainDataLoader.dataset.tensors[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 432, 'val': 108}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{x: dataloaders_train[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEBase(input_dim=data.shape[1],latent_dim=512,hidden_dims=[1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEBase(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=3462, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (decoder_input): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=3462, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VAE(dim_au_in=data_r.shape[1],dim_au_out=128)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae_model(net,data_loaders,optimizer,loss_function,n_epochs,scheduler):\n",
    "    \n",
    "    dataset_sizes = {x: data_loaders[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}\n",
    "    loss_train = {}\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #optimizer = scheduler(optimizer, epoch)\n",
    "                net.train()  # Set model to training mode\n",
    "            else:\n",
    "                net.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # for data in data_loaders[phase]:\n",
    "            for batchidx, (x, _) in enumerate(data_loaders[phase]):\n",
    "\n",
    "                x.requires_grad_(True)\n",
    "                # encode and decode \n",
    "                output = model(x)\n",
    "                # compute loss\n",
    "                loss = loss_function(output, x)      \n",
    "\n",
    "                # zero the parameter (weight) gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    # update the weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                # print loss statistics\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Schedular\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            \n",
    "            loss_train[epoch,phase] = epoch_loss \n",
    "            print('{} Loss: {:.8f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Select best model wts\n",
    "    torch.save(best_model_wts, 'saved/models/ae.pkl')\n",
    "    net.load_state_dict(best_model_wts)           \n",
    "    \n",
    "    return net, loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00093345\n",
      "val Loss: 0.00101526\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00081894\n",
      "val Loss: 0.00095026\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00078748\n",
      "val Loss: 0.00098939\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00074839\n",
      "val Loss: 0.00098055\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00073421\n",
      "val Loss: 0.00097101\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00072094\n",
      "val Loss: 0.00096392\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00069258\n",
      "val Loss: 0.00090361\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00069430\n",
      "val Loss: 0.00087363\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00067177\n",
      "val Loss: 0.00086412\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00065384\n",
      "val Loss: 0.00079331\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00063410\n",
      "val Loss: 0.00077501\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00064146\n",
      "val Loss: 0.00076661\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00061702\n",
      "val Loss: 0.00076201\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00062349\n",
      "val Loss: 0.00074969\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00062530\n",
      "val Loss: 0.00074164\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00060766\n",
      "val Loss: 0.00073837\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00060189\n",
      "val Loss: 0.00073543\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00059664\n",
      "val Loss: 0.00073446\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00059215\n",
      "val Loss: 0.00073342\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00062773\n",
      "val Loss: 0.00073091\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00059213\n",
      "val Loss: 0.00072699\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00059692\n",
      "val Loss: 0.00072183\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00058221\n",
      "val Loss: 0.00071883\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00059270\n",
      "val Loss: 0.00071488\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00059204\n",
      "val Loss: 0.00071591\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00060083\n",
      "val Loss: 0.00071530\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00059297\n",
      "val Loss: 0.00071106\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00059668\n",
      "val Loss: 0.00070680\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00059167\n",
      "val Loss: 0.00070480\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00057426\n",
      "val Loss: 0.00070499\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00058532\n",
      "val Loss: 0.00070416\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00062038\n",
      "val Loss: 0.00070606\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00060040\n",
      "val Loss: 0.00071024\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00059576\n",
      "val Loss: 0.00070882\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00058485\n",
      "val Loss: 0.00070856\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00059150\n",
      "val Loss: 0.00070659\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00059432\n",
      "val Loss: 0.00070536\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00057967\n",
      "val Loss: 0.00070706\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00057783\n",
      "val Loss: 0.00070936\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00058731\n",
      "val Loss: 0.00070819\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00058151\n",
      "val Loss: 0.00070730\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00058385\n",
      "val Loss: 0.00070687\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00060180\n",
      "val Loss: 0.00070535\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00060325\n",
      "val Loss: 0.00070524\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00060306\n",
      "val Loss: 0.00070389\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.00058505\n",
      "val Loss: 0.00070283\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.00059184\n",
      "val Loss: 0.00070319\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.00059533\n",
      "val Loss: 0.00070219\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.00059033\n",
      "val Loss: 0.00070279\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.00058721\n",
      "val Loss: 0.00070502\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.00058348\n",
      "val Loss: 0.00070522\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.00058178\n",
      "val Loss: 0.00070662\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.00058009\n",
      "val Loss: 0.00070896\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.00060641\n",
      "val Loss: 0.00070324\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.00059769\n",
      "val Loss: 0.00070668\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.00060345\n",
      "val Loss: 0.00070942\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.00060611\n",
      "val Loss: 0.00070487\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.00059928\n",
      "val Loss: 0.00070364\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.00060134\n",
      "val Loss: 0.00070237\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.00059164\n",
      "val Loss: 0.00070633\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.00059985\n",
      "val Loss: 0.00070482\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.00059274\n",
      "val Loss: 0.00070796\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.00060278\n",
      "val Loss: 0.00070522\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.00058204\n",
      "val Loss: 0.00070894\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.00067725\n",
      "val Loss: 0.00071191\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.00058676\n",
      "val Loss: 0.00070977\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.00059351\n",
      "val Loss: 0.00070842\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.00059838\n",
      "val Loss: 0.00070848\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.00058788\n",
      "val Loss: 0.00070626\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.00059194\n",
      "val Loss: 0.00070867\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.00058283\n",
      "val Loss: 0.00070775\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.00057401\n",
      "val Loss: 0.00070818\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.00059254\n",
      "val Loss: 0.00070617\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.00057786\n",
      "val Loss: 0.00070556\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.00058765\n",
      "val Loss: 0.00070507\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.00060555\n",
      "val Loss: 0.00070830\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.00058167\n",
      "val Loss: 0.00070758\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.00059113\n",
      "val Loss: 0.00070587\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.00058445\n",
      "val Loss: 0.00070609\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.00059175\n",
      "val Loss: 0.00070500\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.00058584\n",
      "val Loss: 0.00070943\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.00058751\n",
      "val Loss: 0.00071020\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.00059030\n",
      "val Loss: 0.00071101\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.00059934\n",
      "val Loss: 0.00070473\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.00060879\n",
      "val Loss: 0.00070176\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.00060264\n",
      "val Loss: 0.00070418\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.00059359\n",
      "val Loss: 0.00070560\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.00059040\n",
      "val Loss: 0.00070561\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.00059015\n",
      "val Loss: 0.00070685\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.00058211\n",
      "val Loss: 0.00070732\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.00058501\n",
      "val Loss: 0.00070724\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.00058910\n",
      "val Loss: 0.00070866\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.00058572\n",
      "val Loss: 0.00070988\n",
      "Epoch 93/499\n",
      "----------\n",
      "train Loss: 0.00058592\n",
      "val Loss: 0.00071121\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.00060208\n",
      "val Loss: 0.00071346\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.00059872\n",
      "val Loss: 0.00071346\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.00058684\n",
      "val Loss: 0.00071108\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.00060735\n",
      "val Loss: 0.00070832\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.00057914\n",
      "val Loss: 0.00070802\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.00058484\n",
      "val Loss: 0.00070796\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.00059297\n",
      "val Loss: 0.00070555\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.00060018\n",
      "val Loss: 0.00070656\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.00057637\n",
      "val Loss: 0.00070706\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.00058606\n",
      "val Loss: 0.00070985\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.00058442\n",
      "val Loss: 0.00070880\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.00060303\n",
      "val Loss: 0.00070496\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.00059200\n",
      "val Loss: 0.00070464\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.00060766\n",
      "val Loss: 0.00070365\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.00058890\n",
      "val Loss: 0.00070691\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.00059253\n",
      "val Loss: 0.00070548\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.00058197\n",
      "val Loss: 0.00070569\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.00059603\n",
      "val Loss: 0.00070561\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.00058806\n",
      "val Loss: 0.00070476\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.00059028\n",
      "val Loss: 0.00070294\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.00057719\n",
      "val Loss: 0.00070460\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.00058024\n",
      "val Loss: 0.00070350\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.00059325\n",
      "val Loss: 0.00070356\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.00059497\n",
      "val Loss: 0.00070546\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.00059339\n",
      "val Loss: 0.00070653\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.00059705\n",
      "val Loss: 0.00070526\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.00059606\n",
      "val Loss: 0.00070573\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.00058977\n",
      "val Loss: 0.00070537\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.00059242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00070478\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.00060365\n",
      "val Loss: 0.00070699\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.00058961\n",
      "val Loss: 0.00070755\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.00058099\n",
      "val Loss: 0.00070616\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.00060518\n",
      "val Loss: 0.00070335\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.00059305\n",
      "val Loss: 0.00070476\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.00058404\n",
      "val Loss: 0.00070520\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.00058715\n",
      "val Loss: 0.00070457\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.00059303\n",
      "val Loss: 0.00070737\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.00060113\n",
      "val Loss: 0.00071190\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.00058828\n",
      "val Loss: 0.00071411\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.00058450\n",
      "val Loss: 0.00071299\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.00059531\n",
      "val Loss: 0.00071368\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.00058313\n",
      "val Loss: 0.00071255\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.00058448\n",
      "val Loss: 0.00071030\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.00060228\n",
      "val Loss: 0.00070374\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.00058166\n",
      "val Loss: 0.00070650\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.00058099\n",
      "val Loss: 0.00070815\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.00058614\n",
      "val Loss: 0.00070387\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.00057047\n",
      "val Loss: 0.00070467\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.00061171\n",
      "val Loss: 0.00070737\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.00058915\n",
      "val Loss: 0.00070797\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.00059370\n",
      "val Loss: 0.00070498\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.00059615\n",
      "val Loss: 0.00070612\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.00057777\n",
      "val Loss: 0.00070663\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.00059296\n",
      "val Loss: 0.00070958\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.00058668\n",
      "val Loss: 0.00070819\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.00059628\n",
      "val Loss: 0.00070865\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.00060287\n",
      "val Loss: 0.00070349\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.00058613\n",
      "val Loss: 0.00070449\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.00058303\n",
      "val Loss: 0.00070520\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.00058935\n",
      "val Loss: 0.00070169\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.00059305\n",
      "val Loss: 0.00070440\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.00058252\n",
      "val Loss: 0.00070552\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.00059237\n",
      "val Loss: 0.00070660\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.00058371\n",
      "val Loss: 0.00070765\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.00059604\n",
      "val Loss: 0.00071124\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.00060091\n",
      "val Loss: 0.00070794\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.00057962\n",
      "val Loss: 0.00070661\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.00057807\n",
      "val Loss: 0.00070490\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.00058455\n",
      "val Loss: 0.00070439\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.00058577\n",
      "val Loss: 0.00070549\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.00057837\n",
      "val Loss: 0.00070791\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.00057500\n",
      "val Loss: 0.00070811\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.00058255\n",
      "val Loss: 0.00071044\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.00058495\n",
      "val Loss: 0.00070924\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.00058088\n",
      "val Loss: 0.00070934\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.00058480\n",
      "val Loss: 0.00071204\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.00059899\n",
      "val Loss: 0.00071050\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.00058626\n",
      "val Loss: 0.00071036\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.00059793\n",
      "val Loss: 0.00070753\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.00060797\n",
      "val Loss: 0.00070752\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.00061881\n",
      "val Loss: 0.00070882\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.00058791\n",
      "val Loss: 0.00070906\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.00057830\n",
      "val Loss: 0.00070858\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.00058568\n",
      "val Loss: 0.00070875\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.00059058\n",
      "val Loss: 0.00071215\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.00058514\n",
      "val Loss: 0.00071202\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.00058556\n",
      "val Loss: 0.00071123\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.00059913\n",
      "val Loss: 0.00071356\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.00058708\n",
      "val Loss: 0.00071108\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.00060790\n",
      "val Loss: 0.00070931\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.00059143\n",
      "val Loss: 0.00070520\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.00059697\n",
      "val Loss: 0.00070650\n",
      "Epoch 186/499\n",
      "----------\n",
      "train Loss: 0.00060184\n",
      "val Loss: 0.00070504\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.00058500\n",
      "val Loss: 0.00070540\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.00061157\n",
      "val Loss: 0.00070531\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.00059256\n",
      "val Loss: 0.00070776\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.00060648\n",
      "val Loss: 0.00071087\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.00058365\n",
      "val Loss: 0.00071010\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.00058709\n",
      "val Loss: 0.00070584\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.00058780\n",
      "val Loss: 0.00070475\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.00059817\n",
      "val Loss: 0.00070758\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.00059140\n",
      "val Loss: 0.00070719\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.00058551\n",
      "val Loss: 0.00070738\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.00059044\n",
      "val Loss: 0.00070850\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.00059996\n",
      "val Loss: 0.00070495\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.00059038\n",
      "val Loss: 0.00070713\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.00058451\n",
      "val Loss: 0.00070523\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.00064032\n",
      "val Loss: 0.00070844\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.00059093\n",
      "val Loss: 0.00070817\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.00061003\n",
      "val Loss: 0.00070885\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.00058969\n",
      "val Loss: 0.00071002\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.00058789\n",
      "val Loss: 0.00070986\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.00059819\n",
      "val Loss: 0.00070570\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.00057739\n",
      "val Loss: 0.00070777\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.00061133\n",
      "val Loss: 0.00071130\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.00059316\n",
      "val Loss: 0.00070686\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.00059800\n",
      "val Loss: 0.00070507\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.00060291\n",
      "val Loss: 0.00070675\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.00059222\n",
      "val Loss: 0.00070705\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.00060256\n",
      "val Loss: 0.00070884\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.00059183\n",
      "val Loss: 0.00070853\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.00058909\n",
      "val Loss: 0.00070967\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.00061381\n",
      "val Loss: 0.00071270\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.00058010\n",
      "val Loss: 0.00071324\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.00059393\n",
      "val Loss: 0.00071183\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.00059388\n",
      "val Loss: 0.00070963\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.00058969\n",
      "val Loss: 0.00071011\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.00060393\n",
      "val Loss: 0.00070701\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.00060261\n",
      "val Loss: 0.00071020\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.00060125\n",
      "val Loss: 0.00071208\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.00059673\n",
      "val Loss: 0.00071187\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.00058763\n",
      "val Loss: 0.00071275\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.00058095\n",
      "val Loss: 0.00071313\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.00060130\n",
      "val Loss: 0.00070795\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.00058336\n",
      "val Loss: 0.00070694\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.00060272\n",
      "val Loss: 0.00070179\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.00059064\n",
      "val Loss: 0.00070254\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.00058868\n",
      "val Loss: 0.00070412\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.00058613\n",
      "val Loss: 0.00070332\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.00057885\n",
      "val Loss: 0.00070079\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.00058108\n",
      "val Loss: 0.00070273\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.00059332\n",
      "val Loss: 0.00070377\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.00058124\n",
      "val Loss: 0.00070470\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.00059447\n",
      "val Loss: 0.00070504\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.00059259\n",
      "val Loss: 0.00070853\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.00068021\n",
      "val Loss: 0.00071024\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.00060530\n",
      "val Loss: 0.00070888\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.00057398\n",
      "val Loss: 0.00071008\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.00058577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00071194\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.00059383\n",
      "val Loss: 0.00071296\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.00058081\n",
      "val Loss: 0.00071345\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.00059004\n",
      "val Loss: 0.00070858\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.00059007\n",
      "val Loss: 0.00070542\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.00057824\n",
      "val Loss: 0.00070741\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.00062320\n",
      "val Loss: 0.00070949\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.00059346\n",
      "val Loss: 0.00070794\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.00059227\n",
      "val Loss: 0.00070621\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.00058749\n",
      "val Loss: 0.00070482\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.00059269\n",
      "val Loss: 0.00070243\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.00058037\n",
      "val Loss: 0.00070465\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.00059901\n",
      "val Loss: 0.00070789\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.00059047\n",
      "val Loss: 0.00070783\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.00059178\n",
      "val Loss: 0.00070504\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.00058553\n",
      "val Loss: 0.00070647\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.00061738\n",
      "val Loss: 0.00070927\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.00058732\n",
      "val Loss: 0.00071124\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.00058411\n",
      "val Loss: 0.00071195\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.00057707\n",
      "val Loss: 0.00071131\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.00059334\n",
      "val Loss: 0.00071156\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.00058466\n",
      "val Loss: 0.00071161\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.00060580\n",
      "val Loss: 0.00071018\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.00062975\n",
      "val Loss: 0.00071342\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.00058831\n",
      "val Loss: 0.00070761\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.00059058\n",
      "val Loss: 0.00070973\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.00060064\n",
      "val Loss: 0.00070813\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.00059222\n",
      "val Loss: 0.00070922\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.00059980\n",
      "val Loss: 0.00071148\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.00059208\n",
      "val Loss: 0.00071129\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.00060507\n",
      "val Loss: 0.00071238\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.00059536\n",
      "val Loss: 0.00071141\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.00060327\n",
      "val Loss: 0.00071394\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.00060477\n",
      "val Loss: 0.00070939\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.00059972\n",
      "val Loss: 0.00071091\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.00058856\n",
      "val Loss: 0.00070961\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.00060430\n",
      "val Loss: 0.00070390\n",
      "Epoch 279/499\n",
      "----------\n",
      "train Loss: 0.00059207\n",
      "val Loss: 0.00070363\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.00057709\n",
      "val Loss: 0.00070373\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.00057959\n",
      "val Loss: 0.00070706\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.00059349\n",
      "val Loss: 0.00070871\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.00057966\n",
      "val Loss: 0.00070774\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.00058905\n",
      "val Loss: 0.00070447\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.00058064\n",
      "val Loss: 0.00070453\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.00059431\n",
      "val Loss: 0.00070590\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.00058688\n",
      "val Loss: 0.00070506\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.00058768\n",
      "val Loss: 0.00070619\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.00057991\n",
      "val Loss: 0.00070475\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.00063729\n",
      "val Loss: 0.00070619\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.00059031\n",
      "val Loss: 0.00070656\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.00058401\n",
      "val Loss: 0.00070822\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.00058249\n",
      "val Loss: 0.00071047\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.00058521\n",
      "val Loss: 0.00070739\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.00058012\n",
      "val Loss: 0.00070870\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.00060150\n",
      "val Loss: 0.00071113\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.00059626\n",
      "val Loss: 0.00071353\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.00058139\n",
      "val Loss: 0.00071174\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.00059420\n",
      "val Loss: 0.00071642\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.00061961\n",
      "val Loss: 0.00071806\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.00058609\n",
      "val Loss: 0.00071276\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.00058342\n",
      "val Loss: 0.00071131\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.00058307\n",
      "val Loss: 0.00070948\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.00059428\n",
      "val Loss: 0.00071055\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.00059526\n",
      "val Loss: 0.00071066\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.00059642\n",
      "val Loss: 0.00070812\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.00058800\n",
      "val Loss: 0.00070738\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.00059526\n",
      "val Loss: 0.00070838\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.00059107\n",
      "val Loss: 0.00070570\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.00059064\n",
      "val Loss: 0.00070302\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.00059354\n",
      "val Loss: 0.00070362\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.00059752\n",
      "val Loss: 0.00070239\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.00060506\n",
      "val Loss: 0.00070104\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.00059327\n",
      "val Loss: 0.00070171\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.00058305\n",
      "val Loss: 0.00070278\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.00063900\n",
      "val Loss: 0.00070534\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.00059362\n",
      "val Loss: 0.00070801\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.00059079\n",
      "val Loss: 0.00070876\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.00060691\n",
      "val Loss: 0.00070516\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.00058813\n",
      "val Loss: 0.00070648\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.00058348\n",
      "val Loss: 0.00070697\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.00058615\n",
      "val Loss: 0.00070816\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.00058192\n",
      "val Loss: 0.00070842\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.00059076\n",
      "val Loss: 0.00070906\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.00060187\n",
      "val Loss: 0.00071163\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.00058796\n",
      "val Loss: 0.00070756\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.00058474\n",
      "val Loss: 0.00070807\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.00058254\n",
      "val Loss: 0.00070802\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.00059467\n",
      "val Loss: 0.00070386\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.00060198\n",
      "val Loss: 0.00070184\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.00058375\n",
      "val Loss: 0.00070396\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.00059148\n",
      "val Loss: 0.00070356\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.00058350\n",
      "val Loss: 0.00070595\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.00059999\n",
      "val Loss: 0.00070282\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.00060484\n",
      "val Loss: 0.00070561\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.00058774\n",
      "val Loss: 0.00070308\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.00058667\n",
      "val Loss: 0.00070416\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.00057657\n",
      "val Loss: 0.00070599\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.00059306\n",
      "val Loss: 0.00070750\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.00059550\n",
      "val Loss: 0.00070626\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.00058385\n",
      "val Loss: 0.00070572\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.00058548\n",
      "val Loss: 0.00070914\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.00062029\n",
      "val Loss: 0.00071274\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.00058994\n",
      "val Loss: 0.00071349\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.00057992\n",
      "val Loss: 0.00071285\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.00058543\n",
      "val Loss: 0.00071197\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.00058590\n",
      "val Loss: 0.00071136\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.00058340\n",
      "val Loss: 0.00071077\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.00059398\n",
      "val Loss: 0.00070980\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.00059793\n",
      "val Loss: 0.00071228\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.00059190\n",
      "val Loss: 0.00070962\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.00058613\n",
      "val Loss: 0.00070855\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.00059259\n",
      "val Loss: 0.00070750\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.00057944\n",
      "val Loss: 0.00070978\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.00060728\n",
      "val Loss: 0.00071290\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.00059088\n",
      "val Loss: 0.00071169\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.00058959\n",
      "val Loss: 0.00071163\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.00058499\n",
      "val Loss: 0.00070977\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.00060436\n",
      "val Loss: 0.00070561\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.00059099\n",
      "val Loss: 0.00070558\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.00060479\n",
      "val Loss: 0.00070550\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.00057987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00070407\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.00058685\n",
      "val Loss: 0.00070667\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.00057975\n",
      "val Loss: 0.00070820\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.00059295\n",
      "val Loss: 0.00070604\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.00058761\n",
      "val Loss: 0.00070720\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.00057838\n",
      "val Loss: 0.00070818\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.00058367\n",
      "val Loss: 0.00071065\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.00059331\n",
      "val Loss: 0.00071096\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.00059428\n",
      "val Loss: 0.00071023\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.00059161\n",
      "val Loss: 0.00071051\n",
      "Epoch 372/499\n",
      "----------\n",
      "train Loss: 0.00058301\n",
      "val Loss: 0.00070983\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.00058859\n",
      "val Loss: 0.00071152\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.00059696\n",
      "val Loss: 0.00070737\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.00058562\n",
      "val Loss: 0.00070820\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.00058400\n",
      "val Loss: 0.00070778\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.00060155\n",
      "val Loss: 0.00070530\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.00058935\n",
      "val Loss: 0.00070852\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.00059149\n",
      "val Loss: 0.00070651\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.00060268\n",
      "val Loss: 0.00070922\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.00058327\n",
      "val Loss: 0.00070737\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.00058189\n",
      "val Loss: 0.00070678\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.00060619\n",
      "val Loss: 0.00070974\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.00059333\n",
      "val Loss: 0.00070905\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.00058897\n",
      "val Loss: 0.00070780\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.00058314\n",
      "val Loss: 0.00070594\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.00058505\n",
      "val Loss: 0.00070920\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.00057836\n",
      "val Loss: 0.00071074\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.00059071\n",
      "val Loss: 0.00071056\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.00061540\n",
      "val Loss: 0.00071355\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.00059948\n",
      "val Loss: 0.00070704\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.00060616\n",
      "val Loss: 0.00070915\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.00058195\n",
      "val Loss: 0.00070757\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.00058861\n",
      "val Loss: 0.00070682\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.00058248\n",
      "val Loss: 0.00070958\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.00057501\n",
      "val Loss: 0.00070983\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.00058902\n",
      "val Loss: 0.00070913\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.00060839\n",
      "val Loss: 0.00070598\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.00058844\n",
      "val Loss: 0.00070590\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.00058499\n",
      "val Loss: 0.00070583\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.00058387\n",
      "val Loss: 0.00070401\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.00057470\n",
      "val Loss: 0.00070561\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.00061044\n",
      "val Loss: 0.00070169\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.00059024\n",
      "val Loss: 0.00070450\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.00060271\n",
      "val Loss: 0.00070711\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.00059374\n",
      "val Loss: 0.00070919\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.00059016\n",
      "val Loss: 0.00070644\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.00062152\n",
      "val Loss: 0.00070968\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.00059847\n",
      "val Loss: 0.00071268\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.00059152\n",
      "val Loss: 0.00070777\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.00058496\n",
      "val Loss: 0.00070716\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.00057838\n",
      "val Loss: 0.00070522\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.00058185\n",
      "val Loss: 0.00070622\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.00059758\n",
      "val Loss: 0.00070893\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.00060129\n",
      "val Loss: 0.00070852\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.00059004\n",
      "val Loss: 0.00070930\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.00059053\n",
      "val Loss: 0.00070569\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.00057914\n",
      "val Loss: 0.00070658\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.00060089\n",
      "val Loss: 0.00070915\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.00059313\n",
      "val Loss: 0.00070714\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.00059183\n",
      "val Loss: 0.00070645\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.00058484\n",
      "val Loss: 0.00070866\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.00059178\n",
      "val Loss: 0.00071118\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.00057545\n",
      "val Loss: 0.00071067\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.00058880\n",
      "val Loss: 0.00071022\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.00059068\n",
      "val Loss: 0.00070954\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.00058731\n",
      "val Loss: 0.00071156\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.00058500\n",
      "val Loss: 0.00070868\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.00058601\n",
      "val Loss: 0.00070958\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.00059273\n",
      "val Loss: 0.00071072\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.00057633\n",
      "val Loss: 0.00071025\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.00058899\n",
      "val Loss: 0.00070619\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.00059872\n",
      "val Loss: 0.00070668\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.00059012\n",
      "val Loss: 0.00070650\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.00059317\n",
      "val Loss: 0.00070936\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.00058529\n",
      "val Loss: 0.00070909\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.00060678\n",
      "val Loss: 0.00070657\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.00059448\n",
      "val Loss: 0.00070647\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.00060263\n",
      "val Loss: 0.00070144\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.00058524\n",
      "val Loss: 0.00070442\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.00058914\n",
      "val Loss: 0.00070519\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.00058677\n",
      "val Loss: 0.00070486\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.00059052\n",
      "val Loss: 0.00070181\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.00058792\n",
      "val Loss: 0.00070362\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.00058692\n",
      "val Loss: 0.00070639\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.00060104\n",
      "val Loss: 0.00070451\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.00060734\n",
      "val Loss: 0.00070322\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.00059322\n",
      "val Loss: 0.00070684\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.00058901\n",
      "val Loss: 0.00070734\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.00059517\n",
      "val Loss: 0.00070887\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.00059194\n",
      "val Loss: 0.00070957\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.00058518\n",
      "val Loss: 0.00070673\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.00058925\n",
      "val Loss: 0.00070791\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.00059486\n",
      "val Loss: 0.00070568\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.00059352\n",
      "val Loss: 0.00070790\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.00061388\n",
      "val Loss: 0.00071066\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.00059374\n",
      "val Loss: 0.00071078\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.00059011\n",
      "val Loss: 0.00070870\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.00059961\n",
      "val Loss: 0.00070997\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.00060493\n",
      "val Loss: 0.00070975\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.00062074\n",
      "val Loss: 0.00071287\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.00059472\n",
      "val Loss: 0.00071386\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.00059166\n",
      "val Loss: 0.00071227\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.00061316\n",
      "val Loss: 0.00071441\n",
      "Epoch 465/499\n",
      "----------\n",
      "train Loss: 0.00058228\n",
      "val Loss: 0.00071603\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.00060848\n",
      "val Loss: 0.00071154\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.00058985\n",
      "val Loss: 0.00070967\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.00059030\n",
      "val Loss: 0.00070550\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.00058522\n",
      "val Loss: 0.00070760\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.00058255\n",
      "val Loss: 0.00070974\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.00058444\n",
      "val Loss: 0.00070782\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.00057652\n",
      "val Loss: 0.00070674\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.00060508\n",
      "val Loss: 0.00070776\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.00058611\n",
      "val Loss: 0.00070852\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.00059839\n",
      "val Loss: 0.00070864\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.00058293\n",
      "val Loss: 0.00070718\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.00059169\n",
      "val Loss: 0.00070753\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.00059510\n",
      "val Loss: 0.00071035\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.00061642\n",
      "val Loss: 0.00070344\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.00059972\n",
      "val Loss: 0.00070570\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.00062997\n",
      "val Loss: 0.00070872\n",
      "Epoch 482/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00058128\n",
      "val Loss: 0.00070745\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.00059847\n",
      "val Loss: 0.00070535\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.00059225\n",
      "val Loss: 0.00070842\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.00065216\n",
      "val Loss: 0.00071298\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.00058625\n",
      "val Loss: 0.00071058\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.00058650\n",
      "val Loss: 0.00071043\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.00058716\n",
      "val Loss: 0.00071377\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.00058746\n",
      "val Loss: 0.00070979\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.00058763\n",
      "val Loss: 0.00070820\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.00058441\n",
      "val Loss: 0.00070868\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.00060279\n",
      "val Loss: 0.00070829\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.00059013\n",
      "val Loss: 0.00070675\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.00059097\n",
      "val Loss: 0.00070769\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.00059380\n",
      "val Loss: 0.00071045\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.00059101\n",
      "val Loss: 0.00070967\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.00059565\n",
      "val Loss: 0.00070780\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.00058951\n",
      "val Loss: 0.00070958\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.00059203\n",
      "val Loss: 0.00070750\n"
     ]
    }
   ],
   "source": [
    "model,loss_report = train_ae_model(net=model,data_loaders=dataloaders_train,\n",
    "                             optimizer=optimizer,loss_function=loss_function,\n",
    "                            n_epochs=epochs,scheduler=exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'train'): 0.0009334548204033463,\n",
       " (0, 'val'): 0.001015255641606119,\n",
       " (1, 'train'): 0.0008189371373090479,\n",
       " (1, 'val'): 0.0009502593289922785,\n",
       " (2, 'train'): 0.0007874847700198492,\n",
       " (2, 'val'): 0.0009893872257735995,\n",
       " (3, 'train'): 0.0007483921772627919,\n",
       " (3, 'val'): 0.0009805488365667839,\n",
       " (4, 'train'): 0.0007342083069185416,\n",
       " (4, 'val'): 0.0009710087820335671,\n",
       " (5, 'train'): 0.0007209400473921387,\n",
       " (5, 'val'): 0.0009639176919504449,\n",
       " (6, 'train'): 0.0006925836895351056,\n",
       " (6, 'val'): 0.0009036053799920612,\n",
       " (7, 'train'): 0.0006942994616649769,\n",
       " (7, 'val'): 0.000873628106934053,\n",
       " (8, 'train'): 0.0006717746490957561,\n",
       " (8, 'val'): 0.0008641194414209436,\n",
       " (9, 'train'): 0.0006538415500135334,\n",
       " (9, 'val'): 0.0007933125037837912,\n",
       " (10, 'train'): 0.0006340999087249791,\n",
       " (10, 'val'): 0.0007750088417971576,\n",
       " (11, 'train'): 0.0006414561325477229,\n",
       " (11, 'val'): 0.000766613969096431,\n",
       " (12, 'train'): 0.000617018141956241,\n",
       " (12, 'val'): 0.0007620117868538256,\n",
       " (13, 'train'): 0.000623485297654514,\n",
       " (13, 'val'): 0.0007496919069025251,\n",
       " (14, 'train'): 0.000625300221145153,\n",
       " (14, 'val'): 0.0007416432102521261,\n",
       " (15, 'train'): 0.0006076569530974935,\n",
       " (15, 'val'): 0.0007383719914489322,\n",
       " (16, 'train'): 0.0006018868406061773,\n",
       " (16, 'val'): 0.0007354338411931638,\n",
       " (17, 'train'): 0.000596640338362367,\n",
       " (17, 'val'): 0.0007344646448338473,\n",
       " (18, 'train'): 0.0005921470518741342,\n",
       " (18, 'val'): 0.0007334169414308337,\n",
       " (19, 'train'): 0.0006277342671873393,\n",
       " (19, 'val'): 0.0007309094071388245,\n",
       " (20, 'train'): 0.0005921326163742277,\n",
       " (20, 'val'): 0.0007269942650088558,\n",
       " (21, 'train'): 0.0005969162168050254,\n",
       " (21, 'val'): 0.0007218319784711908,\n",
       " (22, 'train'): 0.0005822054593375435,\n",
       " (22, 'val'): 0.0007188304982803486,\n",
       " (23, 'train'): 0.0005927029307241793,\n",
       " (23, 'val'): 0.0007148817595508364,\n",
       " (24, 'train'): 0.0005920375352380452,\n",
       " (24, 'val'): 0.0007159062143829134,\n",
       " (25, 'train'): 0.0006008292893292728,\n",
       " (25, 'val'): 0.0007152978192876886,\n",
       " (26, 'train'): 0.0005929746527086805,\n",
       " (26, 'val'): 0.0007110574731120357,\n",
       " (27, 'train'): 0.0005966841795102313,\n",
       " (27, 'val'): 0.0007067985694717478,\n",
       " (28, 'train'): 0.0005916682658372102,\n",
       " (28, 'val'): 0.0007047982955420459,\n",
       " (29, 'train'): 0.0005742622090986481,\n",
       " (29, 'val'): 0.0007049870435838346,\n",
       " (30, 'train'): 0.0005853224062809238,\n",
       " (30, 'val'): 0.0007041570627027088,\n",
       " (31, 'train'): 0.0006203761289793032,\n",
       " (31, 'val'): 0.0007060565468337801,\n",
       " (32, 'train'): 0.0006003973108750803,\n",
       " (32, 'val'): 0.0007102360466012249,\n",
       " (33, 'train'): 0.0005957562545383418,\n",
       " (33, 'val'): 0.0007088198843929502,\n",
       " (34, 'train'): 0.0005848492081794473,\n",
       " (34, 'val'): 0.0007085569064926218,\n",
       " (35, 'train'): 0.0005914978165593413,\n",
       " (35, 'val'): 0.000706589608280747,\n",
       " (36, 'train'): 0.0005943227593821508,\n",
       " (36, 'val'): 0.0007053592276793939,\n",
       " (37, 'train'): 0.0005796657254298528,\n",
       " (37, 'val'): 0.0007070615473720762,\n",
       " (38, 'train'): 0.0005778349177153022,\n",
       " (38, 'val'): 0.0007093599824993698,\n",
       " (39, 'train'): 0.0005873117285470167,\n",
       " (39, 'val'): 0.0007081901723587955,\n",
       " (40, 'train'): 0.0005815076572751557,\n",
       " (40, 'val'): 0.0007072970685031679,\n",
       " (41, 'train'): 0.0005838517617020342,\n",
       " (41, 'val'): 0.0007068676943028415,\n",
       " (42, 'train'): 0.000601797071458013,\n",
       " (42, 'val'): 0.0007053472929530673,\n",
       " (43, 'train'): 0.0006032542463529993,\n",
       " (43, 'val'): 0.0007052396734555563,\n",
       " (44, 'train'): 0.0006030599966093346,\n",
       " (44, 'val'): 0.0007038889107880769,\n",
       " (45, 'train'): 0.0005850461829039785,\n",
       " (45, 'val'): 0.0007028290656981644,\n",
       " (46, 'train'): 0.0005918449411789576,\n",
       " (46, 'val'): 0.0007031866935668168,\n",
       " (47, 'train'): 0.0005953273977394457,\n",
       " (47, 'val'): 0.0007021910752411242,\n",
       " (48, 'train'): 0.0005903349913380764,\n",
       " (48, 'val'): 0.0007027942273351881,\n",
       " (49, 'train'): 0.0005872090416098083,\n",
       " (49, 'val'): 0.0007050222268810978,\n",
       " (50, 'train'): 0.0005834824923011991,\n",
       " (50, 'val'): 0.0007052184255034835,\n",
       " (51, 'train'): 0.0005817815695923788,\n",
       " (51, 'val'): 0.0007066243086700086,\n",
       " (52, 'train'): 0.0005800930472711722,\n",
       " (52, 'val'): 0.0007089624802271525,\n",
       " (53, 'train'): 0.0006064050658433525,\n",
       " (53, 'val'): 0.0007032367090384165,\n",
       " (54, 'train'): 0.0005976936297008285,\n",
       " (54, 'val'): 0.0007066841892622135,\n",
       " (55, 'train'): 0.0006034457883625119,\n",
       " (55, 'val'): 0.0007094164137487058,\n",
       " (56, 'train'): 0.0006061069908793326,\n",
       " (56, 'val'): 0.0007048730772954446,\n",
       " (57, 'train'): 0.0005992838457502701,\n",
       " (57, 'val'): 0.0007036396612723669,\n",
       " (58, 'train'): 0.0006013357046025771,\n",
       " (58, 'val'): 0.0007023693372805914,\n",
       " (59, 'train'): 0.0005916351176522396,\n",
       " (59, 'val'): 0.0007063345638690171,\n",
       " (60, 'train'): 0.0005998521594813576,\n",
       " (60, 'val'): 0.0007048231997975597,\n",
       " (61, 'train'): 0.000592737286179154,\n",
       " (61, 'val'): 0.0007079562379254235,\n",
       " (62, 'train'): 0.0006027783750107994,\n",
       " (62, 'val'): 0.0007052208400434918,\n",
       " (63, 'train'): 0.0005820387870901161,\n",
       " (63, 'val'): 0.0007089393006430732,\n",
       " (64, 'train'): 0.0006772512570023537,\n",
       " (64, 'val'): 0.0007119090468795211,\n",
       " (65, 'train'): 0.0005867580055362649,\n",
       " (65, 'val'): 0.0007097705232876318,\n",
       " (66, 'train'): 0.000593508490257793,\n",
       " (66, 'val'): 0.0007084208644098706,\n",
       " (67, 'train'): 0.0005983790831157455,\n",
       " (67, 'val'): 0.0007084761918694885,\n",
       " (68, 'train'): 0.0005878787660212429,\n",
       " (68, 'val'): 0.0007062649561299218,\n",
       " (69, 'train'): 0.0005919423333748624,\n",
       " (69, 'val'): 0.0007086708727810117,\n",
       " (70, 'train'): 0.000582827600064101,\n",
       " (70, 'val'): 0.0007077493463401441,\n",
       " (71, 'train'): 0.0005740051640680542,\n",
       " (71, 'val'): 0.0007081845154364904,\n",
       " (72, 'train'): 0.0005925416566983417,\n",
       " (72, 'val'): 0.0007061748592941849,\n",
       " (73, 'train'): 0.0005778606670598189,\n",
       " (73, 'val'): 0.000705563152829806,\n",
       " (74, 'train'): 0.0005876491432664571,\n",
       " (74, 'val'): 0.0007050738980372747,\n",
       " (75, 'train'): 0.0006055496460585682,\n",
       " (75, 'val'): 0.0007083033108048969,\n",
       " (76, 'train'): 0.0005816726738380061,\n",
       " (76, 'val'): 0.0007075788108287034,\n",
       " (77, 'train'): 0.0005911328243436637,\n",
       " (77, 'val'): 0.0007058743525434424,\n",
       " (78, 'train'): 0.0005844469803074995,\n",
       " (78, 'val'): 0.0007060879358538875,\n",
       " (79, 'train'): 0.0005917522400893547,\n",
       " (79, 'val'): 0.0007049983574284448,\n",
       " (80, 'train'): 0.0005858364273552541,\n",
       " (80, 'val'): 0.0007094325566733325,\n",
       " (81, 'train'): 0.0005875104451896968,\n",
       " (81, 'val'): 0.0007102043126468305,\n",
       " (82, 'train'): 0.0005902958757899425,\n",
       " (82, 'val'): 0.0007110140803787443,\n",
       " (83, 'train'): 0.000599338673055172,\n",
       " (83, 'val'): 0.0007047286878029505,\n",
       " (84, 'train'): 0.0006087865094067874,\n",
       " (84, 'val'): 0.0007017558371579205,\n",
       " (85, 'train'): 0.0006026425398886204,\n",
       " (85, 'val'): 0.0007041758271279159,\n",
       " (86, 'train'): 0.0005935865661336316,\n",
       " (86, 'val'): 0.0007056010266145071,\n",
       " (87, 'train'): 0.0005903966138484302,\n",
       " (87, 'val'): 0.0007056084082082465,\n",
       " (88, 'train'): 0.0005901489510304398,\n",
       " (88, 'val'): 0.0007068510684702131,\n",
       " (89, 'train'): 0.0005821111543035066,\n",
       " (89, 'val'): 0.0007073166607706635,\n",
       " (90, 'train'): 0.0005850130174722937,\n",
       " (90, 'val'): 0.0007072356012132433,\n",
       " (91, 'train'): 0.0005890970739225546,\n",
       " (91, 'val'): 0.0007086578342649671,\n",
       " (92, 'train'): 0.0005857154071606972,\n",
       " (92, 'val'): 0.0007098784877194299,\n",
       " (93, 'train'): 0.0005859196944921105,\n",
       " (93, 'val'): 0.0007112071745925479,\n",
       " (94, 'train'): 0.0006020797106126944,\n",
       " (94, 'val'): 0.0007134607682625452,\n",
       " (95, 'train'): 0.0005987192918029097,\n",
       " (95, 'val'): 0.0007134606302888305,\n",
       " (96, 'train'): 0.0005868399274294023,\n",
       " (96, 'val'): 0.0007110752717212394,\n",
       " (97, 'train'): 0.0006073543077541722,\n",
       " (97, 'val'): 0.0007083213163746728,\n",
       " (98, 'train'): 0.0005791429085312066,\n",
       " (98, 'val'): 0.0007080198438079269,\n",
       " (99, 'train'): 0.0005848368250385478,\n",
       " (99, 'val'): 0.000707959618281435,\n",
       " (100, 'train'): 0.0005929707032110956,\n",
       " (100, 'val'): 0.0007055452162468875,\n",
       " (101, 'train'): 0.000600182468554488,\n",
       " (101, 'val'): 0.0007065576673657806,\n",
       " (102, 'train'): 0.0005763703784732907,\n",
       " (102, 'val'): 0.0007070620992669353,\n",
       " (103, 'train'): 0.000586060203473877,\n",
       " (103, 'val'): 0.0007098531695427718,\n",
       " (104, 'train'): 0.000584422438232987,\n",
       " (104, 'val'): 0.0007088049832317564,\n",
       " (105, 'train'): 0.0006030296768855165,\n",
       " (105, 'val'): 0.0007049579311300207,\n",
       " (106, 'train'): 0.000591998747377484,\n",
       " (106, 'val'): 0.0007046441099158039,\n",
       " (107, 'train'): 0.0006076607473746494,\n",
       " (107, 'val'): 0.0007036482156426818,\n",
       " (108, 'train'): 0.000588897270736871,\n",
       " (108, 'val'): 0.0007069146053658591,\n",
       " (109, 'train'): 0.0005925270314845773,\n",
       " (109, 'val'): 0.0007054849217335383,\n",
       " (110, 'train'): 0.0005819693863115928,\n",
       " (110, 'val'): 0.0007056937449508243,\n",
       " (111, 'train'): 0.0005960279592761288,\n",
       " (111, 'val'): 0.0007056077873265302,\n",
       " (112, 'train'): 0.0005880607188575797,\n",
       " (112, 'val'): 0.0007047568344407611,\n",
       " (113, 'train'): 0.0005902840790373308,\n",
       " (113, 'val'): 0.0007029386858145396,\n",
       " (114, 'train'): 0.000577187476058801,\n",
       " (114, 'val'): 0.0007046023728670898,\n",
       " (115, 'train'): 0.0005802443871895472,\n",
       " (115, 'val'): 0.0007035041710844746,\n",
       " (116, 'train'): 0.000593254342675209,\n",
       " (116, 'val'): 0.0007035588776623761,\n",
       " (117, 'train'): 0.0005949736158880922,\n",
       " (117, 'val'): 0.0007054605003860262,\n",
       " (118, 'train'): 0.0005933943687489739,\n",
       " (118, 'val'): 0.0007065275890959634,\n",
       " (119, 'train'): 0.0005970514482922024,\n",
       " (119, 'val'): 0.0007052641637899258,\n",
       " (120, 'train'): 0.0005960601761385247,\n",
       " (120, 'val'): 0.0007057327915121008,\n",
       " (121, 'train'): 0.000589773438319012,\n",
       " (121, 'val'): 0.0007053657124439875,\n",
       " (122, 'train'): 0.0005924237409123668,\n",
       " (122, 'val'): 0.0007047815317357028,\n",
       " (123, 'train'): 0.000603654870280513,\n",
       " (123, 'val'): 0.0007069942161992744,\n",
       " (124, 'train'): 0.0005896117158786014,\n",
       " (124, 'val'): 0.0007075471458611665,\n",
       " (125, 'train'): 0.000580990014390813,\n",
       " (125, 'val'): 0.0007061553360135467,\n",
       " (126, 'train'): 0.0006051767548477208,\n",
       " (126, 'val'): 0.0007033491576159442,\n",
       " (127, 'train'): 0.0005930521939363745,\n",
       " (127, 'val'): 0.0007047600078362006,\n",
       " (128, 'train'): 0.0005840356634170921,\n",
       " (128, 'val'): 0.0007052040072502914,\n",
       " (129, 'train'): 0.000587152196439328,\n",
       " (129, 'val'): 0.0007045659478063937,\n",
       " (130, 'train'): 0.0005930282727435783,\n",
       " (130, 'val'): 0.0007073726780988552,\n",
       " (131, 'train'): 0.0006011292269384419,\n",
       " (131, 'val'): 0.0007119044247600767,\n",
       " (132, 'train'): 0.0005882791484947558,\n",
       " (132, 'val'): 0.0007141119352093449,\n",
       " (133, 'train'): 0.0005845013764445428,\n",
       " (133, 'val'): 0.000712991174724367,\n",
       " (134, 'train'): 0.0005953059428268009,\n",
       " (134, 'val'): 0.0007136835268250218,\n",
       " (135, 'train'): 0.0005831268305579821,\n",
       " (135, 'val'): 0.0007125521423640075,\n",
       " (136, 'train'): 0.0005844805769070431,\n",
       " (136, 'val'): 0.0007103026879054529,\n",
       " (137, 'train'): 0.0006022835667762491,\n",
       " (137, 'val'): 0.000703735828951553,\n",
       " (138, 'train'): 0.0005816557893046627,\n",
       " (138, 'val'): 0.000706495648180997,\n",
       " (139, 'train'): 0.000580988617406951,\n",
       " (139, 'val'): 0.0007081512637712338,\n",
       " (140, 'train'): 0.0005861425392881587,\n",
       " (140, 'val'): 0.0007038653172828533,\n",
       " (141, 'train'): 0.0005704706914171025,\n",
       " (141, 'val'): 0.000704666875578739,\n",
       " (142, 'train'): 0.0006117147772952363,\n",
       " (142, 'val'): 0.0007073676420582666,\n",
       " (143, 'train'): 0.0005891537293791771,\n",
       " (143, 'val'): 0.0007079708631391879,\n",
       " (144, 'train'): 0.0005937028434817437,\n",
       " (144, 'val'): 0.0007049844220832542,\n",
       " (145, 'train'): 0.0005961467029043922,\n",
       " (145, 'val'): 0.000706118979939708,\n",
       " (146, 'train'): 0.0005777675003089287,\n",
       " (146, 'val'): 0.0007066299655923137,\n",
       " (147, 'train'): 0.0005929594756000572,\n",
       " (147, 'val'): 0.0007095763942709675,\n",
       " (148, 'train'): 0.0005866759456694126,\n",
       " (148, 'val'): 0.0007081877578187872,\n",
       " (149, 'train'): 0.0005962849525665795,\n",
       " (149, 'val'): 0.0007086486590129358,\n",
       " (150, 'train'): 0.0006028677819779625,\n",
       " (150, 'val'): 0.0007034912015552874,\n",
       " (151, 'train'): 0.0005861336917236999,\n",
       " (151, 'val'): 0.0007044853021701177,\n",
       " (152, 'train'): 0.0005830251266834912,\n",
       " (152, 'val'): 0.0007051960737616928,\n",
       " (153, 'train'): 0.0005893484275374147,\n",
       " (153, 'val'): 0.0007016860914451105,\n",
       " (154, 'train'): 0.0005930495551890797,\n",
       " (154, 'val'): 0.0007043960331766693,\n",
       " (155, 'train'): 0.0005825200739006201,\n",
       " (155, 'val'): 0.0007055234853868131,\n",
       " (156, 'train'): 0.0005923723629503338,\n",
       " (156, 'val'): 0.0007065956791241964,\n",
       " (157, 'train'): 0.0005837131498588456,\n",
       " (157, 'val'): 0.000707654834345535,\n",
       " (158, 'train'): 0.000596041532440318,\n",
       " (158, 'val'): 0.000711235045282929,\n",
       " (159, 'train'): 0.0006009051748723896,\n",
       " (159, 'val'): 0.0007079444411728117,\n",
       " (160, 'train'): 0.0005796198491696958,\n",
       " (160, 'val'): 0.0007066134777334002,\n",
       " (161, 'train'): 0.0005780703353661078,\n",
       " (161, 'val'): 0.0007049043283418372,\n",
       " (162, 'train'): 0.0005845548067655829,\n",
       " (162, 'val'): 0.0007043862370429215,\n",
       " (163, 'train'): 0.0005857670438234453,\n",
       " (163, 'val'): 0.0007054862324838285,\n",
       " (164, 'train'): 0.0005783699280409901,\n",
       " (164, 'val'): 0.0007079136040475634,\n",
       " (165, 'train'): 0.0005750005581864605,\n",
       " (165, 'val'): 0.0007081107684859524,\n",
       " (166, 'train'): 0.0005825465993472824,\n",
       " (166, 'val'): 0.0007104360395007663,\n",
       " (167, 'train'): 0.0005849492218759325,\n",
       " (167, 'val'): 0.0007092448434344044,\n",
       " (168, 'train'): 0.0005808751167798484,\n",
       " (168, 'val'): 0.0007093398383370152,\n",
       " (169, 'train'): 0.0005848021763894293,\n",
       " (169, 'val'): 0.0007120424674616919,\n",
       " (170, 'train'): 0.0005989898065174068,\n",
       " (170, 'val'): 0.0007105039915552845,\n",
       " (171, 'train'): 0.0005862561433955476,\n",
       " (171, 'val'): 0.0007103630514056594,\n",
       " (172, 'train'): 0.0005979339109250793,\n",
       " (172, 'val'): 0.0007075310029365399,\n",
       " (173, 'train'): 0.0006079684977454168,\n",
       " (173, 'val'): 0.0007075236903296576,\n",
       " (174, 'train'): 0.0006188126108436673,\n",
       " (174, 'val'): 0.0007088176078266568,\n",
       " (175, 'train'): 0.0005879148116542234,\n",
       " (175, 'val'): 0.0007090595447354846,\n",
       " (176, 'train'): 0.0005783029935426182,\n",
       " (176, 'val'): 0.0007085774645761207,\n",
       " (177, 'train'): 0.0005856780507774265,\n",
       " (177, 'val'): 0.000708747379205845,\n",
       " (178, 'train'): 0.0005905821195079221,\n",
       " (178, 'val'): 0.0007121454648397587,\n",
       " (179, 'train'): 0.0005851430749451673,\n",
       " (179, 'val'): 0.0007120230821547684,\n",
       " (180, 'train'): 0.0005855550127172912,\n",
       " (180, 'val'): 0.0007112300092423404,\n",
       " (181, 'train'): 0.0005991296428773138,\n",
       " (181, 'val'): 0.0007135622479297497,\n",
       " (182, 'train'): 0.0005870787081895051,\n",
       " (182, 'val'): 0.0007110827223018363,\n",
       " (183, 'train'): 0.0006079006836646133,\n",
       " (183, 'val'): 0.0007093118296729194,\n",
       " (184, 'train'): 0.0005914293471033927,\n",
       " (184, 'val'): 0.0007052001439862781,\n",
       " (185, 'train'): 0.0005969722168865028,\n",
       " (185, 'val'): 0.0007065023399061627,\n",
       " (186, 'train'): 0.0006018350832164288,\n",
       " (186, 'val'): 0.000705042991925169,\n",
       " (187, 'train'): 0.0005850016518875405,\n",
       " (187, 'val'): 0.000705396204634949,\n",
       " (188, 'train'): 0.000611566127864299,\n",
       " (188, 'val'): 0.0007053063837466416,\n",
       " (189, 'train'): 0.0005925571270011089,\n",
       " (189, 'val'): 0.0007077645234487675,\n",
       " (190, 'train'): 0.000606481882709044,\n",
       " (190, 'val'): 0.0007108686560833896,\n",
       " (191, 'train'): 0.0005836473191501918,\n",
       " (191, 'val'): 0.0007100994526236145,\n",
       " (192, 'train'): 0.0005870947131404171,\n",
       " (192, 'val'): 0.0007058387553250348,\n",
       " (193, 'train'): 0.0005877965164405328,\n",
       " (193, 'val'): 0.0007047543509138955,\n",
       " (194, 'train'): 0.0005981722432706091,\n",
       " (194, 'val'): 0.0007075758443938361,\n",
       " (195, 'train'): 0.0005914008727780094,\n",
       " (195, 'val'): 0.0007071862756102173,\n",
       " (196, 'train'): 0.0005855078429535583,\n",
       " (196, 'val'): 0.0007073835090354636,\n",
       " (197, 'train'): 0.0005904364192651378,\n",
       " (197, 'val'): 0.0007085030967438663,\n",
       " (198, 'train'): 0.0005999632973085951,\n",
       " (198, 'val'): 0.0007049494457465631,\n",
       " (199, 'train'): 0.000590376745633505,\n",
       " (199, 'val'): 0.0007071265329917272,\n",
       " (200, 'train'): 0.0005845131042102972,\n",
       " (200, 'val'): 0.0007052260140577952,\n",
       " (201, 'train'): 0.0006403247308399943,\n",
       " (201, 'val'): 0.0007084383180847874,\n",
       " (202, 'train'): 0.0005909316069274037,\n",
       " (202, 'val'): 0.0007081740294341688,\n",
       " (203, 'train'): 0.0006100336193210549,\n",
       " (203, 'val'): 0.0007088537569399233,\n",
       " (204, 'train'): 0.0005896900849485839,\n",
       " (204, 'val'): 0.0007100217044353485,\n",
       " (205, 'train'): 0.0005878870099506996,\n",
       " (205, 'val'): 0.0007098613789787998,\n",
       " (206, 'train'): 0.0005981863683296575,\n",
       " (206, 'val'): 0.0007056962284776899,\n",
       " (207, 'train'): 0.0005773904526399241,\n",
       " (207, 'val'): 0.0007077710082133611,\n",
       " (208, 'train'): 0.0006113286061143434,\n",
       " (208, 'val'): 0.0007112970644677127,\n",
       " (209, 'train'): 0.0005931553292881559,\n",
       " (209, 'val'): 0.0007068630721833971,\n",
       " (210, 'train'): 0.0005979962577974355,\n",
       " (210, 'val'): 0.0007050652056932449,\n",
       " (211, 'train'): 0.0006029099157011067,\n",
       " (211, 'val'): 0.000706752900172163,\n",
       " (212, 'train'): 0.0005922249897762582,\n",
       " (212, 'val'): 0.0007070496816326071,\n",
       " (213, 'train'): 0.0006025590312977632,\n",
       " (213, 'val'): 0.0007088410633581656,\n",
       " (214, 'train'): 0.0005918307471330519,\n",
       " (214, 'val'): 0.0007085278630256653,\n",
       " (215, 'train'): 0.0005890949525766902,\n",
       " (215, 'val'): 0.000709669871462716,\n",
       " (216, 'train'): 0.000613808097248828,\n",
       " (216, 'val'): 0.0007127033615553821,\n",
       " (217, 'train'): 0.0005801028951450631,\n",
       " (217, 'val'): 0.0007132386995686425,\n",
       " (218, 'train'): 0.0005939305690979516,\n",
       " (218, 'val'): 0.0007118294360461059,\n",
       " (219, 'train'): 0.0005938761729609083,\n",
       " (219, 'val'): 0.0007096292382037198,\n",
       " (220, 'train'): 0.0005896941034330262,\n",
       " (220, 'val'): 0.0007101109734287968,\n",
       " (221, 'train'): 0.0006039348361944711,\n",
       " (221, 'val'): 0.0007070052540964551,\n",
       " (222, 'train'): 0.0006026094261970785,\n",
       " (222, 'val'): 0.0007102022430411091,\n",
       " (223, 'train'): 0.0006012502816264276,\n",
       " (223, 'val'): 0.0007120846184315505,\n",
       " (224, 'train'): 0.000596731383767393,\n",
       " (224, 'val'): 0.0007118683446336676,\n",
       " (225, 'train'): 0.0005876327761345439,\n",
       " (225, 'val'): 0.0007127466163149586,\n",
       " (226, 'train'): 0.0005809488292369577,\n",
       " (226, 'val'): 0.0007131314939922757,\n",
       " (227, 'train'): 0.000601304384569327,\n",
       " (227, 'val'): 0.0007079489943053988,\n",
       " (228, 'train'): 0.0005833596267082073,\n",
       " (228, 'val'): 0.0007069355773705023,\n",
       " (229, 'train'): 0.0006027229095774668,\n",
       " (229, 'val'): 0.0007017860534014526,\n",
       " (230, 'train'): 0.0005906449665349943,\n",
       " (230, 'val'): 0.0007025378031863107,\n",
       " (231, 'train'): 0.0005886765990268301,\n",
       " (231, 'val'): 0.00070412360407688,\n",
       " (232, 'train'): 0.00058612579272853,\n",
       " (232, 'val'): 0.0007033152160821137,\n",
       " (233, 'train'): 0.0005788510251376363,\n",
       " (233, 'val'): 0.0007007902281151877,\n",
       " (234, 'train'): 0.0005810784727886871,\n",
       " (234, 'val'): 0.000702725033517237,\n",
       " (235, 'train'): 0.0005933242781018769,\n",
       " (235, 'val'): 0.0007037661831687998,\n",
       " (236, 'train'): 0.000581238556791235,\n",
       " (236, 'val'): 0.0007047041284817236,\n",
       " (237, 'train'): 0.0005944735818990955,\n",
       " (237, 'val'): 0.0007050421640828803,\n",
       " (238, 'train'): 0.0005925869293234966,\n",
       " (238, 'val'): 0.0007085324851451096,\n",
       " (239, 'train'): 0.000680209016772332,\n",
       " (239, 'val'): 0.0007102412206155283,\n",
       " (240, 'train'): 0.0006052967919795601,\n",
       " (240, 'val'): 0.0007088821795251635,\n",
       " (241, 'train'): 0.0005739780694798187,\n",
       " (241, 'val'): 0.0007100752382366745,\n",
       " (242, 'train'): 0.0005857718039166044,\n",
       " (242, 'val'): 0.0007119396770441974,\n",
       " (243, 'train'): 0.0005938332113954756,\n",
       " (243, 'val'): 0.0007129624761916973,\n",
       " (244, 'train'): 0.000580813063101636,\n",
       " (244, 'val'): 0.0007134513170630844,\n",
       " (245, 'train'): 0.0005900387272790626,\n",
       " (245, 'val'): 0.0007085845702224307,\n",
       " (246, 'train'): 0.0005900658046205839,\n",
       " (246, 'val'): 0.0007054186253635971,\n",
       " (247, 'train'): 0.0005782424058351251,\n",
       " (247, 'val'): 0.0007074131043972793,\n",
       " (248, 'train'): 0.0006231974672388147,\n",
       " (248, 'val'): 0.000709487263251234,\n",
       " (249, 'train'): 0.0005934616309349183,\n",
       " (249, 'val'): 0.0007079434063699511,\n",
       " (250, 'train'): 0.0005922705038554139,\n",
       " (250, 'val'): 0.0007062073521040104,\n",
       " (251, 'train'): 0.0005874872656056175,\n",
       " (251, 'val'): 0.0007048197504546908,\n",
       " (252, 'train'): 0.0005926888229118453,\n",
       " (252, 'val'): 0.0007024256305562125,\n",
       " (253, 'train'): 0.0005803657178249625,\n",
       " (253, 'val'): 0.0007046498358249664,\n",
       " (254, 'train'): 0.0005990106405483352,\n",
       " (254, 'val'): 0.0007078875960023315,\n",
       " (255, 'train'): 0.0005904683601801042,\n",
       " (255, 'val'): 0.0007078268185809806,\n",
       " (256, 'train'): 0.0005917810421023103,\n",
       " (256, 'val'): 0.0007050360932394311,\n",
       " (257, 'train'): 0.0005855322125609275,\n",
       " (257, 'val'): 0.0007064675705300437,\n",
       " (258, 'train'): 0.0006173808748523394,\n",
       " (258, 'val'): 0.0007092683679527707,\n",
       " (259, 'train'): 0.0005873174027160362,\n",
       " (259, 'val'): 0.0007112435306663867,\n",
       " (260, 'train'): 0.0005841133253717864,\n",
       " (260, 'val'): 0.0007119518877179535,\n",
       " (261, 'train'): 0.0005770691463516818,\n",
       " (261, 'val'): 0.0007113081713517507,\n",
       " (262, 'train'): 0.0005933435426818,\n",
       " (262, 'val'): 0.0007115568689726017,\n",
       " (263, 'train'): 0.000584660977539089,\n",
       " (263, 'val'): 0.0007116112306162164,\n",
       " (264, 'train'): 0.000605804535249869,\n",
       " (264, 'val'): 0.000710181822931325,\n",
       " (265, 'train'): 0.0006297454065470784,\n",
       " (265, 'val'): 0.0007134193761481179,\n",
       " (266, 'train'): 0.0005883082437018553,\n",
       " (266, 'val'): 0.0007076108897173846,\n",
       " (267, 'train'): 0.0005905782562439088,\n",
       " (267, 'val'): 0.00070972740650177,\n",
       " (268, 'train'): 0.0006006435939559231,\n",
       " (268, 'val'): 0.0007081280841871545,\n",
       " (269, 'train'): 0.0005922155213300828,\n",
       " (269, 'val'): 0.0007092186974154578,\n",
       " (270, 'train'): 0.0005997977805910287,\n",
       " (270, 'val'): 0.0007114754644808945,\n",
       " (271, 'train'): 0.0005920802208560484,\n",
       " (271, 'val'): 0.0007112871303602502,\n",
       " (272, 'train'): 0.0006050703081267851,\n",
       " (272, 'val'): 0.0007123828486159995,\n",
       " (273, 'train'): 0.0005953579416705503,\n",
       " (273, 'val'): 0.0007114144800989716,\n",
       " (274, 'train'): 0.0006032735799197797,\n",
       " (274, 'val'): 0.0007139380193418927,\n",
       " (275, 'train'): 0.0006047671626287478,\n",
       " (275, 'val'): 0.0007093924753091953,\n",
       " (276, 'train'): 0.000599716048411749,\n",
       " (276, 'val'): 0.0007109135665275433,\n",
       " (277, 'train'): 0.0005885571310365642,\n",
       " (277, 'val'): 0.0007096065415276422,\n",
       " (278, 'train'): 0.000604302950065445,\n",
       " (278, 'val'): 0.0007039011214618329,\n",
       " (279, 'train'): 0.0005920694244128686,\n",
       " (279, 'val'): 0.0007036257259271763,\n",
       " (280, 'train'): 0.0005770876348294594,\n",
       " (280, 'val'): 0.0007037291372263873,\n",
       " (281, 'train'): 0.0005795851487804342,\n",
       " (281, 'val'): 0.0007070585119503516,\n",
       " (282, 'train'): 0.0005934931061885975,\n",
       " (282, 'val'): 0.0007087075737891373,\n",
       " (283, 'train'): 0.0005796604651819776,\n",
       " (283, 'val'): 0.0007077363078240995,\n",
       " (284, 'train'): 0.0005890476103458139,\n",
       " (284, 'val'): 0.0007044679174820582,\n",
       " (285, 'train'): 0.0005806427173040531,\n",
       " (285, 'val'): 0.00070452814300855,\n",
       " (286, 'train'): 0.0005943082031552438,\n",
       " (286, 'val'): 0.0007058968422589478,\n",
       " (287, 'train'): 0.0005868795776256809,\n",
       " (287, 'val'): 0.0007050600316789416,\n",
       " (288, 'train'): 0.0005876771864239816,\n",
       " (288, 'val'): 0.0007061865870599393,\n",
       " (289, 'train'): 0.0005799054892526732,\n",
       " (289, 'val'): 0.0007047450376881494,\n",
       " (290, 'train'): 0.0006372903611648966,\n",
       " (290, 'val'): 0.0007061863800993672,\n",
       " (291, 'train'): 0.0005903101388227056,\n",
       " (291, 'val'): 0.0007065629793537988,\n",
       " (292, 'train'): 0.0005840128805074426,\n",
       " (292, 'val'): 0.0007082172841937454,\n",
       " (293, 'train'): 0.0005824863048339332,\n",
       " (293, 'val'): 0.0007104681873763049,\n",
       " (294, 'train'): 0.0005852057150116673,\n",
       " (294, 'val'): 0.0007073885450760523,\n",
       " (295, 'train'): 0.0005801222287118435,\n",
       " (295, 'val'): 0.0007087001232085404,\n",
       " (296, 'train'): 0.0006014981341582757,\n",
       " (296, 'val'): 0.0007111348763660148,\n",
       " (297, 'train'): 0.0005962583063929169,\n",
       " (297, 'val'): 0.0007135311348570718,\n",
       " (298, 'train'): 0.0005813928976379059,\n",
       " (298, 'val'): 0.000711740443000087,\n",
       " (299, 'train'): 0.000594200497424161,\n",
       " (299, 'val'): 0.0007164158202983715,\n",
       " (300, 'train'): 0.0006196072014669577,\n",
       " (300, 'val'): 0.0007180621226628622,\n",
       " (301, 'train'): 0.0005860880914109724,\n",
       " (301, 'val'): 0.0007127596548310032,\n",
       " (302, 'train'): 0.0005834248710285734,\n",
       " (302, 'val'): 0.0007113079643911786,\n",
       " (303, 'train'): 0.0005830725551479392,\n",
       " (303, 'val'): 0.0007094813303814994,\n",
       " (304, 'train'): 0.0005942838163011604,\n",
       " (304, 'val'): 0.0007105496608548694,\n",
       " (305, 'train'): 0.0005952559446019155,\n",
       " (305, 'val'): 0.0007106605917215347,\n",
       " (306, 'train'): 0.0005964191664976102,\n",
       " (306, 'val'): 0.0007081228411859936,\n",
       " (307, 'train'): 0.0005880036494798131,\n",
       " (307, 'val'): 0.0007073770242708701,\n",
       " (308, 'train'): 0.0005952563067829167,\n",
       " (308, 'val'): 0.0007083823007565958,\n",
       " (309, 'train'): 0.0005910651654832893,\n",
       " (309, 'val'): 0.0007056969873331211,\n",
       " (310, 'train'): 0.0005906446388474217,\n",
       " (310, 'val'): 0.0007030153302130875,\n",
       " (311, 'train'): 0.0005935399310180435,\n",
       " (311, 'val'): 0.0007036211727945893,\n",
       " (312, 'train'): 0.00059751579882922,\n",
       " (312, 'val'): 0.0007023895504298034,\n",
       " (313, 'train'): 0.0006050619607170423,\n",
       " (313, 'val'): 0.00070103968459147,\n",
       " (314, 'train'): 0.0005932685884612578,\n",
       " (314, 'val'): 0.0007017107887400521,\n",
       " (315, 'train'): 0.0005830498239784329,\n",
       " (315, 'val'): 0.0007027769806208434,\n",
       " (316, 'train'): 0.0006389974237040237,\n",
       " (316, 'val'): 0.000705341567043905,\n",
       " (317, 'train'): 0.0005936223013257539,\n",
       " (317, 'val'): 0.000708006391370738,\n",
       " (318, 'train'): 0.0005907865965531932,\n",
       " (318, 'val'): 0.0007087557956024453,\n",
       " (319, 'train'): 0.0006069067210234978,\n",
       " (319, 'val'): 0.000705163718925582,\n",
       " (320, 'train'): 0.0005881348624825478,\n",
       " (320, 'val'): 0.0007064826786518097,\n",
       " (321, 'train'): 0.0005834813885114811,\n",
       " (321, 'val'): 0.0007069668284168949,\n",
       " (322, 'train'): 0.0005861508694511873,\n",
       " (322, 'val'): 0.0007081580934701142,\n",
       " (323, 'train'): 0.0005819200779552813,\n",
       " (323, 'val'): 0.0007084212783310148,\n",
       " (324, 'train'): 0.0005907567769840911,\n",
       " (324, 'val'): 0.0007090629940783536,\n",
       " (325, 'train'): 0.0006018681279211132,\n",
       " (325, 'val'): 0.0007116275805014151,\n",
       " (326, 'train'): 0.0005879565142095089,\n",
       " (326, 'val'): 0.0007075617710749308,\n",
       " (327, 'train'): 0.0005847440894555162,\n",
       " (327, 'val'): 0.0007080733086223956,\n",
       " (328, 'train'): 0.0005825447194554188,\n",
       " (328, 'val'): 0.0007080223963216499,\n",
       " (329, 'train'): 0.0005946650549217507,\n",
       " (329, 'val'): 0.0007038599363079777,\n",
       " (330, 'train'): 0.0006019795416957802,\n",
       " (330, 'val'): 0.0007018438643879361,\n",
       " (331, 'train'): 0.0005837477985079642,\n",
       " (331, 'val'): 0.0007039552071580181,\n",
       " (332, 'train'): 0.0005914780863181308,\n",
       " (332, 'val'): 0.0007035594985440925,\n",
       " (333, 'train'): 0.000583502446749696,\n",
       " (333, 'val'): 0.0007059452020459705,\n",
       " (334, 'train'): 0.0005999909265449753,\n",
       " (334, 'val'): 0.000702816234142692,\n",
       " (335, 'train'): 0.0006048445831294413,\n",
       " (335, 'val'): 0.0007056059936682383,\n",
       " (336, 'train'): 0.0005877373602103304,\n",
       " (336, 'val'): 0.0007030766595292975,\n",
       " (337, 'train'): 0.0005866726860404015,\n",
       " (337, 'val'): 0.0007041604430587204,\n",
       " (338, 'train'): 0.0005765669910168206,\n",
       " (338, 'val'): 0.0007059867321341126,\n",
       " (339, 'train'): 0.0005930582475331095,\n",
       " (339, 'val'): 0.0007075024423775849,\n",
       " (340, 'train'): 0.0005955012273733262,\n",
       " (340, 'val'): 0.0007062585403521856,\n",
       " (341, 'train'): 0.0005838476569840202,\n",
       " (341, 'val'): 0.0007057232023389251,\n",
       " (342, 'train'): 0.0005854753846371615,\n",
       " (342, 'val'): 0.0007091443295832034,\n",
       " (343, 'train'): 0.0006202889813317193,\n",
       " (343, 'val'): 0.0007127404764846519,\n",
       " (344, 'train'): 0.0005899384203884336,\n",
       " (344, 'val'): 0.0007134927091775117,\n",
       " (345, 'train'): 0.0005799205456342963,\n",
       " (345, 'val'): 0.0007128465092844433,\n",
       " (346, 'train'): 0.000585425127711561,\n",
       " (346, 'val'): 0.0007119682376031522,\n",
       " (347, 'train'): 0.0005858973255036053,\n",
       " (347, 'val'): 0.0007113610842713604,\n",
       " (348, 'train'): 0.0005833971555586215,\n",
       " (348, 'val'): 0.0007107697289299082,\n",
       " (349, 'train'): 0.0005939803776089792,\n",
       " (349, 'val'): 0.0007097997047283032,\n",
       " (350, 'train'): 0.0005979255807620508,\n",
       " (350, 'val'): 0.0007122758500002048,\n",
       " (351, 'train'): 0.0005918984232401406,\n",
       " (351, 'val'): 0.0007096212357282639,\n",
       " (352, 'train'): 0.0005861283107488243,\n",
       " (352, 'val'): 0.0007085531812023233,\n",
       " (353, 'train'): 0.0005925872225176405,\n",
       " (353, 'val'): 0.0007075013385878669,\n",
       " (354, 'train'): 0.0005794352920794929,\n",
       " (354, 'val'): 0.0007097759732493648,\n",
       " (355, 'train'): 0.0006072810782050645,\n",
       " (355, 'val'): 0.0007128981804406201,\n",
       " (356, 'train'): 0.0005908763312079289,\n",
       " (356, 'val'): 0.0007116903585416299,\n",
       " (357, 'train'): 0.0005895933998679673,\n",
       " (357, 'val'): 0.000711627925435702,\n",
       " (358, 'train'): 0.0005849881649569228,\n",
       " (358, 'val'): 0.0007097740416173581,\n",
       " (359, 'train'): 0.0006043601229234978,\n",
       " (359, 'val'): 0.000705614134117409,\n",
       " (360, 'train'): 0.0005909927637764702,\n",
       " (360, 'val'): 0.0007055795717018622,\n",
       " (361, 'train'): 0.0006047900317719689,\n",
       " (361, 'val'): 0.0007055007197238781,\n",
       " (362, 'train'): 0.0005798658563031091,\n",
       " (362, 'val'): 0.0007040733816447082,\n",
       " (363, 'train'): 0.0005868487577471468,\n",
       " (363, 'val'): 0.0007066747380627526,\n",
       " (364, 'train'): 0.0005797537009197253,\n",
       " (364, 'val'): 0.0007081952083993841,\n",
       " (365, 'train'): 0.0005929497484531668,\n",
       " (365, 'val'): 0.0007060406108697256,\n",
       " (366, 'train'): 0.0005876109590408978,\n",
       " (366, 'val'): 0.0007071979343891144,\n",
       " (367, 'train'): 0.0005783757919238673,\n",
       " (367, 'val'): 0.0007081806521724771,\n",
       " (368, 'train'): 0.000583667946220548,\n",
       " (368, 'val'): 0.0007106452766391966,\n",
       " (369, 'train'): 0.0005933135161521259,\n",
       " (369, 'val'): 0.0007109573041951215,\n",
       " (370, 'train'): 0.0005942817811888677,\n",
       " (370, 'val'): 0.0007102303896789197,\n",
       " (371, 'train'): 0.0005916116793674451,\n",
       " (371, 'val'): 0.0007105086826615863,\n",
       " (372, 'train'): 0.0005830116397528736,\n",
       " (372, 'val'): 0.0007098310247615531,\n",
       " (373, 'train'): 0.0005885871575662384,\n",
       " (373, 'val'): 0.0007115234793336303,\n",
       " (374, 'train'): 0.0005969624897396123,\n",
       " (374, 'val'): 0.0007073746097308618,\n",
       " (375, 'train'): 0.0005856243790023857,\n",
       " (375, 'val'): 0.0007082025899931237,\n",
       " (376, 'train'): 0.0005840036017751252,\n",
       " (376, 'val'): 0.000707783908755691,\n",
       " (377, 'train'): 0.0006015470458401574,\n",
       " (377, 'val'): 0.0007052961047048922,\n",
       " (378, 'train'): 0.0005893516181795685,\n",
       " (378, 'val'): 0.0007085233788799356,\n",
       " (379, 'train'): 0.0005914877099847352,\n",
       " (379, 'val'): 0.0007065057892490316,\n",
       " (380, 'train'): 0.0006026800859857489,\n",
       " (380, 'val'): 0.0007092209739817514,\n",
       " (381, 'train'): 0.0005832692194316122,\n",
       " (381, 'val'): 0.0007073712983617077,\n",
       " (382, 'train'): 0.000581891396669326,\n",
       " (382, 'val'): 0.0007067800120071128,\n",
       " (383, 'train'): 0.000606193810839344,\n",
       " (383, 'val'): 0.0007097352020166538,\n",
       " (384, 'train'): 0.0005933255198653098,\n",
       " (384, 'val'): 0.0007090511973257418,\n",
       " (385, 'train'): 0.000588970517532693,\n",
       " (385, 'val'): 0.0007078043288654751,\n",
       " (386, 'train'): 0.0005831420076666055,\n",
       " (386, 'val'): 0.0007059395451236654,\n",
       " (387, 'train'): 0.0005850542888597205,\n",
       " (387, 'val'): 0.0007092037272674066,\n",
       " (388, 'train'): 0.0005783625119538219,\n",
       " (388, 'val'): 0.0007107440658189633,\n",
       " (389, 'train'): 0.0005907111076845063,\n",
       " (389, 'val'): 0.0007105611816600517,\n",
       " (390, 'train'): 0.0006153992791142729,\n",
       " (390, 'val'): 0.0007135483125845591,\n",
       " (391, 'train'): 0.0005994846319986714,\n",
       " (391, 'val'): 0.0007070365051428477,\n",
       " (392, 'train'): 0.0006061648018658161,\n",
       " (392, 'val'): 0.0007091516421900855,\n",
       " (393, 'train'): 0.000581954433410256,\n",
       " (393, 'val'): 0.0007075691526686704,\n",
       " (394, 'train'): 0.0005886070430278778,\n",
       " (394, 'val'): 0.0007068238876484058,\n",
       " (395, 'train'): 0.0005824842697216405,\n",
       " (395, 'val'): 0.0007095766012315397,\n",
       " (396, 'train'): 0.0005750125274062157,\n",
       " (396, 'val'): 0.0007098308178009811,\n",
       " (397, 'train'): 0.0005890188255795727,\n",
       " (397, 'val'): 0.00070913204992259,\n",
       " (398, 'train'): 0.000608393060112441,\n",
       " (398, 'val'): 0.0007059761081580762,\n",
       " (399, 'train'): 0.0005884391462637319,\n",
       " (399, 'val'): 0.0007058961523903741,\n",
       " (400, 'train'): 0.0005849874405949204,\n",
       " (400, 'val'): 0.0007058324775210133,\n",
       " (401, 'train'): 0.0005838734063285368,\n",
       " (401, 'val'): 0.0007040065333799079,\n",
       " (402, 'train'): 0.0005747026901830126,\n",
       " (402, 'val'): 0.0007056085461819614,\n",
       " (403, 'train'): 0.0006104390895752995,\n",
       " (403, 'val'): 0.0007016896097748368,\n",
       " (404, 'train'): 0.0005902373404414566,\n",
       " (404, 'val'): 0.00070450392862161,\n",
       " (405, 'train'): 0.0006027092329329915,\n",
       " (405, 'val'): 0.0007071093552642398,\n",
       " (406, 'train'): 0.0005937416140955907,\n",
       " (406, 'val'): 0.0007091891020536423,\n",
       " (407, 'train'): 0.0005901647662674939,\n",
       " (407, 'val'): 0.0007064411485636676,\n",
       " (408, 'train'): 0.0006215203105023613,\n",
       " (408, 'val'): 0.0007096767701484539,\n",
       " (409, 'train'): 0.0005984721981264927,\n",
       " (409, 'val'): 0.0007126848040907471,\n",
       " (410, 'train'): 0.0005915166844648344,\n",
       " (410, 'val'): 0.0007077692145550693,\n",
       " (411, 'train'): 0.000584961553276689,\n",
       " (411, 'val'): 0.0007071612333809888,\n",
       " (412, 'train'): 0.0005783777063091596,\n",
       " (412, 'val'): 0.0007052195982800589,\n",
       " (413, 'train'): 0.0005818549888553443,\n",
       " (413, 'val'): 0.0007062208045411993,\n",
       " (414, 'train'): 0.0005975831644954505,\n",
       " (414, 'val'): 0.000708929435522468,\n",
       " (415, 'train'): 0.0006012895351482762,\n",
       " (415, 'val'): 0.0007085151004570502,\n",
       " (416, 'train'): 0.0005900428837372197,\n",
       " (416, 'val'): 0.0007093036202368912,\n",
       " (417, 'train'): 0.0005905296205094567,\n",
       " (417, 'val'): 0.000705693675963967,\n",
       " (418, 'train'): 0.0005791367687008999,\n",
       " (418, 'val'): 0.0007065829165555813,\n",
       " (419, 'train'): 0.0006008944129226384,\n",
       " (419, 'val'): 0.0007091469510837838,\n",
       " (420, 'train'): 0.0005931289245684942,\n",
       " (420, 'val'): 0.0007071390196129128,\n",
       " (421, 'train'): 0.0005918337825547766,\n",
       " (421, 'val'): 0.0007064479782625481,\n",
       " (422, 'train'): 0.0005848438444512862,\n",
       " (422, 'val'): 0.000708655626685531,\n",
       " (423, 'train'): 0.0005917780066805857,\n",
       " (423, 'val'): 0.0007111799247838833,\n",
       " (424, 'train'): 0.0005754474895419898,\n",
       " (424, 'val'): 0.0007106666625649841,\n",
       " (425, 'train'): 0.0005887986540242478,\n",
       " (425, 'val'): 0.0007102225941640359,\n",
       " (426, 'train'): 0.0005906786838615382,\n",
       " (426, 'val'): 0.0007095372787228337,\n",
       " (427, 'train'): 0.0005873056404568531,\n",
       " (427, 'val'): 0.0007115571449200312,\n",
       " (428, 'train'): 0.0005850009102788237,\n",
       " (428, 'val'): 0.0007086803929673301,\n",
       " (429, 'train'): 0.0005860105501832786,\n",
       " (429, 'val'): 0.0007095828790355612,\n",
       " (430, 'train'): 0.0005927328192801387,\n",
       " (430, 'val'): 0.0007107233007748922,\n",
       " (431, 'train'): 0.0005763278135822879,\n",
       " (431, 'val'): 0.000710251913578422,\n",
       " (432, 'train'): 0.000588985625654459,\n",
       " (432, 'val'): 0.000706186518073082,\n",
       " (433, 'train'): 0.0005987178430789047,\n",
       " (433, 'val'): 0.0007066823266170643,\n",
       " (434, 'train'): 0.0005901244779427847,\n",
       " (434, 'val'): 0.0007065038576170251,\n",
       " (435, 'train'): 0.000593166988067053,\n",
       " (435, 'val'): 0.0007093603274336567,\n",
       " (436, 'train'): 0.0005852909137805303,\n",
       " (436, 'val'): 0.0007090935552561725,\n",
       " (437, 'train'): 0.0006067821824992145,\n",
       " (437, 'val'): 0.000706569395131535,\n",
       " (438, 'train'): 0.0005944771347222504,\n",
       " (438, 'val'): 0.0007064684673591897,\n",
       " (439, 'train'): 0.0006026330197023021,\n",
       " (439, 'val'): 0.0007014424988517055,\n",
       " (440, 'train'): 0.0005852376041864907,\n",
       " (440, 'val'): 0.0007044183159316028,\n",
       " (441, 'train'): 0.0005891412772514202,\n",
       " (441, 'val'): 0.0007051867605359466,\n",
       " (442, 'train'): 0.0005867687847327303,\n",
       " (442, 'val'): 0.0007048573482919623,\n",
       " (443, 'train'): 0.0005905155126971227,\n",
       " (443, 'val'): 0.0007018059906032351,\n",
       " (444, 'train'): 0.0005879161224045136,\n",
       " (444, 'val'): 0.0007036205519128729,\n",
       " (445, 'train'): 0.0005869208490131078,\n",
       " (445, 'val'): 0.0007063868559069104,\n",
       " (446, 'train'): 0.0006010406995537105,\n",
       " (446, 'val'): 0.0007045082058067675,\n",
       " (447, 'train'): 0.000607335836523109,\n",
       " (447, 'val'): 0.0007032192553634997,\n",
       " (448, 'train'): 0.000593223574536818,\n",
       " (448, 'val'): 0.0007068424451130408,\n",
       " (449, 'train'): 0.0005890106506369732,\n",
       " (449, 'val'): 0.000707344600447902,\n",
       " (450, 'train'): 0.0005951652786246052,\n",
       " (450, 'val'): 0.0007088650017976761,\n",
       " (451, 'train'): 0.000591944920382014,\n",
       " (451, 'val'): 0.0007095744626389609,\n",
       " (452, 'train'): 0.0005851765163242817,\n",
       " (452, 'val'): 0.0007067276509823623,\n",
       " (453, 'train'): 0.000589247275557783,\n",
       " (453, 'val'): 0.0007079132591132764,\n",
       " (454, 'train'): 0.0005948574420202662,\n",
       " (454, 'val'): 0.0007056811893427814,\n",
       " (455, 'train'): 0.0005935173378222518,\n",
       " (455, 'val'): 0.0007079042218349598,\n",
       " (456, 'train'): 0.0006138750489939142,\n",
       " (456, 'val'): 0.0007106588670501003,\n",
       " (457, 'train'): 0.0005937393547760115,\n",
       " (457, 'val'): 0.0007107788351950822,\n",
       " (458, 'train'): 0.0005901063171525797,\n",
       " (458, 'val'): 0.0007087010890245438,\n",
       " (459, 'train'): 0.0005996147929518311,\n",
       " (459, 'val'): 0.0007099653421728699,\n",
       " (460, 'train'): 0.0006049279020064407,\n",
       " (460, 'val'): 0.0007097474816772672,\n",
       " (461, 'train'): 0.0006207441566167054,\n",
       " (461, 'val'): 0.0007128705856976685,\n",
       " (462, 'train'): 0.0005947180540749321,\n",
       " (462, 'val'): 0.0007138579256004757,\n",
       " (463, 'train'): 0.0005916584697034624,\n",
       " (463, 'val'): 0.0007122694342224686,\n",
       " (464, 'train'): 0.000613160120944182,\n",
       " (464, 'val'): 0.0007144135457498056,\n",
       " (465, 'train'): 0.0005822846045096716,\n",
       " (465, 'val'): 0.0007160262515147527,\n",
       " (466, 'train'): 0.0006084800007994528,\n",
       " (466, 'val'): 0.0007115442443777014,\n",
       " (467, 'train'): 0.000589854239175717,\n",
       " (467, 'val'): 0.0007096722170158669,\n",
       " (468, 'train'): 0.0005903012395181038,\n",
       " (468, 'val'): 0.0007054974773415813,\n",
       " (469, 'train'): 0.0005852249450981617,\n",
       " (469, 'val'): 0.0007076002657413483,\n",
       " (470, 'train'): 0.0005825468408012832,\n",
       " (470, 'val'): 0.000709741962728677,\n",
       " (471, 'train'): 0.0005844366322788927,\n",
       " (471, 'val'): 0.0007078216445666772,\n",
       " (472, 'train'): 0.0005765202869143751,\n",
       " (472, 'val'): 0.0007067391028006872,\n",
       " (473, 'train'): 0.0006050764652038062,\n",
       " (473, 'val'): 0.0007077631437116199,\n",
       " (474, 'train'): 0.0005861125989920563,\n",
       " (474, 'val'): 0.0007085174460102011,\n",
       " (475, 'train'): 0.0005983931391879365,\n",
       " (475, 'val'): 0.0007086418293140552,\n",
       " (476, 'train'): 0.0005829336501106068,\n",
       " (476, 'val'): 0.0007071825503199189,\n",
       " (477, 'train'): 0.0005916888584141378,\n",
       " (477, 'val'): 0.0007075298991468218,\n",
       " (478, 'train'): 0.0005950960503132255,\n",
       " (478, 'val'): 0.0007103471844284623,\n",
       " (479, 'train'): 0.0006164153692898927,\n",
       " (479, 'val'): 0.000703438702556822,\n",
       " (480, 'train'): 0.000599724551041921,\n",
       " (480, 'val'): 0.0007057038170320016,\n",
       " (481, 'train'): 0.0006299739427588604,\n",
       " (481, 'val'): 0.0007087190256074622,\n",
       " (482, 'train'): 0.0005812794832443749,\n",
       " (482, 'val'): 0.0007074455282202473,\n",
       " (483, 'train'): 0.0005984672828129044,\n",
       " (483, 'val'): 0.0007053511562170806,\n",
       " (484, 'train'): 0.0005922514634827772,\n",
       " (484, 'val'): 0.0007084242447658822,\n",
       " (485, 'train'): 0.0006521584086672023,\n",
       " (485, 'val'): 0.0007129762045763157,\n",
       " (486, 'train'): 0.0005862458471070837,\n",
       " (486, 'val'): 0.0007105816017698358,\n",
       " (487, 'train'): 0.0005865033060588219,\n",
       " (487, 'val'): 0.0007104336249607581,\n",
       " (488, 'train'): 0.0005871638207247963,\n",
       " (488, 'val'): 0.0007137743825161899,\n",
       " (489, 'train'): 0.0005874631374522492,\n",
       " (489, 'val'): 0.000709786390264829,\n",
       " (490, 'train'): 0.0005876319137988267,\n",
       " (490, 'val'): 0.0007081972090182481,\n",
       " (491, 'train'): 0.0005844133492145273,\n",
       " (491, 'val'): 0.0007086761847690299,\n",
       " (492, 'train'): 0.0006027855841373956,\n",
       " (492, 'val'): 0.0007082942735265802,\n",
       " (493, 'train'): 0.0005901281342462257,\n",
       " (493, 'val'): 0.0007067515204350153,\n",
       " (494, 'train'): 0.0005909657209283776,\n",
       " (494, 'val'): 0.0007076857404576407,\n",
       " (495, 'train'): 0.0005938027881913715,\n",
       " (495, 'val'): 0.0007104544589916865,\n",
       " (496, 'train'): 0.0005910091309083833,\n",
       " (496, 'val'): 0.0007096660771855602,\n",
       " (497, 'train'): 0.0005956538780419915,\n",
       " (497, 'val'): 0.0007078015004043226,\n",
       " (498, 'train'): 0.0005895055451050952,\n",
       " (498, 'val'): 0.0007095799126006939,\n",
       " (499, 'train'): 0.0005920259454460056,\n",
       " (499, 'val'): 0.0007074971303895667}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old code for autoencoders\n",
    "# for epoch in range(epochs):\n",
    "#     # ä¸éœ€è¦labelï¼Œæ‰€ä»¥ç”¨ä¸€ä¸ªå ä½ç¬¦\"_\"ä»£æ›¿\n",
    "#     for batchidx, (x, _) in enumerate(X_allDataLoader):\n",
    "#         x.requires_grad_(True)\n",
    "#         # encode and decode \n",
    "#         output = model(x)\n",
    "#         # compute loss\n",
    "#         print(output.shape)\n",
    "#         loss = loss_function(output, x)      \n",
    "#         # update\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "           \n",
    "#     loss_train[epoch,0] = loss.item()  \n",
    "#     print('Epoch: %04d, Training loss=%.8f' %\n",
    "#           (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'saved/models/ae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_batch = model(X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0790, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(recon_batch,X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5515, 0.0381, 0.3997,  ..., 0.4411, 0.3207, 0.5624],\n",
       "        [0.5441, 0.0509, 0.3908,  ..., 0.5186, 0.4760, 0.4010],\n",
       "        [0.0147, 1.0000, 0.3591,  ..., 0.5408, 0.9711, 0.4654],\n",
       "        ...,\n",
       "        [0.5680, 0.0116, 0.5302,  ..., 0.4048, 0.3630, 0.3851],\n",
       "        [0.2286, 1.0000, 0.3620,  ..., 0.4728, 0.6605, 0.4806],\n",
       "        [0.6331, 0.0566, 0.4586,  ..., 0.4623, 0.3378, 0.5212]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7359, 0.0868, 0.4233,  ..., 0.2414, 0.1303, 0.6761],\n",
       "        [0.5435, 0.0496, 0.0739,  ..., 0.1036, 0.2022, 0.3589],\n",
       "        [0.1336, 0.4184, 0.0618,  ..., 0.1218, 0.3170, 0.3572],\n",
       "        ...,\n",
       "        [0.3869, 0.0693, 0.4479,  ..., 0.2760, 0.2085, 0.0863],\n",
       "        [0.1415, 0.0232, 0.0663,  ..., 0.2076, 0.3566, 0.3610],\n",
       "        [0.6655, 0.0682, 0.3321,  ..., 0.2687, 0.1723, 0.2549]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5173557e-08"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_batch.cpu().detach().numpy().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = model.encode(X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = train_embeddings.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.001)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.001)\n",
    "clf.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFrg = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "RFrg.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeature = model.encode(X_testTensor)\n",
    "lasso = clf.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfresult = RFrg.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.728384012277445"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(lasso,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4051580467325753"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(rfresult,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.010893, 0.003969, 0.126364, 0.04815 , 0.013301, 0.003975,\n",
       "       0.006658, 0.236411, 0.024986, 0.112077, 0.004105, 0.007248,\n",
       "       0.015496, 0.079042, 0.066385, 0.006305, 0.006078, 0.025742,\n",
       "       0.030093, 0.024503, 0.011125, 0.037697, 0.012029, 0.003817,\n",
       "       0.010667, 0.067386, 0.057037, 0.006329, 0.067224, 0.003564,\n",
       "       0.186092, 0.108377, 0.101198, 0.029871, 0.02189 , 0.043306,\n",
       "       0.12798 , 0.005638, 0.038664, 0.002193, 0.005611, 0.157488,\n",
       "       0.007132, 0.006106, 0.008437, 0.031169, 0.054449, 0.006736,\n",
       "       0.016306, 0.003624, 0.093631, 0.011592, 0.010451, 0.003869,\n",
       "       0.011615, 0.189386, 0.003639, 0.023794, 0.020825, 0.036415,\n",
       "       0.00255 , 0.005367, 0.018914, 0.013027, 0.185878, 0.003549,\n",
       "       0.004383, 0.007288, 0.012238, 0.019928, 0.075604, 0.01131 ,\n",
       "       0.006595, 0.00253 , 0.005594, 0.018333, 0.015419, 0.010771,\n",
       "       0.008096, 0.03138 , 0.043866, 0.014501, 0.002152, 0.015858,\n",
       "       0.142685, 0.134998, 0.006991, 0.049048, 0.033088, 0.005561,\n",
       "       0.012738, 0.018554, 0.017699, 0.141754, 0.010312, 0.004757,\n",
       "       0.00994 , 0.035553, 0.006461, 0.053599, 0.029555, 0.115657,\n",
       "       0.003604, 0.029185, 0.034422, 0.028011, 0.004901, 0.081803,\n",
       "       0.014341, 0.007389, 0.043371, 0.0064  , 0.093948, 0.010454,\n",
       "       0.017118, 0.00423 , 0.010077, 0.011115, 0.004962, 0.016636,\n",
       "       0.030657, 0.010251, 0.007557, 0.194723, 0.076808, 0.007164,\n",
       "       0.05563 , 0.151063, 0.041997, 0.016026, 0.006891, 0.003589,\n",
       "       0.004219, 0.022708, 0.056148])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06333838, 0.05163833, 0.06567472, 0.01129548, 0.018018  ,\n",
       "       0.03644564, 0.02255642, 0.06655517, 0.08604187, 0.1840049 ,\n",
       "       0.01704251, 0.01657313, 0.01637306, 0.05192906, 0.04636875,\n",
       "       0.0158126 , 0.03550026, 0.04195729, 0.02563312, 0.02081395,\n",
       "       0.02545172, 0.03157807, 0.03359234, 0.04270427, 0.04292507,\n",
       "       0.04729566, 0.03571336, 0.01792041, 0.06086463, 0.01884558,\n",
       "       0.04435331, 0.05352922, 0.06214855, 0.05880985, 0.04370992,\n",
       "       0.03754919, 0.06236823, 0.03367751, 0.014912  , 0.02141574,\n",
       "       0.01891135, 0.03906331, 0.00853745, 0.01525252, 0.04674301,\n",
       "       0.01589686, 0.0159818 , 0.11001802, 0.04571779, 0.03660129,\n",
       "       0.04875667, 0.12059977, 0.02018895, 0.00974867, 0.02984847,\n",
       "       0.04812139, 0.01177737, 0.01921359, 0.02491929, 0.01831419,\n",
       "       0.02661277, 0.05170534, 0.01132798, 0.02398118, 0.10757313,\n",
       "       0.03008334, 0.02077054, 0.01386602, 0.02790825, 0.0578679 ,\n",
       "       0.05979995, 0.02454595, 0.02195155, 0.02974711, 0.0398703 ,\n",
       "       0.04172193, 0.01170313, 0.01101227, 0.02074192, 0.03836589,\n",
       "       0.02150163, 0.0193431 , 0.02742561, 0.0484471 , 0.00827134,\n",
       "       0.05364669, 0.04202063, 0.02353435, 0.02848924, 0.04406588,\n",
       "       0.06492542, 0.02741462, 0.03081573, 0.05490529, 0.02442098,\n",
       "       0.02679573, 0.02560113, 0.03518162, 0.01851107, 0.01074009,\n",
       "       0.01833467, 0.00794974, 0.01278691, 0.05502725, 0.06504407,\n",
       "       0.01281437, 0.01488325, 0.02860066, 0.01554881, 0.0429307 ,\n",
       "       0.02882911, 0.01417558, 0.01620802, 0.02739339, 0.01377968,\n",
       "       0.02245662, 0.02161772, 0.0123807 , 0.0230328 , 0.01526777,\n",
       "       0.01168386, 0.02273019, 0.01770679, 0.01526637, 0.05350913,\n",
       "       0.01886298, 0.01555105, 0.02701976, 0.01135669, 0.01286989,\n",
       "       0.04732301, 0.00853382, 0.04201864, 0.02291672, 0.12357078])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 432 samples in 0.021s...\n",
      "[t-SNE] Computed neighbors for 432 samples in 0.148s...\n",
      "[t-SNE] Computed conditional probabilities for sample 432 / 432\n",
      "[t-SNE] Mean sigma: 4.804942\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 63.244987\n",
      "[t-SNE] KL divergence after 300 iterations: 0.534911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38695967e+00,  9.49048805e+00],\n",
       "       [ 7.56260777e+00, -8.94485855e+00],\n",
       "       [-1.23303595e+01,  1.28850088e+01],\n",
       "       [ 3.12437010e+00,  1.30500593e+01],\n",
       "       [ 2.62618709e+00,  1.62697697e+00],\n",
       "       [-1.43233442e+00,  8.25452709e+00],\n",
       "       [-2.10045949e-01, -3.66724086e+00],\n",
       "       [-3.30791146e-01, -7.03777885e+00],\n",
       "       [-2.64493060e+00,  1.32905018e+00],\n",
       "       [-7.99771118e+00, -3.77446151e+00],\n",
       "       [-2.76347423e+00,  1.97066176e+00],\n",
       "       [ 5.80298901e-01, -6.09831095e+00],\n",
       "       [ 2.48209524e+00,  1.46067441e+00],\n",
       "       [ 1.44414127e-01,  9.88844776e+00],\n",
       "       [-2.01148105e+00,  9.35389519e+00],\n",
       "       [-3.80680776e+00, -2.27106214e+00],\n",
       "       [ 1.57961531e+01, -1.00799692e+00],\n",
       "       [-2.48561525e+00,  9.64710414e-01],\n",
       "       [ 3.67186975e+00,  2.29648042e+00],\n",
       "       [-1.21141825e+01,  9.10669804e+00],\n",
       "       [-1.26251507e+01, -1.07372169e+01],\n",
       "       [ 7.38152933e+00, -7.91032124e+00],\n",
       "       [ 1.47968416e+01, -3.92218083e-01],\n",
       "       [-5.53489971e+00,  2.14613914e-01],\n",
       "       [-1.21900253e+01, -1.18492508e+01],\n",
       "       [-6.04755831e+00, -4.60390902e+00],\n",
       "       [-5.41793919e+00, -1.70116019e+00],\n",
       "       [-6.62054205e+00, -2.33792567e+00],\n",
       "       [-9.86423206e+00,  4.80527782e+00],\n",
       "       [-1.64079952e+00,  9.73632526e+00],\n",
       "       [ 5.69955111e+00, -4.72625077e-01],\n",
       "       [-3.86239076e+00, -1.07013245e+01],\n",
       "       [ 6.78064489e+00, -5.29208612e+00],\n",
       "       [-6.21606255e+00, -3.82051349e+00],\n",
       "       [ 4.74458551e+00, -4.33884048e+00],\n",
       "       [-1.03377695e+01,  6.83498764e+00],\n",
       "       [-1.16934576e+01, -1.05616922e+01],\n",
       "       [-1.20318975e+01,  1.10390882e+01],\n",
       "       [ 5.97777653e+00,  2.27057695e+00],\n",
       "       [-1.22605009e+01,  1.19320230e+01],\n",
       "       [ 4.58507252e+00, -4.26166296e+00],\n",
       "       [-8.50687790e+00, -3.77515435e+00],\n",
       "       [ 2.37341022e+00, -2.66096687e+00],\n",
       "       [ 9.55692863e+00, -6.88600826e+00],\n",
       "       [-2.97967148e+00, -1.08045254e+01],\n",
       "       [ 5.63868105e-01, -5.70236921e+00],\n",
       "       [ 2.87818646e+00,  1.94702470e+00],\n",
       "       [-1.23049707e+01, -1.20019751e+01],\n",
       "       [ 3.69744682e+00, -2.93972564e+00],\n",
       "       [-1.11775246e+01,  8.11417007e+00],\n",
       "       [ 1.61960435e+00,  1.33104982e+01],\n",
       "       [-1.25086994e+01, -1.22174959e+01],\n",
       "       [-4.09013462e+00, -8.33452511e+00],\n",
       "       [ 1.88539612e+00, -6.05611229e+00],\n",
       "       [ 1.53062191e+01, -2.76223731e+00],\n",
       "       [ 1.33514366e+01, -6.55605316e-01],\n",
       "       [ 1.33465872e+01, -1.54307318e+00],\n",
       "       [-1.00098896e+00,  7.29650402e+00],\n",
       "       [-1.15913000e+01,  6.57506561e+00],\n",
       "       [ 9.82919693e+00,  1.10929048e+00],\n",
       "       [-1.20667791e+01, -1.10055752e+01],\n",
       "       [-1.13023586e+01,  7.32196093e+00],\n",
       "       [ 5.96220350e+00, -6.69190025e+00],\n",
       "       [-1.42043293e+00,  8.94597340e+00],\n",
       "       [-1.25229797e+01,  1.10635443e+01],\n",
       "       [-8.84253407e+00,  5.11824989e+00],\n",
       "       [ 1.33949213e+01, -2.61049718e-01],\n",
       "       [-2.94770932e+00, -1.07191048e+01],\n",
       "       [-6.63642597e+00, -2.85487080e+00],\n",
       "       [-1.00923510e+01,  5.26004219e+00],\n",
       "       [ 1.37875662e+01, -3.25982666e+00],\n",
       "       [-1.75862050e+00,  8.95755959e+00],\n",
       "       [-1.27024899e+01, -9.56552696e+00],\n",
       "       [-8.39019871e+00,  5.60894394e+00],\n",
       "       [ 2.22284484e+00,  1.27836103e+01],\n",
       "       [ 1.06478300e+01, -2.07127905e+00],\n",
       "       [ 1.37603178e+01, -3.07315278e+00],\n",
       "       [-7.44523048e+00, -3.67479038e+00],\n",
       "       [-1.25569696e+01,  1.29398394e+01],\n",
       "       [ 2.20153093e+00,  8.68040371e+00],\n",
       "       [-1.17136717e+01,  1.29298344e+01],\n",
       "       [ 1.20906568e+00,  1.30178108e+01],\n",
       "       [ 3.48175454e+00, -2.59641528e+00],\n",
       "       [-1.18878031e+00,  9.73762321e+00],\n",
       "       [-1.90478492e+00,  8.79839420e+00],\n",
       "       [ 8.80477619e+00, -8.83985519e+00],\n",
       "       [-1.19189949e+01,  8.93695831e+00],\n",
       "       [-1.08425331e+01,  5.63562632e+00],\n",
       "       [ 7.88731194e+00, -9.09360600e+00],\n",
       "       [-1.23392200e+01, -1.11486092e+01],\n",
       "       [ 7.42436981e+00, -6.82085752e+00],\n",
       "       [ 1.10786743e+01, -4.66497993e+00],\n",
       "       [ 6.61536837e+00, -4.66973019e+00],\n",
       "       [ 1.47212648e+01, -2.56787062e+00],\n",
       "       [ 4.44575119e+00,  1.22011757e+01],\n",
       "       [ 8.81708527e+00,  2.59482884e+00],\n",
       "       [ 9.24740982e+00, -3.75588012e+00],\n",
       "       [ 9.91623497e+00,  2.67746663e+00],\n",
       "       [ 4.53779125e+00,  1.16065292e+01],\n",
       "       [ 2.90866709e+00,  2.38404989e+00],\n",
       "       [ 1.15130854e+01,  1.50390577e+00],\n",
       "       [-2.01448512e+00, -7.46875000e+00],\n",
       "       [-3.90494132e+00, -1.01349888e+01],\n",
       "       [ 3.58782798e-01, -7.54133654e+00],\n",
       "       [-4.96610498e+00, -4.52236772e-01],\n",
       "       [-4.72201729e+00, -4.86525357e-01],\n",
       "       [ 9.54773235e+00,  4.15201902e-01],\n",
       "       [ 1.06654739e+00, -7.61429310e+00],\n",
       "       [ 9.44554710e+00,  3.42649174e+00],\n",
       "       [ 3.93529534e+00,  2.34725714e+00],\n",
       "       [-1.06806355e+01,  7.42018986e+00],\n",
       "       [-5.31292391e+00, -3.02642560e+00],\n",
       "       [ 1.37267962e+01, -9.58961621e-02],\n",
       "       [ 9.64927197e+00,  2.90404177e+00],\n",
       "       [-3.90795827e+00, -9.71675777e+00],\n",
       "       [ 2.21852779e+00,  8.84229279e+00],\n",
       "       [-2.43094826e+00, -9.86888504e+00],\n",
       "       [ 4.61715364e+00, -6.18211174e+00],\n",
       "       [-3.03012705e+00,  2.26470375e+00],\n",
       "       [-8.38516712e+00, -2.29477191e+00],\n",
       "       [ 1.00158367e+01, -3.29475307e+00],\n",
       "       [-3.16931754e-01,  1.04558480e+00],\n",
       "       [ 5.89423990e+00,  1.64914739e+00],\n",
       "       [ 7.77870607e+00, -1.41305313e-01],\n",
       "       [ 4.31415319e+00,  1.00389957e+00],\n",
       "       [-9.77729797e+00,  6.13716698e+00],\n",
       "       [ 2.67498350e+00,  1.26629086e+01],\n",
       "       [-1.67285872e+00,  2.06066549e-01],\n",
       "       [-5.31611395e+00, -2.15254641e+00],\n",
       "       [-1.03798509e+00,  8.52474117e+00],\n",
       "       [ 1.25469484e+01, -4.20003080e+00],\n",
       "       [-6.78387022e+00, -4.11333466e+00],\n",
       "       [-1.10333414e+01, -1.09794846e+01],\n",
       "       [-4.81986189e+00, -8.55340385e+00],\n",
       "       [-1.05959377e+01,  7.42928791e+00],\n",
       "       [ 1.35578365e+01, -1.10357392e+00],\n",
       "       [ 3.91454196e+00,  1.28018188e+01],\n",
       "       [ 5.35652637e+00,  4.45627499e+00],\n",
       "       [ 2.94139934e+00,  7.54800749e+00],\n",
       "       [ 7.99165606e-01,  2.10200500e+00],\n",
       "       [ 1.34840078e+01,  1.15376556e+00],\n",
       "       [ 9.97941017e+00,  2.09025168e+00],\n",
       "       [ 6.94514942e+00, -5.06976366e+00],\n",
       "       [-1.12156522e+00,  6.51176977e+00],\n",
       "       [ 6.41593838e+00,  5.33590019e-01],\n",
       "       [ 2.48560667e+00, -7.31586790e+00],\n",
       "       [-3.08095717e+00,  3.07573652e+00],\n",
       "       [ 1.49744043e+01, -1.43208981e+00],\n",
       "       [-3.94194698e+00,  2.58045197e+00],\n",
       "       [ 5.12409389e-01, -7.30343866e+00],\n",
       "       [-1.14936543e+00, -8.26963615e+00],\n",
       "       [-1.23701849e+01, -9.94946575e+00],\n",
       "       [ 9.55179024e+00, -2.80841422e+00],\n",
       "       [ 1.45195332e+01, -3.14314938e+00],\n",
       "       [ 6.08344316e+00, -5.75474787e+00],\n",
       "       [-1.66171920e+00, -1.00159903e+01],\n",
       "       [ 5.16826153e+00,  6.87394857e-01],\n",
       "       [ 7.75102520e+00,  1.23512399e+00],\n",
       "       [-5.20001554e+00, -4.42353821e+00],\n",
       "       [-1.16643829e+01,  1.22546082e+01],\n",
       "       [-2.39087313e-01, -7.97114658e+00],\n",
       "       [ 2.66024089e+00,  7.15590343e-02],\n",
       "       [ 2.47979736e+00, -1.20149098e-01],\n",
       "       [ 1.11798134e+01, -1.12309802e+00],\n",
       "       [ 1.00427446e+01,  2.23597598e+00],\n",
       "       [ 2.39647079e+00, -3.00293565e+00],\n",
       "       [ 1.75538778e+00, -2.29874805e-01],\n",
       "       [-8.25767899e+00,  5.56894398e+00],\n",
       "       [ 1.26797457e+01,  1.83052510e-01],\n",
       "       [ 3.65853310e+00,  1.31013641e+01],\n",
       "       [ 3.05042791e+00,  3.28375435e+00],\n",
       "       [ 8.89480019e+00, -8.31329632e+00],\n",
       "       [-6.36011076e+00, -3.27656674e+00],\n",
       "       [-7.72019196e+00, -4.68740082e+00],\n",
       "       [ 2.38099337e+00,  1.14648819e+01],\n",
       "       [ 5.57950020e+00, -3.25398517e+00],\n",
       "       [-1.54504061e+00,  1.73349416e+00],\n",
       "       [-4.77455425e+00, -7.44696426e+00],\n",
       "       [ 1.88495946e+00,  1.24621334e+01],\n",
       "       [-9.94173336e+00,  4.25530529e+00],\n",
       "       [ 1.22518265e+00, -6.02573299e+00],\n",
       "       [-1.02754486e+00,  8.97429180e+00],\n",
       "       [ 1.36594734e+01, -1.69132125e+00],\n",
       "       [ 4.52259588e+00,  1.27486401e+01],\n",
       "       [-1.17935982e+01,  1.24558697e+01],\n",
       "       [ 2.20533228e+00, -3.51578212e+00],\n",
       "       [ 6.07498109e-01,  9.30501080e+00],\n",
       "       [-1.59435844e+00, -9.21544456e+00],\n",
       "       [-1.06637259e+01,  6.24823904e+00],\n",
       "       [-3.28618097e+00,  3.38999200e+00],\n",
       "       [ 8.46410370e+00, -5.24268532e+00],\n",
       "       [ 9.24143505e+00, -6.47025490e+00],\n",
       "       [ 8.85740852e+00, -3.03506875e+00],\n",
       "       [ 9.21223640e+00,  2.60806751e+00],\n",
       "       [-7.93478537e+00, -5.69379997e+00],\n",
       "       [-2.92548275e+00, -1.10004683e+01],\n",
       "       [ 1.22909565e+01,  7.41001189e-01],\n",
       "       [-2.13259792e+00,  2.33912373e+00],\n",
       "       [ 3.03798985e+00,  7.47828531e+00],\n",
       "       [-1.19132538e+01, -9.86878300e+00],\n",
       "       [ 1.13982830e+01, -2.65655112e+00],\n",
       "       [-9.46811676e+00,  5.38901091e+00],\n",
       "       [-7.52211666e+00, -3.76208186e+00],\n",
       "       [-1.27853060e+01, -1.07628317e+01],\n",
       "       [ 1.18364487e+01, -3.67671371e+00],\n",
       "       [-5.49955797e+00, -2.37723517e+00],\n",
       "       [ 1.98000267e-01,  8.98478127e+00],\n",
       "       [-1.20603669e+00,  7.46808863e+00],\n",
       "       [-1.19846535e+01, -1.09674158e+01],\n",
       "       [ 1.02565622e+00, -7.18270731e+00],\n",
       "       [ 1.39770603e+01, -3.21431231e+00],\n",
       "       [-1.71419919e+00, -7.80973768e+00],\n",
       "       [-1.14314795e+01,  7.04884768e+00],\n",
       "       [ 7.03544283e+00, -5.23924685e+00],\n",
       "       [-1.25758829e+01, -8.95474148e+00],\n",
       "       [-3.53125477e+00, -8.94823074e+00],\n",
       "       [-1.27272234e+01, -1.16681395e+01],\n",
       "       [ 1.16679649e+01, -2.44150257e+00],\n",
       "       [ 3.39862752e+00,  1.00116611e+00],\n",
       "       [-7.44967937e+00, -5.29606104e+00],\n",
       "       [-1.27030239e+01,  1.16146460e+01],\n",
       "       [ 1.37509489e+01, -4.99717283e+00],\n",
       "       [-7.02467203e+00,  4.21298838e+00],\n",
       "       [ 1.04485168e+01,  1.68717182e+00],\n",
       "       [-1.07589699e-01, -8.52167797e+00],\n",
       "       [ 7.86521375e-01,  3.44939137e+00],\n",
       "       [ 2.14381242e+00,  8.39000702e+00],\n",
       "       [ 3.45638442e+00, -4.18635559e+00],\n",
       "       [ 1.04587822e+01, -2.23618555e+00],\n",
       "       [-1.17578430e+01, -1.16589003e+01],\n",
       "       [-1.59050846e+00, -6.41801119e+00],\n",
       "       [-1.11288271e+01,  6.67993116e+00],\n",
       "       [-1.29417610e+01,  1.18318977e+01],\n",
       "       [ 8.85124874e+00,  1.06559467e+00],\n",
       "       [ 1.39985189e+01, -2.76247430e+00],\n",
       "       [ 7.44598627e+00,  6.89990878e-01],\n",
       "       [-3.40563941e+00, -1.14003992e+01],\n",
       "       [-1.19377146e+01,  7.12050867e+00],\n",
       "       [-1.26496038e+01,  1.18862476e+01],\n",
       "       [ 1.06652822e+01, -2.00405335e+00],\n",
       "       [-5.73091793e+00,  5.34514189e-02],\n",
       "       [ 1.26410427e+01, -1.76856136e+00],\n",
       "       [-6.88167667e+00,  5.14096308e+00],\n",
       "       [ 7.98751640e+00, -5.01320660e-01],\n",
       "       [-3.40779448e+00, -8.00727558e+00],\n",
       "       [ 7.35423565e-01, -8.95922279e+00],\n",
       "       [ 9.61868668e+00,  1.01949203e+00],\n",
       "       [ 9.64278400e-01, -7.27479315e+00],\n",
       "       [ 7.96007156e+00, -6.87766218e+00],\n",
       "       [ 8.92298603e+00, -7.29401207e+00],\n",
       "       [-1.29683280e+00, -8.88123512e+00],\n",
       "       [ 6.77763128e+00, -8.34247470e-03],\n",
       "       [-1.16009235e+01,  1.13445311e+01],\n",
       "       [-7.28072929e+00,  5.34459734e+00],\n",
       "       [-3.02117658e+00,  3.43587518e+00],\n",
       "       [-3.32070589e+00, -9.96820068e+00],\n",
       "       [ 4.71528482e+00,  8.15983593e-01],\n",
       "       [-8.32585621e+00, -4.14603853e+00],\n",
       "       [-1.06976299e+01,  6.15989780e+00],\n",
       "       [ 3.87063050e+00,  1.28176775e+01],\n",
       "       [ 9.17123985e+00, -8.03847408e+00],\n",
       "       [ 5.14356089e+00,  1.22900572e+01],\n",
       "       [-1.01919127e+00,  4.09820747e+00],\n",
       "       [ 8.63532925e+00, -8.85489273e+00],\n",
       "       [ 4.26895714e+00, -5.19281578e+00],\n",
       "       [ 1.40365620e+01, -5.02813530e+00],\n",
       "       [-7.99599528e-01, -9.03569889e+00],\n",
       "       [-1.70468318e+00,  3.78973603e-01],\n",
       "       [ 9.92479324e+00, -5.73882246e+00],\n",
       "       [-1.21435680e+01,  1.02431240e+01],\n",
       "       [-6.87431240e+00, -2.34714317e+00],\n",
       "       [-3.31086731e+00, -1.01944284e+01],\n",
       "       [-7.78267765e+00, -2.98950553e+00],\n",
       "       [-7.83591175e+00, -3.39019918e+00],\n",
       "       [ 4.50287199e+00,  5.09823132e+00],\n",
       "       [ 7.80595684e+00,  6.61109149e-01],\n",
       "       [-6.47045279e+00,  4.86629677e+00],\n",
       "       [ 3.35609651e+00,  1.24073591e+01],\n",
       "       [ 2.62724400e+00,  1.69955239e-01],\n",
       "       [ 8.39008427e+00,  1.82605267e+00],\n",
       "       [ 9.40023136e+00, -3.83224273e+00],\n",
       "       [ 9.76836562e-01,  3.80863309e-01],\n",
       "       [ 5.06734967e-01,  9.52676201e+00],\n",
       "       [-9.87027836e+00,  6.05390549e+00],\n",
       "       [ 1.46180887e+01, -1.73347092e+00],\n",
       "       [-2.44592404e+00,  2.75671744e+00],\n",
       "       [-1.01150811e+00, -6.61248636e+00],\n",
       "       [-4.87366132e-02, -5.31904173e+00],\n",
       "       [ 8.34443569e+00, -6.75339174e+00],\n",
       "       [-6.90460300e+00, -4.21875334e+00],\n",
       "       [ 9.50686550e+00,  2.19590020e+00],\n",
       "       [-3.20454860e+00,  1.77946806e+00],\n",
       "       [ 2.98691368e+00,  1.77655721e+00],\n",
       "       [-1.15277805e+01, -1.19646292e+01],\n",
       "       [-2.16122723e+00, -8.82502651e+00],\n",
       "       [ 4.00611734e+00, -5.19384623e+00],\n",
       "       [-7.77830124e+00, -4.91452980e+00],\n",
       "       [ 1.08347282e+01, -5.33953190e-01],\n",
       "       [ 3.57835817e+00, -5.63576651e+00],\n",
       "       [ 1.15446167e+01, -7.71387517e-01],\n",
       "       [-8.61838341e+00, -3.14647007e+00],\n",
       "       [ 1.03372335e+00,  1.00353003e+01],\n",
       "       [ 3.47802019e+00,  1.19113264e+01],\n",
       "       [ 1.08932333e+01, -3.83961940e+00],\n",
       "       [ 4.16877222e+00,  1.31835175e+01],\n",
       "       [ 4.51931763e+00,  4.10322094e+00],\n",
       "       [ 7.95422971e-01,  9.87105942e+00],\n",
       "       [ 7.00532961e+00,  2.33410883e+00],\n",
       "       [-1.77625251e+00, -6.58595943e+00],\n",
       "       [ 1.09881449e+01, -2.14291739e+00],\n",
       "       [ 7.92135620e+00, -7.92921495e+00],\n",
       "       [-6.74679756e+00, -3.07297182e+00],\n",
       "       [ 7.17850351e+00, -9.07462692e+00],\n",
       "       [-1.32314720e+01, -8.90387726e+00],\n",
       "       [-6.39008427e+00,  2.74388599e+00],\n",
       "       [-1.80033565e+00, -6.56648111e+00],\n",
       "       [-5.40848970e+00, -6.29632807e+00],\n",
       "       [-4.73750353e+00, -8.73533535e+00],\n",
       "       [-1.26012602e+01, -1.01176023e+01],\n",
       "       [ 4.01109409e+00, -4.73544073e+00],\n",
       "       [-9.01334524e-01, -8.08257198e+00],\n",
       "       [ 1.19591153e+00,  1.18862448e+01],\n",
       "       [-1.81650627e+00,  9.58441162e+00],\n",
       "       [-1.13639593e+01, -1.18142166e+01],\n",
       "       [ 1.11782084e+01, -2.72225237e+00],\n",
       "       [ 9.08543777e+00, -9.29002985e-02],\n",
       "       [-2.49736333e+00, -8.47980118e+00],\n",
       "       [-4.82802010e+00, -4.84700060e+00],\n",
       "       [-5.87577963e+00, -4.79699707e+00],\n",
       "       [ 5.22836876e+00,  1.23222609e+01],\n",
       "       [-1.17283897e+01,  7.81341553e+00],\n",
       "       [-6.54445696e+00,  4.90296030e+00],\n",
       "       [ 1.06393938e+01,  3.47426414e+00],\n",
       "       [-3.63041830e+00, -8.45958710e+00],\n",
       "       [-1.15638113e+01,  6.41595316e+00],\n",
       "       [-4.73708725e+00, -2.41020823e+00],\n",
       "       [ 9.51444149e+00, -6.79164505e+00],\n",
       "       [-1.18571396e+01,  1.12147160e+01],\n",
       "       [-5.35808229e+00, -9.89423943e+00],\n",
       "       [-1.09522362e+01,  6.16901112e+00],\n",
       "       [ 1.40560541e+01, -5.03369093e+00],\n",
       "       [-5.93312979e+00, -1.87494445e+00],\n",
       "       [ 1.40057421e+01, -1.28912842e+00],\n",
       "       [-1.12678223e+01, -1.04590273e+01],\n",
       "       [-4.12643003e+00, -8.84049416e+00],\n",
       "       [ 9.39710999e+00, -1.06505835e+00],\n",
       "       [ 8.10017967e+00, -6.01050901e+00],\n",
       "       [ 1.89661062e+00,  1.22822027e+01],\n",
       "       [-1.11238012e+01, -1.11509027e+01],\n",
       "       [-2.30032015e+00, -9.30286217e+00],\n",
       "       [ 9.78619480e+00, -7.44994307e+00],\n",
       "       [-1.28069954e+01, -9.94468975e+00],\n",
       "       [-7.38131714e+00,  5.32490158e+00],\n",
       "       [-4.06526184e+00, -3.31944203e+00],\n",
       "       [ 1.49776335e+01, -1.99236333e+00],\n",
       "       [-2.61863470e-01,  1.02981055e+00],\n",
       "       [-4.12673712e+00, -2.44898343e+00],\n",
       "       [ 1.46044798e+01, -4.19576931e+00],\n",
       "       [-1.16727009e+01,  1.21965265e+01],\n",
       "       [ 3.48724914e+00,  1.24871302e+01],\n",
       "       [-1.19262018e+01,  1.16486540e+01],\n",
       "       [ 5.09088612e+00,  4.66958857e+00],\n",
       "       [ 7.67978859e+00, -7.82323503e+00],\n",
       "       [-1.28685322e+01,  1.25811501e+01],\n",
       "       [ 5.36644268e+00,  7.35731125e-01],\n",
       "       [-1.72885105e-01, -5.56544590e+00],\n",
       "       [-1.39693654e+00,  5.98956156e+00],\n",
       "       [ 1.45882690e+00,  1.37628431e+01],\n",
       "       [-7.43658447e+00,  5.15564537e+00],\n",
       "       [-1.12313471e+01,  8.10450935e+00],\n",
       "       [ 1.04108419e+01, -4.31989574e+00],\n",
       "       [ 8.55010509e+00, -4.43104601e+00],\n",
       "       [-2.77694851e-01,  8.48146152e+00],\n",
       "       [-1.06221809e+01, -1.07808542e+01],\n",
       "       [-1.21376667e+01,  9.78461266e+00],\n",
       "       [ 1.13702030e+01, -3.24175417e-01],\n",
       "       [-3.83689356e+00,  4.10027313e+00],\n",
       "       [ 1.31037416e+01, -4.12405109e+00],\n",
       "       [-1.25130663e+01,  1.25305920e+01],\n",
       "       [ 6.31402159e+00,  1.84512067e+00],\n",
       "       [-6.74675894e+00, -4.85360193e+00],\n",
       "       [ 1.51678696e+01, -3.06201124e+00],\n",
       "       [-1.24259405e+01, -1.02758694e+01],\n",
       "       [ 3.05581641e+00, -2.48061585e+00],\n",
       "       [ 8.04061604e+00, -8.77308369e+00],\n",
       "       [ 4.74202347e+00,  1.37930453e+00],\n",
       "       [-1.16018248e+00,  1.01503630e+01],\n",
       "       [ 8.54798698e+00,  1.07516336e+00],\n",
       "       [ 1.24580469e+01, -9.58229721e-01],\n",
       "       [-5.43672180e+00, -9.08186531e+00],\n",
       "       [ 1.22798691e+01, -4.29013014e-01],\n",
       "       [-1.16097984e+01,  7.12215710e+00],\n",
       "       [ 2.16816521e+00,  1.34104319e+01],\n",
       "       [-2.54358125e+00,  2.35355806e+00],\n",
       "       [-8.11634159e+00,  5.45572376e+00],\n",
       "       [-1.48381174e+00, -6.35055351e+00],\n",
       "       [ 6.44572163e+00,  7.00097859e-01],\n",
       "       [ 8.35554981e+00,  1.63882494e+00],\n",
       "       [ 1.52773428e+00,  1.22393188e+01],\n",
       "       [-1.33690357e+01, -9.71035290e+00],\n",
       "       [ 6.95298862e+00, -9.13971710e+00],\n",
       "       [-3.18639994e+00, -9.66592216e+00],\n",
       "       [ 9.58747768e+00, -6.92675972e+00],\n",
       "       [ 5.67427301e+00, -1.66476145e-01],\n",
       "       [ 3.45124677e-03, -7.43487072e+00],\n",
       "       [-4.87117338e+00, -3.22280169e+00],\n",
       "       [-3.48549771e+00,  2.08232641e+00],\n",
       "       [-1.30003624e+01,  1.24835987e+01],\n",
       "       [ 1.39795554e+00,  1.29781427e+01],\n",
       "       [ 2.40506768e+00, -7.09963894e+00],\n",
       "       [-1.20050030e+01,  1.30176067e+01],\n",
       "       [-1.28974619e+01, -9.60777664e+00],\n",
       "       [ 1.32489519e+01, -2.37109518e+00],\n",
       "       [-7.14530563e+00,  5.21370935e+00],\n",
       "       [ 1.45546780e+01, -1.02124619e+00],\n",
       "       [ 8.66032887e+00, -7.59465647e+00],\n",
       "       [ 3.85402489e+00,  1.17759399e+01],\n",
       "       [-2.97424436e+00, -8.87494659e+00],\n",
       "       [ 1.50330496e+01, -3.42887282e+00],\n",
       "       [ 1.32717190e+01,  1.48814559e-01],\n",
       "       [-1.26945763e+01,  1.10713158e+01],\n",
       "       [ 2.10269833e+00,  1.29232483e+01],\n",
       "       [-1.88688064e+00, -1.06785774e+01],\n",
       "       [ 8.99435043e+00, -4.14116001e+00],\n",
       "       [-9.76909101e-01,  9.37844753e+00],\n",
       "       [ 4.24387884e+00,  9.08010960e-01],\n",
       "       [-6.33574724e+00, -1.24545717e+00],\n",
       "       [-1.16540337e+01,  1.17112808e+01],\n",
       "       [-1.21819067e+01,  1.04077406e+01],\n",
       "       [-5.79116964e+00, -5.20605421e+00],\n",
       "       [-1.19490528e+01,  8.49355221e+00],\n",
       "       [ 4.36608696e+00,  5.15368080e+00]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=\"label\")\n",
    "plt.legend()\n",
    "plt.savefig(\"saved/figures/tsne_ae_gdsc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data type conversion\n",
    "B_feature = torch.FloatTensor(feature).to(device)\n",
    "y = torch.FloatTensor(Y_train.values).to(device)\n",
    "# construct TensorDataset\n",
    "b_data = TensorDataset(B_feature, y)\n",
    "trainDataLoader2 = DataLoader(dataset=b_data, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization DNN model\n",
    "\n",
    "predictor = DNN(128, dim_dnn_out).to(device)\n",
    "optimizer = optim.Adam(predictor.parameters(), lr=1e-3,betas=(0.9,0.99))\n",
    "#loss1-softmax\n",
    "loss_func = nn.MSELoss().to(device)\n",
    "#loss2-sigmoid\n",
    "#loss_func = nn.BCELoss()\n",
    "#loss3-sigmoid\n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = torch.nn.MSELoss(size_average=True)\n",
    "#criterion = torch.nn.BCELoss(size_average=True) # Defined loss function\n",
    "#optimizer = optim.Adm(model.parameters(), lr=0.01) # Defined optimizer\n",
    "loss_train = np.zeros((epochs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ',epoch)\n",
    "    for step,(batch_x,batch_y) in enumerate(trainDataLoader2):\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "        # predict label\n",
    "        output = predictor(b_x)\n",
    "        # b_y=F.sigmoid(b_y) \n",
    "        \n",
    "        #print\n",
    "        #print(output)\n",
    "        #print(b_y)\n",
    "        # compute loss\n",
    "        loss = loss_func(output,b_y)\n",
    "        #loss = criterion(output, b_y)\n",
    "        \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item())) \n",
    "\n",
    "# Save model\n",
    "torch.save(predictor.state_dict(), 'saved/models/DNN_GDSC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict = predictor(testFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
