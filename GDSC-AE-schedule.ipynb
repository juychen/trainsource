{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "#import scipy.io as sio\n",
    "\n",
    "from models import VAE,AEBase\n",
    "from models import DNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import models\n",
    "import utils as ut\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "epochs = 500 #200,500,1000  \n",
    "#dim_au_in = 20049\n",
    "dim_au_out = 512 #8, 16, 32, 64, 128, 256,512\n",
    "dim_dnn_in = dim_au_out\n",
    "dim_dnn_out=1\n",
    "select_drug = 'Gefitinib'\n",
    "na = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r=pd.read_csv('data/GDSCexpression.csv',index_col=0)\n",
    "label_r=pd.read_csv('data/GDSClabel.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_r=label_r.fillna(na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
     ]
    }
   ],
   "source": [
    "hvg,adata = ut.highly_variable_genes(data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Erlotinib</th>\n",
       "      <th>AICAR</th>\n",
       "      <th>Camptothecin</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Cisplatin</th>\n",
       "      <th>Cytarabine</th>\n",
       "      <th>Docetaxel</th>\n",
       "      <th>Methotrexate</th>\n",
       "      <th>ATRA</th>\n",
       "      <th>Gefitinib</th>\n",
       "      <th>...</th>\n",
       "      <th>CMK</th>\n",
       "      <th>Pyrimethamine</th>\n",
       "      <th>JW-7-52-1</th>\n",
       "      <th>A-443654</th>\n",
       "      <th>GW843682X</th>\n",
       "      <th>MS-275</th>\n",
       "      <th>Parthenolide</th>\n",
       "      <th>MG-132</th>\n",
       "      <th>KIN001-135</th>\n",
       "      <th>TGX221</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BxPC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.159708</td>\n",
       "      <td>0.188393</td>\n",
       "      <td>0.436127</td>\n",
       "      <td>0.154274</td>\n",
       "      <td>0.113764</td>\n",
       "      <td>0.249987</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMOE-2</th>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.318206</td>\n",
       "      <td>0.281740</td>\n",
       "      <td>0.063808</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.101530</td>\n",
       "      <td>0.191210</td>\n",
       "      <td>0.031229</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022662</td>\n",
       "      <td>0.025217</td>\n",
       "      <td>0.270791</td>\n",
       "      <td>0.095608</td>\n",
       "      <td>0.139914</td>\n",
       "      <td>0.171747</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFM-223</th>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.334087</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.361438</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>0.186659</td>\n",
       "      <td>0.228797</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUGC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.416792</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.266409</td>\n",
       "      <td>0.237811</td>\n",
       "      <td>0.234952</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>0.153002</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OC-314</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.479915</td>\n",
       "      <td>0.390145</td>\n",
       "      <td>0.067738</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>0.110418</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.020309</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KP-N-S19s</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC-3</th>\n",
       "      <td>0.003515</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048227</td>\n",
       "      <td>0.146476</td>\n",
       "      <td>0.400087</td>\n",
       "      <td>0.205247</td>\n",
       "      <td>0.147090</td>\n",
       "      <td>0.298803</td>\n",
       "      <td>0.023796</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.069181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Panc 08.13</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.180653</td>\n",
       "      <td>0.090963</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EKVX</th>\n",
       "      <td>0.177238</td>\n",
       "      <td>0.078395</td>\n",
       "      <td>0.078938</td>\n",
       "      <td>0.072213</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>0.095642</td>\n",
       "      <td>0.111598</td>\n",
       "      <td>0.126364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.243635</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.177759</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>0.007216</td>\n",
       "      <td>0.063911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMS-114</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.127677</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.281940</td>\n",
       "      <td>0.092074</td>\n",
       "      <td>0.191972</td>\n",
       "      <td>0.185916</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.043920</td>\n",
       "      <td>0.285969</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.174841</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.080451</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.028733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Erlotinib     AICAR  Camptothecin  Vinblastine  Cisplatin  \\\n",
       "BxPC-3      -1.000000  0.159708      0.188393     0.436127   0.154274   \n",
       "KMOE-2       0.002108  0.339814      0.318206     0.281740   0.063808   \n",
       "MFM-223      0.002148  0.334087      0.122671     0.361438   0.051983   \n",
       "NUGC-3      -1.000000  0.095099      0.416792     0.422631   0.266409   \n",
       "OC-314      -1.000000  0.012740      0.479915     0.390145   0.067738   \n",
       "...               ...       ...           ...          ...        ...   \n",
       "KP-N-S19s   -1.000000 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "BC-3         0.003515 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "Panc 08.13  -1.000000  0.152818      0.196279     0.180653   0.090963   \n",
       "EKVX         0.177238  0.078395      0.078938     0.072213   0.007225   \n",
       "DMS-114      0.000322  0.127677      0.168317     0.281940   0.092074   \n",
       "\n",
       "            Cytarabine  Docetaxel  Methotrexate      ATRA  Gefitinib  ...  \\\n",
       "BxPC-3        0.113764   0.249987      0.015324  0.015324   0.020293  ...   \n",
       "KMOE-2        0.140681   0.101530      0.191210  0.031229   0.007419  ...   \n",
       "MFM-223       0.186659   0.228797      0.003639  0.023043   0.003639  ...   \n",
       "NUGC-3        0.237811   0.234952      0.011558  0.028838   0.153002  ...   \n",
       "OC-314        0.125869   0.249668      0.110418  0.002214   0.020309  ...   \n",
       "...                ...        ...           ...       ...        ...  ...   \n",
       "KP-N-S19s    -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "BC-3         -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "Panc 08.13    0.093197   0.097267      0.018269  0.010237   0.009405  ...   \n",
       "EKVX          0.003470   0.129184      0.095642  0.111598   0.126364  ...   \n",
       "DMS-114       0.191972   0.185916      0.007135  0.009329   0.020822  ...   \n",
       "\n",
       "                 CMK  Pyrimethamine  JW-7-52-1  A-443654  GW843682X    MS-275  \\\n",
       "BxPC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "KMOE-2      0.022662       0.025217   0.270791  0.095608   0.139914  0.171747   \n",
       "MFM-223     0.038997       0.006167   0.002185  0.102592   0.020338  0.010874   \n",
       "NUGC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "OC-314     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "...              ...            ...        ...       ...        ...       ...   \n",
       "KP-N-S19s  -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "BC-3        0.048227       0.146476   0.400087  0.205247   0.147090  0.298803   \n",
       "Panc 08.13 -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "EKVX        0.020472       0.044047   0.243635  0.079375   0.019859  0.177759   \n",
       "DMS-114     0.002137       0.043920   0.285969  0.075195   0.174841  0.008383   \n",
       "\n",
       "            Parthenolide    MG-132  KIN001-135    TGX221  \n",
       "BxPC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "KMOE-2          0.026670  0.013904    0.002116  0.000956  \n",
       "MFM-223         0.000970  0.035206    0.007051  0.000323  \n",
       "NUGC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "OC-314         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "...                  ...       ...         ...       ...  \n",
       "KP-N-S19s      -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "BC-3            0.023796  0.017219    0.002065  0.069181  \n",
       "Panc 08.13     -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "EKVX            0.014843  0.114431    0.007216  0.063911  \n",
       "DMS-114         0.002135  0.080451    0.002135  0.028733  \n",
       "\n",
       "[789 rows x 139 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = label_r.loc[:,select_drug]!=na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r.columns = adata.var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your is gene-cell, mine is cell-gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_r.loc[selected_idx,:]\n",
    "#data = data_r.loc[selected_idx,hvg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_r.loc[selected_idx,select_drug]\n",
    "#sscaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "mmscaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "data = mmscaler.fit_transform(data)\n",
    "#data = sscaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22376755104184073\n",
      "0.33693391083156693\n"
     ]
    }
   ],
   "source": [
    "print(np.std(data))\n",
    "print(np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49458949, 0.09687665, 0.50474807, ..., 0.20001366, 0.27789963,\n",
       "       0.31386817])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000009\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(data.max())\n",
    "print(data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 11833)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 139)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_all, X_test, Y_train_all, Y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_all, Y_train_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 11833)\n",
      "(675,)\n",
      "(432, 11833) (432,)\n",
      "(135, 11833) (135,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000009\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all data to AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainTensor = torch.FloatTensor(X_train).to(device)\n",
    "X_validTensor = torch.FloatTensor(X_valid).to(device)\n",
    "X_testTensor = torch.FloatTensor(X_test).to(device)\n",
    "X_allTensor = torch.FloatTensor(data).to(device)\n",
    "#X_alltrainTensor = torch.FloatTensor(X_train_all).to(device)\n",
    "\n",
    "\n",
    "Y_trainTensor = torch.FloatTensor(Y_train.values).to(device)\n",
    "Y_validTensor = torch.FloatTensor(Y_valid.values).to(device)\n",
    "\n",
    "# construct TensorDataset\n",
    "train_dataset = TensorDataset(X_trainTensor, X_trainTensor)\n",
    "valid_dataset = TensorDataset(X_validTensor, X_validTensor)\n",
    "test_dataset = TensorDataset(X_testTensor, X_testTensor)\n",
    "all_dataset = TensorDataset(X_allTensor, X_allTensor)\n",
    "\n",
    "X_trainDataLoader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "X_validDataLoader = DataLoader(dataset=valid_dataset, batch_size=200, shuffle=True)\n",
    "X_allDataLoader = DataLoader(dataset=all_dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = X_trainDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_train = {'train':X_trainDataLoader,'val':X_validDataLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainDataLoader.dataset.tensors[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 432, 'val': 108}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{x: dataloaders_train[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEBase(input_dim=data.shape[1],latent_dim=512,hidden_dims=[2048,1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEBase(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=11833, out_features=2048, bias=True)\n",
      "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (decoder_input): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=11833, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VAE(dim_au_in=data_r.shape[1],dim_au_out=128)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae_model(net,data_loaders,optimizer,loss_function,n_epochs,scheduler):\n",
    "    \n",
    "    dataset_sizes = {x: data_loaders[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}\n",
    "    loss_train = {}\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #optimizer = scheduler(optimizer, epoch)\n",
    "                net.train()  # Set model to training mode\n",
    "            else:\n",
    "                net.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # for data in data_loaders[phase]:\n",
    "            for batchidx, (x, _) in enumerate(data_loaders[phase]):\n",
    "\n",
    "                x.requires_grad_(True)\n",
    "                # encode and decode \n",
    "                output = model(x)\n",
    "                # compute loss\n",
    "                loss = loss_function(output, x)      \n",
    "\n",
    "                # zero the parameter (weight) gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    # update the weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                # print loss statistics\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Schedular\n",
    "#             if phase == 'train':\n",
    "#                 scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)\n",
    "                \n",
    "            last_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            loss_train[epoch,phase] = epoch_loss\n",
    "            print('{} Loss: {:.8f}. Learning rate = {}'.format(phase, epoch_loss,last_lr))\n",
    "            \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Select best model wts\n",
    "    torch.save(best_model_wts, 'saved/models/ae.pkl')\n",
    "    net.load_state_dict(best_model_wts)           \n",
    "    \n",
    "    return net, loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00124929. Learning rate = 0.01\n",
      "val Loss: 0.00305093. Learning rate = 0.01\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00109289. Learning rate = 0.01\n",
      "val Loss: 0.00248298. Learning rate = 0.01\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00096754. Learning rate = 0.01\n",
      "val Loss: 0.00213574. Learning rate = 0.01\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00090810. Learning rate = 0.01\n",
      "val Loss: 0.00181604. Learning rate = 0.01\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00096537. Learning rate = 0.01\n",
      "val Loss: 0.00135841. Learning rate = 0.01\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00090909. Learning rate = 0.01\n",
      "val Loss: 0.00138355. Learning rate = 0.01\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00085292. Learning rate = 0.01\n",
      "val Loss: 0.00128438. Learning rate = 0.01\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00084545. Learning rate = 0.01\n",
      "val Loss: 0.00115831. Learning rate = 0.01\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00077238. Learning rate = 0.01\n",
      "val Loss: 0.00109419. Learning rate = 0.01\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00079010. Learning rate = 0.01\n",
      "val Loss: 0.00099125. Learning rate = 0.01\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00070405. Learning rate = 0.01\n",
      "val Loss: 0.00097289. Learning rate = 0.01\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00072488. Learning rate = 0.01\n",
      "val Loss: 0.00090067. Learning rate = 0.01\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00066999. Learning rate = 0.01\n",
      "val Loss: 0.00088943. Learning rate = 0.01\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00066324. Learning rate = 0.01\n",
      "val Loss: 0.00087647. Learning rate = 0.01\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00065144. Learning rate = 0.01\n",
      "val Loss: 0.00084386. Learning rate = 0.01\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00063250. Learning rate = 0.01\n",
      "val Loss: 0.00081573. Learning rate = 0.01\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00058255. Learning rate = 0.01\n",
      "val Loss: 0.00081072. Learning rate = 0.01\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00057991. Learning rate = 0.01\n",
      "val Loss: 0.00075466. Learning rate = 0.01\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00056878. Learning rate = 0.01\n",
      "val Loss: 0.00071329. Learning rate = 0.01\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00053929. Learning rate = 0.01\n",
      "val Loss: 0.00070242. Learning rate = 0.01\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00054993. Learning rate = 0.01\n",
      "val Loss: 0.00065066. Learning rate = 0.01\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00052836. Learning rate = 0.01\n",
      "val Loss: 0.00063014. Learning rate = 0.01\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00054947. Learning rate = 0.01\n",
      "val Loss: 0.00060625. Learning rate = 0.01\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00048060. Learning rate = 0.01\n",
      "val Loss: 0.00063720. Learning rate = 0.01\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00050813. Learning rate = 0.01\n",
      "val Loss: 0.00058638. Learning rate = 0.01\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00046824. Learning rate = 0.01\n",
      "val Loss: 0.00057160. Learning rate = 0.01\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00045797. Learning rate = 0.01\n",
      "val Loss: 0.00055261. Learning rate = 0.01\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00046070. Learning rate = 0.01\n",
      "val Loss: 0.00051723. Learning rate = 0.01\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00043595. Learning rate = 0.01\n",
      "val Loss: 0.00049065. Learning rate = 0.01\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00038929. Learning rate = 0.01\n",
      "val Loss: 0.00048056. Learning rate = 0.01\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00037021. Learning rate = 0.01\n",
      "val Loss: 0.00044359. Learning rate = 0.01\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00035770. Learning rate = 0.01\n",
      "val Loss: 0.00040780. Learning rate = 0.01\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00037500. Learning rate = 0.01\n",
      "val Loss: 0.00038410. Learning rate = 0.01\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00032262. Learning rate = 0.01\n",
      "val Loss: 0.00037495. Learning rate = 0.01\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00030955. Learning rate = 0.01\n",
      "val Loss: 0.00037714. Learning rate = 0.01\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00029268. Learning rate = 0.01\n",
      "val Loss: 0.00036303. Learning rate = 0.01\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00029011. Learning rate = 0.01\n",
      "val Loss: 0.00034425. Learning rate = 0.01\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00027416. Learning rate = 0.01\n",
      "val Loss: 0.00030256. Learning rate = 0.01\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00024867. Learning rate = 0.01\n",
      "val Loss: 0.00029798. Learning rate = 0.01\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00024493. Learning rate = 0.01\n",
      "val Loss: 0.00028706. Learning rate = 0.01\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00026624. Learning rate = 0.01\n",
      "val Loss: 0.00028168. Learning rate = 0.01\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00024529. Learning rate = 0.01\n",
      "val Loss: 0.00028836. Learning rate = 0.01\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00023876. Learning rate = 0.01\n",
      "val Loss: 0.00028557. Learning rate = 0.01\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00022856. Learning rate = 0.01\n",
      "val Loss: 0.00029899. Learning rate = 0.01\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00022163. Learning rate = 0.01\n",
      "val Loss: 0.00030482. Learning rate = 0.01\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.00023792. Learning rate = 0.01\n",
      "val Loss: 0.00027787. Learning rate = 0.01\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.00025073. Learning rate = 0.01\n",
      "val Loss: 0.00025906. Learning rate = 0.01\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.00023308. Learning rate = 0.01\n",
      "val Loss: 0.00026306. Learning rate = 0.01\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.00022508. Learning rate = 0.01\n",
      "val Loss: 0.00025633. Learning rate = 0.01\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.00021828. Learning rate = 0.01\n",
      "val Loss: 0.00025496. Learning rate = 0.01\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.00021075. Learning rate = 0.01\n",
      "val Loss: 0.00025064. Learning rate = 0.01\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.00020728. Learning rate = 0.01\n",
      "val Loss: 0.00024839. Learning rate = 0.01\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.00019975. Learning rate = 0.01\n",
      "val Loss: 0.00024813. Learning rate = 0.01\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.00020114. Learning rate = 0.01\n",
      "val Loss: 0.00024417. Learning rate = 0.01\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.00019908. Learning rate = 0.01\n",
      "val Loss: 0.00023844. Learning rate = 0.01\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.00020532. Learning rate = 0.01\n",
      "val Loss: 0.00023807. Learning rate = 0.01\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.00019928. Learning rate = 0.01\n",
      "val Loss: 0.00023970. Learning rate = 0.01\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.00020449. Learning rate = 0.01\n",
      "val Loss: 0.00023266. Learning rate = 0.01\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.00020116. Learning rate = 0.01\n",
      "val Loss: 0.00022846. Learning rate = 0.01\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.00019653. Learning rate = 0.01\n",
      "val Loss: 0.00022551. Learning rate = 0.01\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.00019131. Learning rate = 0.01\n",
      "val Loss: 0.00022544. Learning rate = 0.01\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.00018278. Learning rate = 0.01\n",
      "val Loss: 0.00022224. Learning rate = 0.01\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.00018403. Learning rate = 0.01\n",
      "val Loss: 0.00021857. Learning rate = 0.01\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.00018874. Learning rate = 0.01\n",
      "val Loss: 0.00021376. Learning rate = 0.01\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.00017661. Learning rate = 0.01\n",
      "val Loss: 0.00021475. Learning rate = 0.01\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.00018027. Learning rate = 0.01\n",
      "val Loss: 0.00021022. Learning rate = 0.01\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.00017440. Learning rate = 0.01\n",
      "val Loss: 0.00021160. Learning rate = 0.01\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.00017439. Learning rate = 0.01\n",
      "val Loss: 0.00020990. Learning rate = 0.01\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.00017333. Learning rate = 0.01\n",
      "val Loss: 0.00021272. Learning rate = 0.01\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.00017122. Learning rate = 0.01\n",
      "val Loss: 0.00020718. Learning rate = 0.01\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.00016879. Learning rate = 0.01\n",
      "val Loss: 0.00020614. Learning rate = 0.01\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.00017285. Learning rate = 0.01\n",
      "val Loss: 0.00019948. Learning rate = 0.01\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.00016540. Learning rate = 0.01\n",
      "val Loss: 0.00019646. Learning rate = 0.01\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.00016198. Learning rate = 0.01\n",
      "val Loss: 0.00020135. Learning rate = 0.01\n",
      "Epoch 74/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00016383. Learning rate = 0.01\n",
      "val Loss: 0.00019560. Learning rate = 0.01\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.00016233. Learning rate = 0.01\n",
      "val Loss: 0.00019241. Learning rate = 0.01\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.00015574. Learning rate = 0.01\n",
      "val Loss: 0.00019378. Learning rate = 0.01\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.00015734. Learning rate = 0.01\n",
      "val Loss: 0.00018968. Learning rate = 0.01\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.00015724. Learning rate = 0.01\n",
      "val Loss: 0.00019272. Learning rate = 0.01\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.00015704. Learning rate = 0.01\n",
      "val Loss: 0.00018954. Learning rate = 0.01\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.00015207. Learning rate = 0.01\n",
      "val Loss: 0.00018895. Learning rate = 0.01\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.00015123. Learning rate = 0.01\n",
      "val Loss: 0.00018773. Learning rate = 0.01\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.00015116. Learning rate = 0.01\n",
      "val Loss: 0.00018618. Learning rate = 0.01\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.00014546. Learning rate = 0.01\n",
      "val Loss: 0.00018354. Learning rate = 0.01\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.00014421. Learning rate = 0.01\n",
      "val Loss: 0.00018192. Learning rate = 0.01\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.00014355. Learning rate = 0.01\n",
      "val Loss: 0.00018165. Learning rate = 0.01\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.00014652. Learning rate = 0.01\n",
      "val Loss: 0.00017866. Learning rate = 0.01\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.00014023. Learning rate = 0.01\n",
      "val Loss: 0.00017783. Learning rate = 0.01\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.00014082. Learning rate = 0.01\n",
      "val Loss: 0.00017883. Learning rate = 0.01\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.00014117. Learning rate = 0.01\n",
      "val Loss: 0.00017847. Learning rate = 0.01\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.00013811. Learning rate = 0.01\n",
      "val Loss: 0.00017760. Learning rate = 0.01\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.00013540. Learning rate = 0.01\n",
      "val Loss: 0.00017564. Learning rate = 0.01\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.00013739. Learning rate = 0.01\n",
      "val Loss: 0.00017687. Learning rate = 0.01\n",
      "Epoch 93/499\n",
      "----------\n",
      "train Loss: 0.00013336. Learning rate = 0.01\n",
      "val Loss: 0.00017391. Learning rate = 0.01\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.00013359. Learning rate = 0.01\n",
      "val Loss: 0.00017319. Learning rate = 0.01\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.00012994. Learning rate = 0.01\n",
      "val Loss: 0.00017320. Learning rate = 0.01\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.00013707. Learning rate = 0.01\n",
      "val Loss: 0.00017170. Learning rate = 0.01\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.00013150. Learning rate = 0.01\n",
      "val Loss: 0.00017270. Learning rate = 0.01\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.00013124. Learning rate = 0.01\n",
      "val Loss: 0.00017134. Learning rate = 0.01\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.00013263. Learning rate = 0.01\n",
      "val Loss: 0.00017067. Learning rate = 0.01\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.00012883. Learning rate = 0.01\n",
      "val Loss: 0.00017003. Learning rate = 0.01\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.00012963. Learning rate = 0.01\n",
      "val Loss: 0.00016981. Learning rate = 0.01\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.00012936. Learning rate = 0.01\n",
      "val Loss: 0.00017099. Learning rate = 0.01\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.00012762. Learning rate = 0.01\n",
      "val Loss: 0.00017181. Learning rate = 0.01\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.00013026. Learning rate = 0.01\n",
      "val Loss: 0.00016987. Learning rate = 0.01\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.00012759. Learning rate = 0.01\n",
      "val Loss: 0.00016933. Learning rate = 0.01\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.00012833. Learning rate = 0.01\n",
      "val Loss: 0.00017008. Learning rate = 0.01\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.00012487. Learning rate = 0.01\n",
      "val Loss: 0.00017023. Learning rate = 0.01\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.00012524. Learning rate = 0.01\n",
      "val Loss: 0.00016909. Learning rate = 0.01\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.00012993. Learning rate = 0.01\n",
      "val Loss: 0.00016790. Learning rate = 0.01\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.00012914. Learning rate = 0.01\n",
      "val Loss: 0.00016895. Learning rate = 0.01\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.00012568. Learning rate = 0.01\n",
      "val Loss: 0.00016851. Learning rate = 0.01\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.00012495. Learning rate = 0.01\n",
      "val Loss: 0.00016843. Learning rate = 0.01\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.00012411. Learning rate = 0.01\n",
      "val Loss: 0.00016833. Learning rate = 0.01\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.00012329. Learning rate = 0.01\n",
      "val Loss: 0.00016781. Learning rate = 0.01\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.00012671. Learning rate = 0.01\n",
      "val Loss: 0.00016832. Learning rate = 0.01\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.00012565. Learning rate = 0.01\n",
      "val Loss: 0.00016743. Learning rate = 0.01\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.00012606. Learning rate = 0.01\n",
      "val Loss: 0.00016788. Learning rate = 0.01\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.00012629. Learning rate = 0.01\n",
      "val Loss: 0.00016792. Learning rate = 0.01\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.00012333. Learning rate = 0.01\n",
      "val Loss: 0.00016824. Learning rate = 0.01\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.00012353. Learning rate = 0.01\n",
      "val Loss: 0.00016803. Learning rate = 0.01\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.00012208. Learning rate = 0.01\n",
      "val Loss: 0.00016794. Learning rate = 0.01\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.00012511. Learning rate = 0.01\n",
      "val Loss: 0.00016838. Learning rate = 0.01\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.00012425. Learning rate = 0.01\n",
      "val Loss: 0.00016944. Learning rate = 0.01\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.00012418. Learning rate = 0.01\n",
      "val Loss: 0.00016799. Learning rate = 0.01\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.00012541. Learning rate = 0.01\n",
      "val Loss: 0.00016777. Learning rate = 0.01\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.00012590. Learning rate = 0.01\n",
      "val Loss: 0.00016841. Learning rate = 0.01\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.00012423. Learning rate = 0.01\n",
      "val Loss: 0.00016768. Learning rate = 0.01\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.00012335. Learning rate = 0.01\n",
      "val Loss: 0.00016752. Learning rate = 0.01\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.00012505. Learning rate = 0.01\n",
      "val Loss: 0.00016764. Learning rate = 0.01\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.00012570. Learning rate = 0.01\n",
      "val Loss: 0.00016716. Learning rate = 0.01\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.00012336. Learning rate = 0.01\n",
      "val Loss: 0.00016734. Learning rate = 0.01\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.00012590. Learning rate = 0.001\n",
      "val Loss: 0.00016752. Learning rate = 0.001\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.00012314. Learning rate = 0.001\n",
      "val Loss: 0.00016739. Learning rate = 0.001\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.00012348. Learning rate = 0.001\n",
      "val Loss: 0.00016718. Learning rate = 0.001\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.00012281. Learning rate = 0.001\n",
      "val Loss: 0.00016698. Learning rate = 0.001\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.00012425. Learning rate = 0.001\n",
      "val Loss: 0.00016686. Learning rate = 0.001\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.00012398. Learning rate = 0.001\n",
      "val Loss: 0.00016689. Learning rate = 0.001\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.00012370. Learning rate = 0.001\n",
      "val Loss: 0.00016677. Learning rate = 0.001\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.00012381. Learning rate = 0.001\n",
      "val Loss: 0.00016681. Learning rate = 0.001\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.00012512. Learning rate = 0.001\n",
      "val Loss: 0.00016687. Learning rate = 0.001\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.00012205. Learning rate = 0.001\n",
      "val Loss: 0.00016689. Learning rate = 0.001\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.00012568. Learning rate = 0.001\n",
      "val Loss: 0.00016683. Learning rate = 0.001\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.00012367. Learning rate = 0.001\n",
      "val Loss: 0.00016674. Learning rate = 0.001\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.00012258. Learning rate = 0.001\n",
      "val Loss: 0.00016667. Learning rate = 0.001\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.00012393. Learning rate = 0.001\n",
      "val Loss: 0.00016663. Learning rate = 0.001\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.00012264. Learning rate = 0.001\n",
      "val Loss: 0.00016665. Learning rate = 0.001\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.00012268. Learning rate = 0.001\n",
      "val Loss: 0.00016663. Learning rate = 0.001\n",
      "Epoch 148/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00012488. Learning rate = 0.001\n",
      "val Loss: 0.00016661. Learning rate = 0.001\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.00012246. Learning rate = 0.001\n",
      "val Loss: 0.00016660. Learning rate = 0.001\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.00012455. Learning rate = 0.001\n",
      "val Loss: 0.00016662. Learning rate = 0.001\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.00012027. Learning rate = 0.001\n",
      "val Loss: 0.00016657. Learning rate = 0.001\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.00012384. Learning rate = 0.001\n",
      "val Loss: 0.00016652. Learning rate = 0.001\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.00012251. Learning rate = 0.001\n",
      "val Loss: 0.00016650. Learning rate = 0.001\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.00012234. Learning rate = 0.001\n",
      "val Loss: 0.00016649. Learning rate = 0.001\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.00012475. Learning rate = 0.001\n",
      "val Loss: 0.00016656. Learning rate = 0.001\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.00012315. Learning rate = 0.001\n",
      "val Loss: 0.00016652. Learning rate = 0.001\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.00012291. Learning rate = 0.001\n",
      "val Loss: 0.00016652. Learning rate = 0.001\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.00012295. Learning rate = 0.001\n",
      "val Loss: 0.00016651. Learning rate = 0.001\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.00012252. Learning rate = 0.001\n",
      "val Loss: 0.00016650. Learning rate = 0.001\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.00012355. Learning rate = 0.001\n",
      "val Loss: 0.00016650. Learning rate = 0.001\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.00012440. Learning rate = 0.001\n",
      "val Loss: 0.00016650. Learning rate = 0.001\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.00012259. Learning rate = 0.0001\n",
      "val Loss: 0.00016652. Learning rate = 0.0001\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.00012433. Learning rate = 0.0001\n",
      "val Loss: 0.00016651. Learning rate = 0.0001\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.00012243. Learning rate = 0.0001\n",
      "val Loss: 0.00016654. Learning rate = 0.0001\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.00012301. Learning rate = 0.0001\n",
      "val Loss: 0.00016654. Learning rate = 0.0001\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.00012311. Learning rate = 0.0001\n",
      "val Loss: 0.00016653. Learning rate = 0.0001\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.00012173. Learning rate = 0.0001\n",
      "val Loss: 0.00016652. Learning rate = 0.0001\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.00012335. Learning rate = 0.0001\n",
      "val Loss: 0.00016651. Learning rate = 0.0001\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.00012206. Learning rate = 0.0001\n",
      "val Loss: 0.00016652. Learning rate = 0.0001\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.00012413. Learning rate = 0.0001\n",
      "val Loss: 0.00016650. Learning rate = 0.0001\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.00012353. Learning rate = 0.0001\n",
      "val Loss: 0.00016648. Learning rate = 0.0001\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.00012468. Learning rate = 0.0001\n",
      "val Loss: 0.00016651. Learning rate = 0.0001\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.00012506. Learning rate = 1e-05\n",
      "val Loss: 0.00016653. Learning rate = 1e-05\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.00012368. Learning rate = 1e-05\n",
      "val Loss: 0.00016656. Learning rate = 1e-05\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.00012382. Learning rate = 1e-05\n",
      "val Loss: 0.00016661. Learning rate = 1e-05\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.00012269. Learning rate = 1e-05\n",
      "val Loss: 0.00016664. Learning rate = 1e-05\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.00012430. Learning rate = 1e-05\n",
      "val Loss: 0.00016657. Learning rate = 1e-05\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.00012060. Learning rate = 1e-05\n",
      "val Loss: 0.00016661. Learning rate = 1e-05\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.00012294. Learning rate = 1e-05\n",
      "val Loss: 0.00016662. Learning rate = 1e-05\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.00012359. Learning rate = 1e-05\n",
      "val Loss: 0.00016659. Learning rate = 1e-05\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.00012673. Learning rate = 1e-05\n",
      "val Loss: 0.00016652. Learning rate = 1e-05\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.00012348. Learning rate = 1e-05\n",
      "val Loss: 0.00016659. Learning rate = 1e-05\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.00012257. Learning rate = 1e-05\n",
      "val Loss: 0.00016671. Learning rate = 1e-05\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.00012242. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016664. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.00012340. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 186/499\n",
      "----------\n",
      "train Loss: 0.00012305. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016674. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.00012132. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016668. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.00012325. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.00012075. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.00012382. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.00012387. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.00012436. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.00012158. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.00012151. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.00011991. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.00012104. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.00012159. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.00012250. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.00012209. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.00012364. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.00012323. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.00012296. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.00012139. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.00012102. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.00012224. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.00012209. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.00012266. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.00012120. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.00012196. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.00012211. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.00012193. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 212/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00012389. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.00012634. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.00012263. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.00012397. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.00012500. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.00012053. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.00012477. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.00012272. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.00012471. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.00012254. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.00012189. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.00012172. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.00012326. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.00012161. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.00012379. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.00012444. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.00012323. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.00012172. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.00012131. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016666. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.00012260. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.00012342. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.00012289. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.00012150. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.00012165. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.00012372. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.00012410. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016684. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.00012441. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016674. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.00012434. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016664. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.00012292. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016665. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.00012441. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.00012201. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.00012356. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.00012319. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.00012207. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.00012239. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.00012175. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.00012254. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.00012469. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.00012242. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.00012297. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.00012152. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.00012484. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.00012103. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.00012284. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.00012309. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.00012409. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.00012270. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.00012326. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.00012433. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016648. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.00012485. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.00012093. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016647. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.00012240. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016648. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.00012149. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.00012352. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.00012270. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.00012337. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 268/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00012348. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.00012235. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.00012247. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.00012341. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.00012215. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.00012450. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.00012438. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.00012359. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.00012424. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.00012321. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.00012321. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 279/499\n",
      "----------\n",
      "train Loss: 0.00012343. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.00012245. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.00012319. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.00012291. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.00012229. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.00012273. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.00012379. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.00012113. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.00012405. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.00012548. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016665. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.00012312. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016666. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.00012367. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016663. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.00012355. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.00012428. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016666. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.00012292. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016670. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.00012112. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016663. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.00012267. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.00012331. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.00012469. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016689. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.00012205. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016676. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.00012384. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016665. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.00012329. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016665. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.00012186. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.00012230. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.00012043. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.00012352. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.00012336. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.00012194. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.00012649. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.00012348. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.00012255. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.00012277. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.00012267. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.00012332. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016664. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.00012405. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016669. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.00012376. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016673. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.00012314. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.00012378. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.00012480. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.00012260. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.00012340. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.00012173. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.00012241. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.00012445. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.00012347. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 324/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00012518. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.00012299. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.00012340. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.00012533. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.00012637. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.00012279. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.00012261. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.00012237. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016664. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.00012531. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016668. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.00012269. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.00012406. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.00012169. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.00012245. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.00012288. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.00012310. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.00012228. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.00012172. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.00012431. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.00012379. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.00012337. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.00012458. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016647. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.00012252. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.00012334. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.00012124. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.00012145. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.00012245. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.00012436. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.00012277. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.00012312. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.00012360. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.00012446. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.00012336. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.00012264. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.00012331. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.00012243. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.00012289. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.00012142. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.00012302. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016663. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.00012470. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.00012454. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.00012392. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.00012424. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.00012119. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.00012183. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.00012252. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.00012267. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.00012598. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.00012113. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 372/499\n",
      "----------\n",
      "train Loss: 0.00012355. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.00012237. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016665. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.00012460. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.00012301. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016663. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.00012350. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.00012209. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.00012397. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.00012363. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 380/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00012142. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.00012328. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.00012142. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.00012408. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.00012464. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.00012453. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.00012118. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.00012284. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.00012171. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.00012179. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.00012105. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.00012269. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.00012320. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.00012473. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016671. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.00012304. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016669. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.00012372. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.00012348. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.00012379. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.00012330. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.00012371. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.00012446. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.00012428. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.00012528. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.00012308. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.00012866. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.00012021. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.00012379. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.00012316. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.00012235. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.00012317. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.00012497. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.00012321. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.00012374. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.00012378. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.00012402. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.00011971. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.00012235. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.00012225. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.00012185. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.00012603. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016664. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.00012252. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016666. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.00012434. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016659. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.00012249. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.00012358. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.00012356. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.00012384. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.00012433. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.00012231. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.00012484. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.00012203. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.00012523. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.00012332. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.00012458. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.00012030. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.00012197. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.00012428. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016647. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 436/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00012374. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.00012717. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016669. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.00012225. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016663. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.00012264. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016663. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.00012100. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016662. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.00012254. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.00012278. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.00012551. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.00012399. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.00012438. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.00012273. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.00012528. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016679. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.00012550. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.00012247. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.00012593. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.00012576. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.00012072. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.00012292. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.00012320. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.00012293. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.00012467. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.00012243. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.00012474. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.00012233. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.00012280. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016652. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.00012294. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.00012319. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.00012089. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.00012353. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 465/499\n",
      "----------\n",
      "train Loss: 0.00012181. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.00012255. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.00012364. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.00012207. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.00012493. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016648. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.00012043. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.00012168. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.00012429. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016648. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.00012237. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.00012132. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.00012338. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016648. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.00012303. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016648. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.00012173. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016650. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.00012313. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016649. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.00012075. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016648. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.00012762. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.00012130. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016664. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.00012361. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.00012170. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.00012480. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.00012458. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.00012102. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016655. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.00012260. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016661. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.00012420. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016678. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.00012410. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016667. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.00012375. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016660. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.00012354. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016663. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 492/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00012514. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.00012231. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016651. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.00012273. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016653. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.00012317. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016654. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.00012458. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.00012193. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016657. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.00012401. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016656. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.00012332. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00016658. Learning rate = 1.0000000000000004e-08\n"
     ]
    }
   ],
   "source": [
    "model,loss_report = train_ae_model(net=model,data_loaders=dataloaders_train,\n",
    "                             optimizer=optimizer,loss_function=loss_function,\n",
    "                            n_epochs=epochs,scheduler=exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'train'): 0.001249286311644095,\n",
       " (0, 'val'): 0.003050928314526876,\n",
       " (1, 'train'): 0.0010928946581703644,\n",
       " (1, 'val'): 0.0024829818694679824,\n",
       " (2, 'train'): 0.0009675351557908234,\n",
       " (2, 'val'): 0.002135741627878613,\n",
       " (3, 'train'): 0.0009080959070059988,\n",
       " (3, 'val'): 0.0018160409397549098,\n",
       " (4, 'train'): 0.0009653714864894196,\n",
       " (4, 'val'): 0.0013584134479363759,\n",
       " (5, 'train'): 0.0009090929913024107,\n",
       " (5, 'val'): 0.0013835519828178265,\n",
       " (6, 'train'): 0.0008529184765561864,\n",
       " (6, 'val'): 0.0012843839272304817,\n",
       " (7, 'train'): 0.0008454506664916322,\n",
       " (7, 'val'): 0.0011583063061590547,\n",
       " (8, 'train'): 0.0007723803218040201,\n",
       " (8, 'val'): 0.0010941891620556514,\n",
       " (9, 'train'): 0.0007901015622472321,\n",
       " (9, 'val'): 0.0009912529753314124,\n",
       " (10, 'train'): 0.0007040542722852142,\n",
       " (10, 'val'): 0.0009728924681742986,\n",
       " (11, 'train'): 0.0007248849401043521,\n",
       " (11, 'val'): 0.0009006728866585979,\n",
       " (12, 'train'): 0.0006699894761873616,\n",
       " (12, 'val'): 0.0008894312023012726,\n",
       " (13, 'train'): 0.0006632362319915383,\n",
       " (13, 'val'): 0.0008764744356826499,\n",
       " (14, 'train'): 0.0006514376512280216,\n",
       " (14, 'val'): 0.0008438551039607437,\n",
       " (15, 'train'): 0.0006324992929067877,\n",
       " (15, 'val'): 0.0008157268166542053,\n",
       " (16, 'train'): 0.000582548634459575,\n",
       " (16, 'val'): 0.0008107223720462234,\n",
       " (17, 'train'): 0.0005799128018595555,\n",
       " (17, 'val'): 0.0007546582017783765,\n",
       " (18, 'train'): 0.0005687841524680456,\n",
       " (18, 'val'): 0.0007132885770665274,\n",
       " (19, 'train'): 0.0005392921674582693,\n",
       " (19, 'val'): 0.000702416800238468,\n",
       " (20, 'train'): 0.0005499256291875133,\n",
       " (20, 'val'): 0.000650662514898512,\n",
       " (21, 'train'): 0.0005283611826598644,\n",
       " (21, 'val'): 0.0006301380279991361,\n",
       " (22, 'train'): 0.0005494666596253713,\n",
       " (22, 'val'): 0.0006062489140916754,\n",
       " (23, 'train'): 0.00048059895117249753,\n",
       " (23, 'val'): 0.0006371952455352854,\n",
       " (24, 'train'): 0.0005081284756737727,\n",
       " (24, 'val'): 0.0005863815270088337,\n",
       " (25, 'train'): 0.0004682442069881492,\n",
       " (25, 'val'): 0.0005716046801319828,\n",
       " (26, 'train'): 0.0004579678815961988,\n",
       " (26, 'val'): 0.0005526068723864026,\n",
       " (27, 'train'): 0.0004607020118446262,\n",
       " (27, 'val'): 0.0005172319296333524,\n",
       " (28, 'train'): 0.0004359510278812161,\n",
       " (28, 'val'): 0.0004906460159906635,\n",
       " (29, 'train'): 0.0003892929741629848,\n",
       " (29, 'val'): 0.0004805608618038672,\n",
       " (30, 'train'): 0.00037021257190240756,\n",
       " (30, 'val'): 0.00044359473718537227,\n",
       " (31, 'train'): 0.00035769960636066063,\n",
       " (31, 'val'): 0.0004077957322200139,\n",
       " (32, 'train'): 0.00037500231216351193,\n",
       " (32, 'val'): 0.00038410492103408886,\n",
       " (33, 'train'): 0.0003226245149832081,\n",
       " (33, 'val'): 0.0003749529520670573,\n",
       " (34, 'train'): 0.00030954930655382296,\n",
       " (34, 'val'): 0.000377136524076815,\n",
       " (35, 'train'): 0.00029267615604179875,\n",
       " (35, 'val'): 0.00036302564182767163,\n",
       " (36, 'train'): 0.00029010936203930114,\n",
       " (36, 'val'): 0.00034424672938055464,\n",
       " (37, 'train'): 0.00027416414512252367,\n",
       " (37, 'val'): 0.000302561041381624,\n",
       " (38, 'train'): 0.00024866868948770896,\n",
       " (38, 'val'): 0.00029797634730736416,\n",
       " (39, 'train'): 0.000244933042537283,\n",
       " (39, 'val'): 0.00028706373025973636,\n",
       " (40, 'train'): 0.00026623642554989566,\n",
       " (40, 'val'): 0.00028167549451744116,\n",
       " (41, 'train'): 0.0002452939472816609,\n",
       " (41, 'val'): 0.00028836237335646593,\n",
       " (42, 'train'): 0.0002387554074327151,\n",
       " (42, 'val'): 0.0002855701992909114,\n",
       " (43, 'train'): 0.00022855890845810927,\n",
       " (43, 'val'): 0.00029898835001168427,\n",
       " (44, 'train'): 0.00022163331784583903,\n",
       " (44, 'val'): 0.00030482118880307234,\n",
       " (45, 'train'): 0.00023792398645094149,\n",
       " (45, 'val'): 0.0002778706106322783,\n",
       " (46, 'train'): 0.0002507292967358673,\n",
       " (46, 'val'): 0.0002590632238597782,\n",
       " (47, 'train'): 0.00023308084173886864,\n",
       " (47, 'val'): 0.0002630553035824387,\n",
       " (48, 'train'): 0.0002250815569250672,\n",
       " (48, 'val'): 0.00025632556665826725,\n",
       " (49, 'train'): 0.0002182763828723519,\n",
       " (49, 'val'): 0.00025496035124416704,\n",
       " (50, 'train'): 0.00021074946831773828,\n",
       " (50, 'val'): 0.00025064484388739975,\n",
       " (51, 'train'): 0.000207280110636795,\n",
       " (51, 'val'): 0.00024838659360452937,\n",
       " (52, 'train'): 0.0001997479530810206,\n",
       " (52, 'val'): 0.00024813198036065806,\n",
       " (53, 'train'): 0.0002011370250127382,\n",
       " (53, 'val'): 0.0002441687371443819,\n",
       " (54, 'train'): 0.00019907663766018772,\n",
       " (54, 'val'): 0.00023844355234393366,\n",
       " (55, 'train'): 0.00020531824513993883,\n",
       " (55, 'val'): 0.00023807366206138222,\n",
       " (56, 'train'): 0.00019927860099684308,\n",
       " (56, 'val'): 0.00023969937184894525,\n",
       " (57, 'train'): 0.00020448638005527082,\n",
       " (57, 'val'): 0.00023266038408985845,\n",
       " (58, 'train'): 0.00020116456370386813,\n",
       " (58, 'val'): 0.00022845680790918845,\n",
       " (59, 'train'): 0.0001965348600168471,\n",
       " (59, 'val'): 0.00022551332841868753,\n",
       " (60, 'train'): 0.00019130965315357403,\n",
       " (60, 'val'): 0.00022543954697472077,\n",
       " (61, 'train'): 0.00018277961364084923,\n",
       " (61, 'val'): 0.000222240022763058,\n",
       " (62, 'train'): 0.00018403167336213368,\n",
       " (62, 'val'): 0.00021856631738720116,\n",
       " (63, 'train'): 0.00018874286777443357,\n",
       " (63, 'val'): 0.00021376071014889963,\n",
       " (64, 'train'): 0.00017661192557877966,\n",
       " (64, 'val'): 0.00021474915384142487,\n",
       " (65, 'train'): 0.00018026986745772538,\n",
       " (65, 'val'): 0.00021022058057564277,\n",
       " (66, 'train'): 0.00017439830116927624,\n",
       " (66, 'val'): 0.00021159581633077728,\n",
       " (67, 'train'): 0.00017438537906855345,\n",
       " (67, 'val'): 0.0002099035514725579,\n",
       " (68, 'train'): 0.0001733278881551491,\n",
       " (68, 'val'): 0.00021272473451164033,\n",
       " (69, 'train'): 0.00017121684289088957,\n",
       " (69, 'val'): 0.00020718064021181177,\n",
       " (70, 'train'): 0.00016879190311387733,\n",
       " (70, 'val'): 0.000206142439748402,\n",
       " (71, 'train'): 0.00017284923578026119,\n",
       " (71, 'val'): 0.0001994828969516136,\n",
       " (72, 'train'): 0.00016539709435568916,\n",
       " (72, 'val'): 0.00019645835790369246,\n",
       " (73, 'train'): 0.00016197886455942084,\n",
       " (73, 'val'): 0.00020135461387258988,\n",
       " (74, 'train'): 0.00016383249830040667,\n",
       " (74, 'val'): 0.00019560440408962745,\n",
       " (75, 'train'): 0.00016233222386627285,\n",
       " (75, 'val'): 0.0001924089845959787,\n",
       " (76, 'train'): 0.00015573914990656904,\n",
       " (76, 'val'): 0.0001937839444036837,\n",
       " (77, 'train'): 0.00015734141278598044,\n",
       " (77, 'val'): 0.000189680192205641,\n",
       " (78, 'train'): 0.00015724480962725702,\n",
       " (78, 'val'): 0.00019271830441775146,\n",
       " (79, 'train'): 0.00015704062146445116,\n",
       " (79, 'val'): 0.000189538958861872,\n",
       " (80, 'train'): 0.00015207295547480936,\n",
       " (80, 'val'): 0.00018894988008671335,\n",
       " (81, 'train'): 0.00015122764657630964,\n",
       " (81, 'val'): 0.0001877257945360961,\n",
       " (82, 'train'): 0.0001511630705661244,\n",
       " (82, 'val'): 0.00018618279999053038,\n",
       " (83, 'train'): 0.00014545626213981045,\n",
       " (83, 'val'): 0.0001835406550930606,\n",
       " (84, 'train'): 0.00014420778973510973,\n",
       " (84, 'val'): 0.00018191572140764306,\n",
       " (85, 'train'): 0.00014355213864258043,\n",
       " (85, 'val'): 0.00018165239857302772,\n",
       " (86, 'train'): 0.00014652254025417346,\n",
       " (86, 'val'): 0.00017865626486363235,\n",
       " (87, 'train'): 0.0001402295203189607,\n",
       " (87, 'val'): 0.0001778272153050811,\n",
       " (88, 'train'): 0.0001408234454208502,\n",
       " (88, 'val'): 0.00017883340586666708,\n",
       " (89, 'train'): 0.00014116788386470743,\n",
       " (89, 'val'): 0.0001784719837208589,\n",
       " (90, 'train'): 0.00013810898505013297,\n",
       " (90, 'val'): 0.00017759605759271868,\n",
       " (91, 'train'): 0.0001354039025803407,\n",
       " (91, 'val'): 0.0001756435570617517,\n",
       " (92, 'train'): 0.00013739146568157053,\n",
       " (92, 'val'): 0.00017687136790266745,\n",
       " (93, 'train'): 0.00013336245212014074,\n",
       " (93, 'val'): 0.00017391383233997558,\n",
       " (94, 'train'): 0.00013358854361016442,\n",
       " (94, 'val'): 0.00017318605548805662,\n",
       " (95, 'train'): 0.0001299359611477013,\n",
       " (95, 'val'): 0.00017320042200110578,\n",
       " (96, 'train'): 0.00013706730506210415,\n",
       " (96, 'val'): 0.0001717006304749736,\n",
       " (97, 'train'): 0.00013149899637533558,\n",
       " (97, 'val'): 0.00017269809419910112,\n",
       " (98, 'train'): 0.00013123967477844821,\n",
       " (98, 'val'): 0.00017134084676702818,\n",
       " (99, 'train'): 0.0001326330454537162,\n",
       " (99, 'val'): 0.000170669742618446,\n",
       " (100, 'train'): 0.00012882607040443905,\n",
       " (100, 'val'): 0.00017003002748997123,\n",
       " (101, 'train'): 0.00012962638693689196,\n",
       " (101, 'val'): 0.00016980628586477705,\n",
       " (102, 'train'): 0.00012936427137228074,\n",
       " (102, 'val'): 0.0001709924976306933,\n",
       " (103, 'train'): 0.00012762370277886038,\n",
       " (103, 'val'): 0.00017181351022035987,\n",
       " (104, 'train'): 0.0001302608073240629,\n",
       " (104, 'val'): 0.000169872323534003,\n",
       " (105, 'train'): 0.00012759272767989724,\n",
       " (105, 'val'): 0.0001693262063242771,\n",
       " (106, 'train'): 0.00012832623906433582,\n",
       " (106, 'val'): 0.00017008482030144445,\n",
       " (107, 'train'): 0.0001248719334533369,\n",
       " (107, 'val'): 0.00017023447004181367,\n",
       " (108, 'train'): 0.00012523804670544687,\n",
       " (108, 'val'): 0.0001690893571961809,\n",
       " (109, 'train'): 0.0001299330248945841,\n",
       " (109, 'val'): 0.00016790064465668466,\n",
       " (110, 'train'): 0.00012914075395437303,\n",
       " (110, 'val'): 0.00016895100405370749,\n",
       " (111, 'train'): 0.0001256824859107534,\n",
       " (111, 'val'): 0.00016850683217247328,\n",
       " (112, 'train'): 0.00012494544326155274,\n",
       " (112, 'val'): 0.0001684326540540766,\n",
       " (113, 'train'): 0.00012411232779009474,\n",
       " (113, 'val'): 0.00016833039828472666,\n",
       " (114, 'train'): 0.0001232910220062843,\n",
       " (114, 'val'): 0.00016781463529224748,\n",
       " (115, 'train'): 0.00012670954499669648,\n",
       " (115, 'val'): 0.00016831613525196358,\n",
       " (116, 'train'): 0.00012564821668935042,\n",
       " (116, 'val'): 0.0001674294644207866,\n",
       " (117, 'train'): 0.00012605846859514713,\n",
       " (117, 'val'): 0.00016787725811203322,\n",
       " (118, 'train'): 0.0001262929204299494,\n",
       " (118, 'val'): 0.00016791747744988511,\n",
       " (119, 'train'): 0.0001233333626900006,\n",
       " (119, 'val'): 0.00016824102581099228,\n",
       " (120, 'train'): 0.00012352503404987079,\n",
       " (120, 'val'): 0.0001680299777675558,\n",
       " (121, 'train'): 0.00012208283361461427,\n",
       " (121, 'val'): 0.00016794248518568498,\n",
       " (122, 'train'): 0.00012511061935651082,\n",
       " (122, 'val'): 0.00016838179349347397,\n",
       " (123, 'train'): 0.0001242484862881678,\n",
       " (123, 'val'): 0.00016944315629424873,\n",
       " (124, 'train'): 0.00012417536021934616,\n",
       " (124, 'val'): 0.00016798948248227438,\n",
       " (125, 'train'): 0.00012540822003588632,\n",
       " (125, 'val'): 0.0001677672585679425,\n",
       " (126, 'train'): 0.0001258990572144588,\n",
       " (126, 'val'): 0.00016841111290786002,\n",
       " (127, 'train'): 0.00012423430517729785,\n",
       " (127, 'val'): 0.00016768228400636603,\n",
       " (128, 'train'): 0.0001233541837858933,\n",
       " (128, 'val'): 0.00016752444207668304,\n",
       " (129, 'train'): 0.00012504687981197126,\n",
       " (129, 'val'): 0.00016763844285850172,\n",
       " (130, 'train'): 0.00012570259126800078,\n",
       " (130, 'val'): 0.00016715846679828786,\n",
       " (131, 'train'): 0.00012336214314456338,\n",
       " (131, 'val'): 0.00016733960903905057,\n",
       " (132, 'train'): 0.0001258998936801045,\n",
       " (132, 'val'): 0.00016751581871951067,\n",
       " (133, 'train'): 0.0001231449207773915,\n",
       " (133, 'val'): 0.00016738607168749527,\n",
       " (134, 'train'): 0.00012347622153659663,\n",
       " (134, 'val'): 0.00016718016316493353,\n",
       " (135, 'train'): 0.00012281279648757644,\n",
       " (135, 'val'): 0.00016697984257781946,\n",
       " (136, 'train'): 0.0001242475506539146,\n",
       " (136, 'val'): 0.00016685825324168913,\n",
       " (137, 'train'): 0.0001239797911020341,\n",
       " (137, 'val'): 0.00016688510637592386,\n",
       " (138, 'train'): 0.0001236991568778952,\n",
       " (138, 'val'): 0.00016676677666880465,\n",
       " (139, 'train'): 0.00012381329132175005,\n",
       " (139, 'val'): 0.00016681346352453584,\n",
       " (140, 'train'): 0.00012511528459274105,\n",
       " (140, 'val'): 0.00016687080884973207,\n",
       " (141, 'train'): 0.00012205090563468359,\n",
       " (141, 'val'): 0.00016688603769849847,\n",
       " (142, 'train'): 0.00012567650130087578,\n",
       " (142, 'val'): 0.0001668277438040133,\n",
       " (143, 'train'): 0.00012366974260658026,\n",
       " (143, 'val'): 0.00016674464913430038,\n",
       " (144, 'train'): 0.00012257813338052343,\n",
       " (144, 'val'): 0.00016667216119390946,\n",
       " (145, 'train'): 0.000123927706024713,\n",
       " (145, 'val'): 0.0001666348048106388,\n",
       " (146, 'train'): 0.00012264147625063305,\n",
       " (146, 'val'): 0.00016665006815283387,\n",
       " (147, 'train'): 0.00012267792286972204,\n",
       " (147, 'val'): 0.00016663444262963754,\n",
       " (148, 'train'): 0.00012487632705381624,\n",
       " (148, 'val'): 0.0001666064339655417,\n",
       " (149, 'train'): 0.00012245580243567625,\n",
       " (149, 'val'): 0.00016660286389567233,\n",
       " (150, 'train'): 0.00012454687600472458,\n",
       " (150, 'val'): 0.00016661740287586495,\n",
       " (151, 'train'): 0.00012026953993848076,\n",
       " (151, 'val'): 0.0001665733547674285,\n",
       " (152, 'train'): 0.00012383718664447466,\n",
       " (152, 'val'): 0.0001665198382128168,\n",
       " (153, 'train'): 0.00012251340646158765,\n",
       " (153, 'val'): 0.00016649657239516577,\n",
       " (154, 'train'): 0.00012234315552093363,\n",
       " (154, 'val'): 0.00016649322653258287,\n",
       " (155, 'train'): 0.0001247510857259234,\n",
       " (155, 'val'): 0.0001665605921988134,\n",
       " (156, 'train'): 0.0001231455157890364,\n",
       " (156, 'val'): 0.00016651666481737738,\n",
       " (157, 'train'): 0.00012290911507551318,\n",
       " (157, 'val'): 0.00016652447757897553,\n",
       " (158, 'train'): 0.000122954784375098,\n",
       " (158, 'val'): 0.00016651256009936333,\n",
       " (159, 'train'): 0.0001225225644669047,\n",
       " (159, 'val'): 0.000166498435040315,\n",
       " (160, 'train'): 0.00012355456904818615,\n",
       " (160, 'val'): 0.00016649734849731127,\n",
       " (161, 'train'): 0.0001243972736928198,\n",
       " (161, 'val'): 0.0001664960894871641,\n",
       " (162, 'train'): 0.00012258699819169664,\n",
       " (162, 'val'): 0.0001665213386769648,\n",
       " (163, 'train'): 0.0001243316801264882,\n",
       " (163, 'val'): 0.00016650895553606528,\n",
       " (164, 'train'): 0.0001224266080599692,\n",
       " (164, 'val'): 0.00016653817147016525,\n",
       " (165, 'train'): 0.00012301271608858197,\n",
       " (165, 'val'): 0.0001665435179516121,\n",
       " (166, 'train'): 0.0001231140922755003,\n",
       " (166, 'val'): 0.000166533256156577,\n",
       " (167, 'train'): 0.00012173246661270107,\n",
       " (167, 'val'): 0.00016651750990638025,\n",
       " (168, 'train'): 0.00012335241599767296,\n",
       " (168, 'val'): 0.00016651193921764693,\n",
       " (169, 'train'): 0.00012206033527575157,\n",
       " (169, 'val'): 0.00016652001067996025,\n",
       " (170, 'train'): 0.00012413352831370302,\n",
       " (170, 'val'): 0.0001665013324883249,\n",
       " (171, 'train'): 0.00012352574116515893,\n",
       " (171, 'val'): 0.0001664774285422431,\n",
       " (172, 'train'): 0.00012468146936347088,\n",
       " (172, 'val'): 0.00016651214617821906,\n",
       " (173, 'train'): 0.00012506097037759092,\n",
       " (173, 'val'): 0.00016653075538299703,\n",
       " (174, 'train'): 0.00012367789599078673,\n",
       " (174, 'val'): 0.0001665626790512491,\n",
       " (175, 'train'): 0.00012382015551405925,\n",
       " (175, 'val'): 0.0001666098660616963,\n",
       " (176, 'train'): 0.00012269152621566145,\n",
       " (176, 'val'): 0.00016664329019409638,\n",
       " (177, 'train'): 0.00012429574659715095,\n",
       " (177, 'val'): 0.00016656888786841323,\n",
       " (178, 'train'): 0.00012060386749605338,\n",
       " (178, 'val'): 0.00016661283249656358,\n",
       " (179, 'train'): 0.00012294301349255774,\n",
       " (179, 'val'): 0.00016662014510344575,\n",
       " (180, 'train'): 0.0001235926196117092,\n",
       " (180, 'val'): 0.00016658861810962358,\n",
       " (181, 'train'): 0.0001267259552453955,\n",
       " (181, 'val'): 0.00016651790658081018,\n",
       " (182, 'train'): 0.00012347731670295752,\n",
       " (182, 'val'): 0.0001665880834614789,\n",
       " (183, 'train'): 0.00012256992394449535,\n",
       " (183, 'val'): 0.00016670894843560677,\n",
       " (184, 'train'): 0.00012241786828747503,\n",
       " (184, 'val'): 0.00016664263481895128,\n",
       " (185, 'train'): 0.00012340153464012675,\n",
       " (185, 'val'): 0.00016659272282763764,\n",
       " (186, 'train'): 0.000123046213519518,\n",
       " (186, 'val'): 0.00016673635346470055,\n",
       " (187, 'train'): 0.00012132069268436344,\n",
       " (187, 'val'): 0.00016668078455108184,\n",
       " (188, 'train'): 0.00012324821997295926,\n",
       " (188, 'val'): 0.0001665876178001916,\n",
       " (189, 'train'): 0.00012074594592882527,\n",
       " (189, 'val'): 0.00016654881269291595,\n",
       " (190, 'train'): 0.000123824419764181,\n",
       " (190, 'val'): 0.00016651816528152536,\n",
       " (191, 'train'): 0.0001238677564456507,\n",
       " (191, 'val'): 0.00016653175569242902,\n",
       " (192, 'train'): 0.00012435951201176203,\n",
       " (192, 'val'): 0.000166577631952586,\n",
       " (193, 'train'): 0.00012157647008145297,\n",
       " (193, 'val'): 0.00016655083055849428,\n",
       " (194, 'train'): 0.00012150661226499963,\n",
       " (194, 'val'): 0.00016652221825939637,\n",
       " (195, 'train'): 0.00011990638449788094,\n",
       " (195, 'val'): 0.00016651766512680937,\n",
       " (196, 'train'): 0.00012103811820486078,\n",
       " (196, 'val'): 0.00016658829042205103,\n",
       " (197, 'train'): 0.00012159424282058521,\n",
       " (197, 'val'): 0.00016661583342485956,\n",
       " (198, 'train'): 0.0001225002903353285,\n",
       " (198, 'val'): 0.00016659931107251732,\n",
       " (199, 'train'): 0.0001220947122891192,\n",
       " (199, 'val'): 0.00016659380937064135,\n",
       " (200, 'train'): 0.00012363908226015392,\n",
       " (200, 'val'): 0.00016657371694842973,\n",
       " (201, 'train'): 0.00012322919253535843,\n",
       " (201, 'val'): 0.00016656478315039917,\n",
       " (202, 'train'): 0.00012295579330788719,\n",
       " (202, 'val'): 0.0001665975346609398,\n",
       " (203, 'train'): 0.00012138649321126718,\n",
       " (203, 'val'): 0.00016657618322858103,\n",
       " (204, 'train'): 0.00012102195372184117,\n",
       " (204, 'val'): 0.00016654060325688787,\n",
       " (205, 'train'): 0.000122241813827444,\n",
       " (205, 'val'): 0.00016652982406042242,\n",
       " (206, 'train'): 0.0001220922546323251,\n",
       " (206, 'val'): 0.00016653427371272333,\n",
       " (207, 'train'): 0.00012265717938404392,\n",
       " (207, 'val'): 0.00016657570032057938,\n",
       " (208, 'train'): 0.00012119597306957951,\n",
       " (208, 'val'): 0.00016653232483400239,\n",
       " (209, 'train'): 0.00012196387440242149,\n",
       " (209, 'val'): 0.0001665526069700718,\n",
       " (210, 'train'): 0.00012211200643192838,\n",
       " (210, 'val'): 0.00016656469691682746,\n",
       " (211, 'train'): 0.00012193261904435026,\n",
       " (211, 'val'): 0.00016652459830597595,\n",
       " (212, 'train'): 0.00012388993140861944,\n",
       " (212, 'val'): 0.00016653251454786018,\n",
       " (213, 'train'): 0.00012633701596685029,\n",
       " (213, 'val'): 0.00016654234517503668,\n",
       " (214, 'train'): 0.00012262688121861883,\n",
       " (214, 'val'): 0.00016652896172470518,\n",
       " (215, 'train'): 0.00012396525643352006,\n",
       " (215, 'val'): 0.00016650836914777756,\n",
       " (216, 'train'): 0.0001249954716681882,\n",
       " (216, 'val'): 0.00016651407781022567,\n",
       " (217, 'train'): 0.00012052908574265462,\n",
       " (217, 'val'): 0.00016649622746087885,\n",
       " (218, 'train'): 0.00012476999674820236,\n",
       " (218, 'val'): 0.00016652854780356088,\n",
       " (219, 'train'): 0.0001227222167438379,\n",
       " (219, 'val'): 0.00016656052321195602,\n",
       " (220, 'train'): 0.00012471371640761694,\n",
       " (220, 'val'): 0.00016652756474084326,\n",
       " (221, 'train'): 0.00012254477392330214,\n",
       " (221, 'val'): 0.00016659443025235777,\n",
       " (222, 'train'): 0.00012188697130315834,\n",
       " (222, 'val'): 0.00016654843326520036,\n",
       " (223, 'train'): 0.00012171765168507893,\n",
       " (223, 'val'): 0.00016655017518334917,\n",
       " (224, 'train'): 0.00012325629143527262,\n",
       " (224, 'val'): 0.00016656338616653725,\n",
       " (225, 'train'): 0.00012160695796073587,\n",
       " (225, 'val'): 0.00016656787031226687,\n",
       " (226, 'train'): 0.0001237859682145494,\n",
       " (226, 'val'): 0.0001665707332668481,\n",
       " (227, 'train'): 0.00012443626420227465,\n",
       " (227, 'val'): 0.0001666141432468538,\n",
       " (228, 'train'): 0.0001232298306637892,\n",
       " (228, 'val'): 0.00016660691687354334,\n",
       " (229, 'train'): 0.00012172085957394706,\n",
       " (229, 'val'): 0.00016662318052517044,\n",
       " (230, 'train'): 0.00012130758949314002,\n",
       " (230, 'val'): 0.00016666405523816744,\n",
       " (231, 'train'): 0.00012259803608887724,\n",
       " (231, 'val'): 0.00016662400836745897,\n",
       " (232, 'train'): 0.00012342132093315876,\n",
       " (232, 'val'): 0.00016656228237681917,\n",
       " (233, 'train'): 0.00012289351542238837,\n",
       " (233, 'val'): 0.0001665142847707978,\n",
       " (234, 'train'): 0.0001215029688965943,\n",
       " (234, 'val'): 0.0001665659904204033,\n",
       " (235, 'train'): 0.00012165291614278599,\n",
       " (235, 'val'): 0.00016653877510516732,\n",
       " (236, 'train'): 0.00012372194409922317,\n",
       " (236, 'val'): 0.00016658528949375505,\n",
       " (237, 'train'): 0.0001241021996570958,\n",
       " (237, 'val'): 0.00016683609121375613,\n",
       " (238, 'train'): 0.0001244147489261296,\n",
       " (238, 'val'): 0.00016674456290072866,\n",
       " (239, 'train'): 0.00012434089418362687,\n",
       " (239, 'val'): 0.0001666419104569488,\n",
       " (240, 'train'): 0.00012291502207517624,\n",
       " (240, 'val'): 0.0001666496542316896,\n",
       " (241, 'train'): 0.00012440633252952939,\n",
       " (241, 'val'): 0.00016660155314538215,\n",
       " (242, 'train'): 0.00012201254463030232,\n",
       " (242, 'val'): 0.00016656566273283075,\n",
       " (243, 'train'): 0.000123558363325342,\n",
       " (243, 'val'): 0.00016659356791664054,\n",
       " (244, 'train'): 0.0001231896760011161,\n",
       " (244, 'val'): 0.00016659386111078439,\n",
       " (245, 'train'): 0.00012207276153343696,\n",
       " (245, 'val'): 0.0001666180410042957,\n",
       " (246, 'train'): 0.00012239461971653832,\n",
       " (246, 'val'): 0.00016658042592030984,\n",
       " (247, 'train'): 0.00012175181311451727,\n",
       " (247, 'val'): 0.0001665588157872359,\n",
       " (248, 'train'): 0.00012254414441822855,\n",
       " (248, 'val'): 0.000166508041460205,\n",
       " (249, 'train'): 0.00012468522052384086,\n",
       " (249, 'val'): 0.0001665883594089084,\n",
       " (250, 'train'): 0.0001224185279742987,\n",
       " (250, 'val'): 0.00016656181671553187,\n",
       " (251, 'train'): 0.00012296975883482783,\n",
       " (251, 'val'): 0.00016660313984310186,\n",
       " (252, 'train'): 0.00012152288885166247,\n",
       " (252, 'val'): 0.00016651716497209336,\n",
       " (253, 'train'): 0.00012484208801416335,\n",
       " (253, 'val'): 0.0001664887596335676,\n",
       " (254, 'train'): 0.00012103027526151252,\n",
       " (254, 'val'): 0.00016651920008438605,\n",
       " (255, 'train'): 0.00012283733856208897,\n",
       " (255, 'val'): 0.00016649227796329392,\n",
       " (256, 'train'): 0.00012308717015440816,\n",
       " (256, 'val'): 0.00016649622746087885,\n",
       " (257, 'train'): 0.0001240889671155148,\n",
       " (257, 'val'): 0.00016649888345488794,\n",
       " (258, 'train'): 0.00012269698480075157,\n",
       " (258, 'val'): 0.0001665161819093757,\n",
       " (259, 'train'): 0.000123260163322643,\n",
       " (259, 'val'): 0.0001665240809045456,\n",
       " (260, 'train'): 0.00012433253383884826,\n",
       " (260, 'val'): 0.00016647979134210833,\n",
       " (261, 'train'): 0.00012484631345917782,\n",
       " (261, 'val'): 0.0001665106629607854,\n",
       " (262, 'train'): 0.00012093025725334883,\n",
       " (262, 'val'): 0.00016647078855722039,\n",
       " (263, 'train'): 0.00012240044479430826,\n",
       " (263, 'val'): 0.00016648327517840598,\n",
       " (264, 'train'): 0.00012149096518341039,\n",
       " (264, 'val'): 0.0001665136638890814,\n",
       " (265, 'train'): 0.00012352162351210913,\n",
       " (265, 'val'): 0.00016651507811965766,\n",
       " (266, 'train'): 0.00012269563955703266,\n",
       " (266, 'val'): 0.0001665131637343654,\n",
       " (267, 'train'): 0.0001233657606428972,\n",
       " (267, 'val'): 0.00016650993859878294,\n",
       " (268, 'train'): 0.00012348382733762264,\n",
       " (268, 'val'): 0.00016654105167146081,\n",
       " (269, 'train'): 0.00012235006283002872,\n",
       " (269, 'val'): 0.0001665760107614376,\n",
       " (270, 'train'): 0.00012247479969152697,\n",
       " (270, 'val'): 0.00016656091988638595,\n",
       " (271, 'train'): 0.00012340953711558272,\n",
       " (271, 'val'): 0.00016653605012430085,\n",
       " (272, 'train'): 0.00012214774593572925,\n",
       " (272, 'val'): 0.00016651928631795777,\n",
       " (273, 'train'): 0.00012449565757479932,\n",
       " (273, 'val'): 0.00016650048739932203,\n",
       " (274, 'train'): 0.00012438041502954784,\n",
       " (274, 'val'): 0.00016659613767707788,\n",
       " (275, 'train'): 0.00012359360698610544,\n",
       " (275, 'val'): 0.00016655857433323508,\n",
       " (276, 'train'): 0.00012424491621829844,\n",
       " (276, 'val'): 0.00016657695933072655,\n",
       " (277, 'train'): 0.00012321207948304987,\n",
       " (277, 'val'): 0.00016654331099103998,\n",
       " (278, 'train'): 0.0001232062716519943,\n",
       " (278, 'val'): 0.00016651728569909378,\n",
       " (279, 'train'): 0.00012342518419717198,\n",
       " (279, 'val'): 0.00016651788933409585,\n",
       " (280, 'train'): 0.00012245468139924384,\n",
       " (280, 'val'): 0.0001665343771930094,\n",
       " (281, 'train'): 0.0001231902925711539,\n",
       " (281, 'val'): 0.00016652268392068368,\n",
       " (282, 'train'): 0.00012291080956519754,\n",
       " (282, 'val'): 0.0001665149401459429,\n",
       " (283, 'train'): 0.00012228524967752122,\n",
       " (283, 'val'): 0.00016654627742590728,\n",
       " (284, 'train'): 0.0001227256833334212,\n",
       " (284, 'val'): 0.00016656757711812302,\n",
       " (285, 'train'): 0.00012378731345826828,\n",
       " (285, 'val'): 0.0001665651798248291,\n",
       " (286, 'train'): 0.00012112781836616773,\n",
       " (286, 'val'): 0.000166561971935961,\n",
       " (287, 'train'): 0.00012405000678781007,\n",
       " (287, 'val'): 0.0001666000354345198,\n",
       " (288, 'train'): 0.00012548077696313462,\n",
       " (288, 'val'): 0.00016665215500526957,\n",
       " (289, 'train'): 0.000123117304476047,\n",
       " (289, 'val'): 0.00016666448640602607,\n",
       " (290, 'train'): 0.00012366515066888597,\n",
       " (290, 'val'): 0.0001666348048106388,\n",
       " (291, 'train'): 0.00012355005040902783,\n",
       " (291, 'val'): 0.0001666115217462734,\n",
       " (292, 'train'): 0.00012427820668866238,\n",
       " (292, 'val'): 0.00016666336536959366,\n",
       " (293, 'train'): 0.00012291665188968182,\n",
       " (293, 'val'): 0.00016669701370928023,\n",
       " (294, 'train'): 0.0001211214629519317,\n",
       " (294, 'val'): 0.0001666339597216359,\n",
       " (295, 'train'): 0.0001226658286112878,\n",
       " (295, 'val'): 0.00016654819181119955,\n",
       " (296, 'train'): 0.00012331451203122183,\n",
       " (296, 'val'): 0.00016658151246331357,\n",
       " (297, 'train'): 0.00012468612166466537,\n",
       " (297, 'val'): 0.0001668915911405175,\n",
       " (298, 'train'): 0.00012204730969474272,\n",
       " (298, 'val'): 0.0001667555663044806,\n",
       " (299, 'train'): 0.00012384278320327953,\n",
       " (299, 'val'): 0.0001666499991659765,\n",
       " (300, 'train'): 0.00012328641713355427,\n",
       " (300, 'val'): 0.0001666543970781344,\n",
       " (301, 'train'): 0.00012186406766650853,\n",
       " (301, 'val'): 0.0001665868416980461,\n",
       " (302, 'train'): 0.0001223011295897541,\n",
       " (302, 'val'): 0.00016659087742920274,\n",
       " (303, 'train'): 0.00012043340959482723,\n",
       " (303, 'val'): 0.0001665741998564314,\n",
       " (304, 'train'): 0.00012352080429317775,\n",
       " (304, 'val'): 0.00016653991338831407,\n",
       " (305, 'train'): 0.00012335704242879593,\n",
       " (305, 'val'): 0.00016658625530975837,\n",
       " (306, 'train'): 0.00012193909087390812,\n",
       " (306, 'val'): 0.00016651918283767172,\n",
       " (307, 'train'): 0.00012649368511995785,\n",
       " (307, 'val'): 0.0001665556941319395,\n",
       " (308, 'train'): 0.00012347763576717289,\n",
       " (308, 'val'): 0.00016656135105424456,\n",
       " (309, 'train'): 0.000122546692620273,\n",
       " (309, 'val'): 0.00016653248005443148,\n",
       " (310, 'train'): 0.00012277328857669123,\n",
       " (310, 'val'): 0.00016657459653086134,\n",
       " (311, 'train'): 0.0001226689804483343,\n",
       " (311, 'val'): 0.0001666170751882924,\n",
       " (312, 'train'): 0.0001233165255851216,\n",
       " (312, 'val'): 0.00016663618454778635,\n",
       " (313, 'train'): 0.00012404785094851696,\n",
       " (313, 'val'): 0.00016668609653910002,\n",
       " (314, 'train'): 0.00012375564849073135,\n",
       " (314, 'val'): 0.00016672840272938765,\n",
       " (315, 'train'): 0.00012314312280742106,\n",
       " (315, 'val'): 0.00016661145275941603,\n",
       " (316, 'train'): 0.00012377909108720445,\n",
       " (316, 'val'): 0.00016655360727950379,\n",
       " (317, 'train'): 0.00012479642733793567,\n",
       " (317, 'val'): 0.00016658201261802956,\n",
       " (318, 'train'): 0.00012260133883467428,\n",
       " (318, 'val'): 0.00016655455584879274,\n",
       " (319, 'train'): 0.00012339854664686654,\n",
       " (319, 'val'): 0.0001665334113770061,\n",
       " (320, 'train'): 0.0001217295217362267,\n",
       " (320, 'val'): 0.00016657037108584686,\n",
       " (321, 'train'): 0.00012241408263367635,\n",
       " (321, 'val'): 0.00016655319335835952,\n",
       " (322, 'train'): 0.00012444864734317417,\n",
       " (322, 'val'): 0.00016652111446967832,\n",
       " (323, 'train'): 0.00012347353536083742,\n",
       " (323, 'val'): 0.00016654569103761955,\n",
       " (324, 'train'): 0.00012517663546734385,\n",
       " (324, 'val'): 0.00016657131965513582,\n",
       " (325, 'train'): 0.00012299018756796917,\n",
       " (325, 'val'): 0.00016662238717631058,\n",
       " (326, 'train'): 0.00012339673574186035,\n",
       " (326, 'val'): 0.0001665506063512078,\n",
       " (327, 'train'): 0.00012532663876535716,\n",
       " (327, 'val'): 0.00016653265252157494,\n",
       " (328, 'train'): 0.00012636653371845131,\n",
       " (328, 'val'): 0.0001665646796701131,\n",
       " (329, 'train'): 0.00012278716787006015,\n",
       " (329, 'val'): 0.0001665180618012393,\n",
       " (330, 'train'): 0.00012261395911789604,\n",
       " (330, 'val'): 0.0001665355672162992,\n",
       " (331, 'train'): 0.00012237059073177752,\n",
       " (331, 'val'): 0.0001666423761182361,\n",
       " (332, 'train'): 0.00012531059500933797,\n",
       " (332, 'val'): 0.00016668059483722404,\n",
       " (333, 'train'): 0.00012269449696220732,\n",
       " (333, 'val'): 0.0001665949821472168,\n",
       " (334, 'train'): 0.00012406335574471287,\n",
       " (334, 'val'): 0.00016657428609000312,\n",
       " (335, 'train'): 0.00012169216104127743,\n",
       " (335, 'val'): 0.00016653523952872665,\n",
       " (336, 'train'): 0.00012244621326250058,\n",
       " (336, 'val'): 0.00016653075538299703,\n",
       " (337, 'train'): 0.00012288154189095453,\n",
       " (337, 'val'): 0.00016656964672384438,\n",
       " (338, 'train'): 0.0001231011787981347,\n",
       " (338, 'val'): 0.00016654005136202882,\n",
       " (339, 'train'): 0.00012227849327717667,\n",
       " (339, 'val'): 0.00016653401501200819,\n",
       " (340, 'train'): 0.0001217226230504888,\n",
       " (340, 'val'): 0.00016653793001616442,\n",
       " (341, 'train'): 0.0001243122473911003,\n",
       " (341, 'val'): 0.00016662054177787568,\n",
       " (342, 'train'): 0.00012378708062762463,\n",
       " (342, 'val'): 0.0001665457082843339,\n",
       " (343, 'train'): 0.00012337052935941351,\n",
       " (343, 'val'): 0.000166500987554038,\n",
       " (344, 'train'): 0.0001245776743248657,\n",
       " (344, 'val'): 0.00016647139219222245,\n",
       " (345, 'train'): 0.0001225182915934258,\n",
       " (345, 'val'): 0.0001665231840753997,\n",
       " (346, 'train'): 0.0001233352986336858,\n",
       " (346, 'val'): 0.00016648896659413973,\n",
       " (347, 'train'): 0.00012124249176984584,\n",
       " (347, 'val'): 0.00016650816218720542,\n",
       " (348, 'train'): 0.00012144712403554607,\n",
       " (348, 'val'): 0.0001665015222021827,\n",
       " (349, 'train'): 0.00012244781289525605,\n",
       " (349, 'val'): 0.00016649486497044563,\n",
       " (350, 'train'): 0.0001243577356001845,\n",
       " (350, 'val'): 0.00016649745197759735,\n",
       " (351, 'train'): 0.0001227743320029091,\n",
       " (351, 'val'): 0.000166522149272539,\n",
       " (352, 'train'): 0.00012311739070961872,\n",
       " (352, 'val'): 0.00016652240797325417,\n",
       " (353, 'train'): 0.00012359544376118315,\n",
       " (353, 'val'): 0.00016653691246001809,\n",
       " (354, 'train'): 0.000124464923929837,\n",
       " (354, 'val'): 0.00016655046837749305,\n",
       " (355, 'train'): 0.00012336454906121448,\n",
       " (355, 'val'): 0.0001665297895669937,\n",
       " (356, 'train'): 0.00012264184274331287,\n",
       " (356, 'val'): 0.00016655083055849428,\n",
       " (357, 'train'): 0.00012331356346193288,\n",
       " (357, 'val'): 0.00016655039939063567,\n",
       " (358, 'train'): 0.00012242620707386069,\n",
       " (358, 'val'): 0.0001665585053463777,\n",
       " (359, 'train'): 0.0001228861122702559,\n",
       " (359, 'val'): 0.0001665748207381478,\n",
       " (360, 'train'): 0.00012141743381680162,\n",
       " (360, 'val'): 0.00016661157348641642,\n",
       " (361, 'train'): 0.00012301868345174525,\n",
       " (361, 'val'): 0.00016662631942718117,\n",
       " (362, 'train'): 0.0001246961635640926,\n",
       " (362, 'val'): 0.00016654927835420326,\n",
       " (363, 'train'): 0.0001245421590283513,\n",
       " (363, 'val'): 0.00016652987580056543,\n",
       " (364, 'train'): 0.0001239163749333885,\n",
       " (364, 'val'): 0.00016653780928916403,\n",
       " (365, 'train'): 0.0001242408416820345,\n",
       " (365, 'val'): 0.00016650372978161882,\n",
       " (366, 'train'): 0.00012118523698989991,\n",
       " (366, 'val'): 0.00016652806489555924,\n",
       " (367, 'train'): 0.00012183142394793254,\n",
       " (367, 'val'): 0.00016656362762053808,\n",
       " (368, 'train'): 0.00012251958940868025,\n",
       " (368, 'val'): 0.00016656969846398742,\n",
       " (369, 'train'): 0.00012266715229661377,\n",
       " (369, 'val'): 0.0001665851515200403,\n",
       " (370, 'train'): 0.00012598420424317873,\n",
       " (370, 'val'): 0.00016660455407367813,\n",
       " (371, 'train'): 0.00012113499731101372,\n",
       " (371, 'val'): 0.00016659184324520606,\n",
       " (372, 'train'): 0.00012354557057497678,\n",
       " (372, 'val'): 0.0001666098660616963,\n",
       " (373, 'train'): 0.00012236936190338047,\n",
       " (373, 'val'): 0.0001666466877968223,\n",
       " (374, 'train'): 0.0001246048810167445,\n",
       " (374, 'val'): 0.00016658111578888365,\n",
       " (375, 'train'): 0.00012300977121210762,\n",
       " (375, 'val'): 0.0001666262504403238,\n",
       " (376, 'train'): 0.00012349898288785307,\n",
       " (376, 'val'): 0.000166565524759116,\n",
       " (377, 'train'): 0.0001220872013450221,\n",
       " (377, 'val'): 0.0001665561597932268,\n",
       " (378, 'train'): 0.00012397411693301465,\n",
       " (378, 'val'): 0.00016661524703657187,\n",
       " (379, 'train'): 0.00012362789776590135,\n",
       " (379, 'val'): 0.00016660098400380876,\n",
       " (380, 'train'): 0.00012141913261816458,\n",
       " (380, 'val'): 0.00016657347549442893,\n",
       " (381, 'train'): 0.00012327682364870001,\n",
       " (381, 'val'): 0.00016660024239509194,\n",
       " (382, 'train'): 0.00012141997770716746,\n",
       " (382, 'val'): 0.00016654919212063154,\n",
       " (383, 'train'): 0.00012408479772232198,\n",
       " (383, 'val'): 0.0001665511065059238,\n",
       " (384, 'train'): 0.0001246425780226235,\n",
       " (384, 'val'): 0.000166590739455488,\n",
       " (385, 'train'): 0.00012452799085251712,\n",
       " (385, 'val'): 0.0001665579362048043,\n",
       " (386, 'train'): 0.00012117695856701445,\n",
       " (386, 'val'): 0.0001665328767288614,\n",
       " (387, 'train'): 0.00012284124063120948,\n",
       " (387, 'val'): 0.00016655764301066046,\n",
       " (388, 'train'): 0.0001217054323879657,\n",
       " (388, 'val'): 0.00016654187951374938,\n",
       " (389, 'train'): 0.00012178533641552483,\n",
       " (389, 'val'): 0.00016654239691517972,\n",
       " (390, 'train'): 0.00012104931563414909,\n",
       " (390, 'val'): 0.0001665444665209011,\n",
       " (391, 'train'): 0.00012269404854763438,\n",
       " (391, 'val'): 0.0001665495198082041,\n",
       " (392, 'train'): 0.0001231987477728614,\n",
       " (392, 'val'): 0.00016659798307551278,\n",
       " (393, 'train'): 0.0001247322609372161,\n",
       " (393, 'val'): 0.00016670505067816488,\n",
       " (394, 'train'): 0.0001230397891184246,\n",
       " (394, 'val'): 0.00016669290899126618,\n",
       " (395, 'train'): 0.00012371648551413305,\n",
       " (395, 'val'): 0.00016655409018750543,\n",
       " (396, 'train'): 0.00012347913623132086,\n",
       " (396, 'val'): 0.0001665079552266333,\n",
       " (397, 'train'): 0.00012379238830396422,\n",
       " (397, 'val'): 0.00016653601563087217,\n",
       " (398, 'train'): 0.00012329692900594737,\n",
       " (398, 'val'): 0.00016651549204080193,\n",
       " (399, 'train'): 0.00012370653847163475,\n",
       " (399, 'val'): 0.00016652258044039763,\n",
       " (400, 'train'): 0.00012445619709237858,\n",
       " (400, 'val'): 0.00016651561276780235,\n",
       " (401, 'train'): 0.00012427938808859499,\n",
       " (401, 'val'): 0.00016648938051528402,\n",
       " (402, 'train'): 0.00012528064177819975,\n",
       " (402, 'val'): 0.00016648848368613808,\n",
       " (403, 'train'): 0.0001230848246012573,\n",
       " (403, 'val'): 0.0001664930368187251,\n",
       " (404, 'train'): 0.00012866201103423481,\n",
       " (404, 'val'): 0.00016650588562091193,\n",
       " (405, 'train'): 0.00012021406588179093,\n",
       " (405, 'val'): 0.00016651290503365022,\n",
       " (406, 'train'): 0.00012379256939446486,\n",
       " (406, 'val'): 0.0001665391200394542,\n",
       " (407, 'train'): 0.00012316435351277942,\n",
       " (407, 'val'): 0.00016662138686687858,\n",
       " (408, 'train'): 0.0001223503775825655,\n",
       " (408, 'val'): 0.00016658503079303988,\n",
       " (409, 'train'): 0.00012317334336263162,\n",
       " (409, 'val'): 0.0001665895494321982,\n",
       " (410, 'train'): 0.00012497345623732717,\n",
       " (410, 'val'): 0.000166605899317397,\n",
       " (411, 'train'): 0.00012321161813344116,\n",
       " (411, 'val'): 0.00016656800828598165,\n",
       " (412, 'train'): 0.00012374186836596992,\n",
       " (412, 'val'): 0.00016657542437314987,\n",
       " (413, 'train'): 0.00012377990599445724,\n",
       " (413, 'val'): 0.0001664987454811732,\n",
       " (414, 'train'): 0.00012401692758969688,\n",
       " (414, 'val'): 0.00016650112552775277,\n",
       " (415, 'train'): 0.00011970629674141053,\n",
       " (415, 'val'): 0.00016650022869860685,\n",
       " (416, 'train'): 0.00012234647120176642,\n",
       " (416, 'val'): 0.00016655546992465302,\n",
       " (417, 'train'): 0.00012225010087368665,\n",
       " (417, 'val'): 0.00016659118787006096,\n",
       " (418, 'train'): 0.00012185460353201186,\n",
       " (418, 'val'): 0.00016658235755231645,\n",
       " (419, 'train'): 0.00012602756679471995,\n",
       " (419, 'val'): 0.0001666419104569488,\n",
       " (420, 'train'): 0.00012252444867044687,\n",
       " (420, 'val'): 0.00016665500071313646,\n",
       " (421, 'train'): 0.00012433649195979038,\n",
       " (421, 'val'): 0.00016659098090948882,\n",
       " (422, 'train'): 0.00012248809259660817,\n",
       " (422, 'val'): 0.0001665824092924595,\n",
       " (423, 'train'): 0.00012357840400741056,\n",
       " (423, 'val'): 0.00016650095306060934,\n",
       " (424, 'train'): 0.00012355597465540523,\n",
       " (424, 'val'): 0.0001665038332619049,\n",
       " (425, 'train'): 0.00012384261504781466,\n",
       " (425, 'val'): 0.00016650840364120626,\n",
       " (426, 'train'): 0.00012433396962781748,\n",
       " (426, 'val'): 0.00016654406984647116,\n",
       " (427, 'train'): 0.00012230526880119687,\n",
       " (427, 'val'): 0.0001665624375972483,\n",
       " (428, 'train'): 0.00012483865160633017,\n",
       " (428, 'val'): 0.00016652570209569402,\n",
       " (429, 'train'): 0.00012202978272128988,\n",
       " (429, 'val'): 0.00016649043256485903,\n",
       " (430, 'train'): 0.00012522849202569987,\n",
       " (430, 'val'): 0.0001665542626546489,\n",
       " (431, 'train'): 0.00012332343289421665,\n",
       " (431, 'val'): 0.00016651057672721368,\n",
       " (432, 'train'): 0.00012458077442176915,\n",
       " (432, 'val'): 0.00016651431926422648,\n",
       " (433, 'train'): 0.00012029643187782279,\n",
       " (433, 'val'): 0.00016648793179127906,\n",
       " (434, 'train'): 0.00012197394217192023,\n",
       " (434, 'val'): 0.00016649127765386193,\n",
       " (435, 'train'): 0.00012427915094627274,\n",
       " (435, 'val'): 0.00016647451384751885,\n",
       " (436, 'train'): 0.00012373600017141412,\n",
       " (436, 'val'): 0.00016653301470257617,\n",
       " (437, 'train'): 0.00012716574646118615,\n",
       " (437, 'val'): 0.00016668935616811117,\n",
       " (438, 'train'): 0.0001222485314226813,\n",
       " (438, 'val'): 0.00016663301115234694,\n",
       " (439, 'train'): 0.0001226396825923412,\n",
       " (439, 'val'): 0.00016663071733933907,\n",
       " (440, 'train'): 0.00012099991673258719,\n",
       " (440, 'val'): 0.00016661743736929365,\n",
       " (441, 'train'): 0.00012254208343586437,\n",
       " (441, 'val'): 0.00016658280596688942,\n",
       " (442, 'train'): 0.00012278348138486899,\n",
       " (442, 'val'): 0.000166551416946782,\n",
       " (443, 'train'): 0.00012551299813720916,\n",
       " (443, 'val'): 0.0001664861381329872,\n",
       " (444, 'train'): 0.0001239850901550165,\n",
       " (444, 'val'): 0.00016649184679543532,\n",
       " (445, 'train'): 0.00012438000542008213,\n",
       " (445, 'val'): 0.00016649329551944025,\n",
       " (446, 'train'): 0.0001227300769339005,\n",
       " (446, 'val'): 0.00016649507193101777,\n",
       " (447, 'train'): 0.0001252763300996136,\n",
       " (447, 'val'): 0.00016678897319016633,\n",
       " (448, 'train'): 0.00012550248626481603,\n",
       " (448, 'val'): 0.00016657718353801302,\n",
       " (449, 'train'): 0.00012246966879400943,\n",
       " (449, 'val'): 0.00016653958570074151,\n",
       " (450, 'train'): 0.000125929010445597,\n",
       " (450, 'val'): 0.00016654448376761543,\n",
       " (451, 'train'): 0.00012575972963262488,\n",
       " (451, 'val'): 0.00016653701594030417,\n",
       " (452, 'train'): 0.00012072183933384993,\n",
       " (452, 'val'): 0.0001665359466440148,\n",
       " (453, 'train'): 0.0001229160008262153,\n",
       " (453, 'val'): 0.00016658389250989313,\n",
       " (454, 'train'): 0.00012320450386377397,\n",
       " (454, 'val'): 0.00016653853365116648,\n",
       " (455, 'train'): 0.00012293219980266359,\n",
       " (455, 'val'): 0.00016655971261638183,\n",
       " (456, 'train'): 0.00012466775822556682,\n",
       " (456, 'val'): 0.0001665217698448234,\n",
       " (457, 'train'): 0.00012243132934802107,\n",
       " (457, 'val'): 0.000166524650046119,\n",
       " (458, 'train'): 0.00012474101364474605,\n",
       " (458, 'val'): 0.00016656393806139627,\n",
       " (459, 'train'): 0.00012233172526100166,\n",
       " (459, 'val'): 0.00016650083233360892,\n",
       " (460, 'train'): 0.00012280068498242785,\n",
       " (460, 'val'): 0.00016652240797325417,\n",
       " (461, 'train'): 0.00012294228913055526,\n",
       " (461, 'val'): 0.0001665571945960875,\n",
       " (462, 'train'): 0.00012319374191402285,\n",
       " (462, 'val'): 0.0001665276682211293,\n",
       " (463, 'train'): 0.00012088864524331357,\n",
       " (463, 'val'): 0.0001665348773477254,\n",
       " (464, 'train'): 0.00012352776334241585,\n",
       " (464, 'val'): 0.00016652918593199165,\n",
       " (465, 'train'): 0.00012181271126286851,\n",
       " (465, 'val'): 0.00016651371562922441,\n",
       " (466, 'train'): 0.00012254917183546004,\n",
       " (466, 'val'): 0.0001665103180264985,\n",
       " (467, 'train'): 0.00012363790517189987,\n",
       " (467, 'val'): 0.00016650003898474906,\n",
       " (468, 'train'): 0.00012206978647521248,\n",
       " (468, 'val'): 0.00016649029459114427,\n",
       " (469, 'train'): 0.00012493024028285786,\n",
       " (469, 'val'): 0.00016647682490724105,\n",
       " (470, 'train'): 0.00012043234892189503,\n",
       " (470, 'val'): 0.00016649648616159403,\n",
       " (471, 'train'): 0.00012168431378625057,\n",
       " (471, 'val'): 0.00016649141562757668,\n",
       " (472, 'train'): 0.00012429450914539671,\n",
       " (472, 'val'): 0.00016648403403383714,\n",
       " (473, 'train'): 0.0001223671672589801,\n",
       " (473, 'val'): 0.00016650581663405455,\n",
       " (474, 'train'): 0.00012131513924234443,\n",
       " (474, 'val'): 0.0001664896047225705,\n",
       " (475, 'train'): 0.00012338143790623657,\n",
       " (475, 'val'): 0.00016648080889825469,\n",
       " (476, 'train'): 0.00012303192030500482,\n",
       " (476, 'val'): 0.00016647610054523857,\n",
       " (477, 'train'): 0.00012172813768740054,\n",
       " (477, 'val'): 0.0001664962792010219,\n",
       " (478, 'train'): 0.0001231297522921253,\n",
       " (478, 'val'): 0.00016648743163656305,\n",
       " (479, 'train'): 0.00012075005064683932,\n",
       " (479, 'val'): 0.00016648037773039605,\n",
       " (480, 'train'): 0.0001276163599902281,\n",
       " (480, 'val'): 0.00016656442096939794,\n",
       " (481, 'train'): 0.00012129824177396518,\n",
       " (481, 'val'): 0.0001666373228309331,\n",
       " (482, 'train'): 0.00012360956450855291,\n",
       " (482, 'val'): 0.00016659612043036355,\n",
       " (483, 'train'): 0.0001217021210188115,\n",
       " (483, 'val'): 0.0001665810295553119,\n",
       " (484, 'train'): 0.00012480052343259254,\n",
       " (484, 'val'): 0.000166611952914132,\n",
       " (485, 'train'): 0.00012457662227529067,\n",
       " (485, 'val'): 0.0001665455530639048,\n",
       " (486, 'train'): 0.000121022070137163,\n",
       " (486, 'val'): 0.00016655274494378655,\n",
       " (487, 'train'): 0.00012260128278285265,\n",
       " (487, 'val'): 0.00016660943489383767,\n",
       " (488, 'train'): 0.00012419798790856644,\n",
       " (488, 'val'): 0.00016677933227684762,\n",
       " (489, 'train'): 0.00012410132438634282,\n",
       " (489, 'val'): 0.00016666567642931586,\n",
       " (490, 'train'): 0.00012374918097285208,\n",
       " (490, 'val'): 0.0001666016911190969,\n",
       " (491, 'train'): 0.00012353770607323558,\n",
       " (491, 'val'): 0.00016662669885489676,\n",
       " (492, 'train'): 0.00012514296988094294,\n",
       " (492, 'val'): 0.00016657375144185844,\n",
       " (493, 'train'): 0.0001223111197490383,\n",
       " (493, 'val'): 0.00016651299126722193,\n",
       " (494, 'train'): 0.00012272854628800242,\n",
       " (494, 'val'): 0.0001665343771930094,\n",
       " (495, 'train'): 0.00012316953183876144,\n",
       " (495, 'val'): 0.00016653617085130127,\n",
       " (496, 'train'): 0.00012457531583667905,\n",
       " (496, 'val'): 0.0001665550387567944,\n",
       " (497, 'train'): 0.0001219312306838455,\n",
       " (497, 'val'): 0.00016656550751240165,\n",
       " (498, 'train'): 0.00012401219336660924,\n",
       " (498, 'val'): 0.00016655935043538057,\n",
       " (499, 'train'): 0.000123321824638104,\n",
       " (499, 'val'): 0.0001665763901891532}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old code for autoencoders\n",
    "# for epoch in range(epochs):\n",
    "#     # ä¸éœ€è¦labelï¼Œæ‰€ä»¥ç”¨ä¸€ä¸ªå ä½ç¬¦\"_\"ä»£æ›¿\n",
    "#     for batchidx, (x, _) in enumerate(X_allDataLoader):\n",
    "#         x.requires_grad_(True)\n",
    "#         # encode and decode \n",
    "#         output = model(x)\n",
    "#         # compute loss\n",
    "#         print(output.shape)\n",
    "#         loss = loss_function(output, x)      \n",
    "#         # update\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "           \n",
    "#     loss_train[epoch,0] = loss.item()  \n",
    "#     print('Epoch: %04d, Training loss=%.8f' %\n",
    "#           (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'saved/models/ae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_batch = model(X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0174, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(recon_batch,X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5140, 0.0913, 0.5288,  ..., 0.2051, 0.3364, 0.3238],\n",
       "        [0.5580, 0.1173, 0.4742,  ..., 0.2005, 0.3032, 0.3639],\n",
       "        [0.0429, 0.1376, 0.3726,  ..., 0.1790, 0.3363, 0.3920],\n",
       "        ...,\n",
       "        [0.5027, 0.0823, 0.4501,  ..., 0.1798, 0.2450, 0.2799],\n",
       "        [0.1499, 0.1126, 0.5041,  ..., 0.2068, 0.3374, 0.3552],\n",
       "        [0.6287, 0.0781, 0.6299,  ..., 0.2292, 0.2406, 0.2678]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7359, 0.0469, 0.5691,  ..., 0.1282, 0.3710, 0.2462],\n",
       "        [0.5435, 0.1483, 0.4982,  ..., 0.3278, 0.3599, 0.4306],\n",
       "        [0.1336, 0.0690, 0.3926,  ..., 0.2639, 0.3219, 0.1819],\n",
       "        ...,\n",
       "        [0.3869, 0.0385, 0.4286,  ..., 0.1089, 0.2763, 0.2801],\n",
       "        [0.1415, 0.0447, 0.4723,  ..., 0.1098, 0.2252, 0.4184],\n",
       "        [0.6655, 0.0802, 0.5544,  ..., 0.1544, 0.1192, 0.1983]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008136829"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_batch.cpu().detach().numpy().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = model.encode(X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = train_embeddings.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01398324966430664, tolerance: 0.00010032937279902399\n",
      "  positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.001)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.001)\n",
    "clf.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFrg = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "RFrg.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeature = model.encode(X_testTensor)\n",
    "lasso = clf.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfresult = RFrg.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.917931343703417"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(lasso,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.09862380052972"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(rfresult,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.010893, 0.003969, 0.126364, 0.04815 , 0.013301, 0.003975,\n",
       "       0.006658, 0.236411, 0.024986, 0.112077, 0.004105, 0.007248,\n",
       "       0.015496, 0.079042, 0.066385, 0.006305, 0.006078, 0.025742,\n",
       "       0.030093, 0.024503, 0.011125, 0.037697, 0.012029, 0.003817,\n",
       "       0.010667, 0.067386, 0.057037, 0.006329, 0.067224, 0.003564,\n",
       "       0.186092, 0.108377, 0.101198, 0.029871, 0.02189 , 0.043306,\n",
       "       0.12798 , 0.005638, 0.038664, 0.002193, 0.005611, 0.157488,\n",
       "       0.007132, 0.006106, 0.008437, 0.031169, 0.054449, 0.006736,\n",
       "       0.016306, 0.003624, 0.093631, 0.011592, 0.010451, 0.003869,\n",
       "       0.011615, 0.189386, 0.003639, 0.023794, 0.020825, 0.036415,\n",
       "       0.00255 , 0.005367, 0.018914, 0.013027, 0.185878, 0.003549,\n",
       "       0.004383, 0.007288, 0.012238, 0.019928, 0.075604, 0.01131 ,\n",
       "       0.006595, 0.00253 , 0.005594, 0.018333, 0.015419, 0.010771,\n",
       "       0.008096, 0.03138 , 0.043866, 0.014501, 0.002152, 0.015858,\n",
       "       0.142685, 0.134998, 0.006991, 0.049048, 0.033088, 0.005561,\n",
       "       0.012738, 0.018554, 0.017699, 0.141754, 0.010312, 0.004757,\n",
       "       0.00994 , 0.035553, 0.006461, 0.053599, 0.029555, 0.115657,\n",
       "       0.003604, 0.029185, 0.034422, 0.028011, 0.004901, 0.081803,\n",
       "       0.014341, 0.007389, 0.043371, 0.0064  , 0.093948, 0.010454,\n",
       "       0.017118, 0.00423 , 0.010077, 0.011115, 0.004962, 0.016636,\n",
       "       0.030657, 0.010251, 0.007557, 0.194723, 0.076808, 0.007164,\n",
       "       0.05563 , 0.151063, 0.041997, 0.016026, 0.006891, 0.003589,\n",
       "       0.004219, 0.022708, 0.056148])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05835779, 0.04809723, 0.08393456, 0.01316745, 0.01641077,\n",
       "       0.03210582, 0.02638387, 0.09814184, 0.05537106, 0.17477762,\n",
       "       0.02174419, 0.01008483, 0.05913225, 0.02961461, 0.04120421,\n",
       "       0.03671159, 0.03117072, 0.03934981, 0.01289991, 0.03020184,\n",
       "       0.01180884, 0.05170509, 0.02744221, 0.06279885, 0.03629024,\n",
       "       0.04566286, 0.04937772, 0.0138828 , 0.07842328, 0.01707409,\n",
       "       0.05549958, 0.03840715, 0.03311106, 0.06620793, 0.04880786,\n",
       "       0.01589267, 0.07003126, 0.02660924, 0.01285711, 0.00950247,\n",
       "       0.01283124, 0.03617788, 0.01042021, 0.02128065, 0.07431409,\n",
       "       0.04035205, 0.01197993, 0.09221617, 0.02525261, 0.01656044,\n",
       "       0.02447865, 0.10361253, 0.01008788, 0.01298795, 0.0279641 ,\n",
       "       0.06334959, 0.02276789, 0.01078643, 0.03309207, 0.01934929,\n",
       "       0.0295273 , 0.03260531, 0.03851115, 0.02190465, 0.16168004,\n",
       "       0.03004164, 0.02372988, 0.04582957, 0.01219899, 0.03269482,\n",
       "       0.06253785, 0.04725378, 0.02765245, 0.03629509, 0.04448985,\n",
       "       0.04734431, 0.01286329, 0.01634997, 0.01280674, 0.05046457,\n",
       "       0.02566315, 0.02287993, 0.03154614, 0.05938819, 0.01725539,\n",
       "       0.03355772, 0.06258407, 0.04531535, 0.02965802, 0.05869021,\n",
       "       0.05889597, 0.01525516, 0.02361496, 0.06068944, 0.04192264,\n",
       "       0.04540816, 0.0202335 , 0.04784987, 0.02777353, 0.03497224,\n",
       "       0.01955198, 0.00782624, 0.01703603, 0.03593047, 0.07485706,\n",
       "       0.01934664, 0.03711584, 0.0313352 , 0.05162903, 0.05446445,\n",
       "       0.02777669, 0.03443277, 0.04996809, 0.02994503, 0.02449468,\n",
       "       0.02759377, 0.0118499 , 0.0462724 , 0.01659376, 0.04646762,\n",
       "       0.01222194, 0.02568732, 0.01795877, 0.01400877, 0.06488429,\n",
       "       0.01763504, 0.01173873, 0.02314007, 0.01728442, 0.03260236,\n",
       "       0.03861159, 0.01138125, 0.03048467, 0.02376963, 0.08449274])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=\"label\")\n",
    "plt.legend()\n",
    "plt.savefig(\"saved/figures/tsne_ae_gdsc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data type conversion\n",
    "B_feature = torch.FloatTensor(feature).to(device)\n",
    "y = torch.FloatTensor(Y_train.values).to(device)\n",
    "# construct TensorDataset\n",
    "b_data = TensorDataset(B_feature, y)\n",
    "trainDataLoader2 = DataLoader(dataset=b_data, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization DNN model\n",
    "\n",
    "predictor = DNN(128, dim_dnn_out).to(device)\n",
    "optimizer = optim.Adam(predictor.parameters(), lr=1e-3,betas=(0.9,0.99))\n",
    "#loss1-softmax\n",
    "loss_func = nn.MSELoss().to(device)\n",
    "#loss2-sigmoid\n",
    "#loss_func = nn.BCELoss()\n",
    "#loss3-sigmoid\n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = torch.nn.MSELoss(size_average=True)\n",
    "#criterion = torch.nn.BCELoss(size_average=True) # Defined loss function\n",
    "#optimizer = optim.Adm(model.parameters(), lr=0.01) # Defined optimizer\n",
    "loss_train = np.zeros((epochs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ',epoch)\n",
    "    for step,(batch_x,batch_y) in enumerate(trainDataLoader2):\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "        # predict label\n",
    "        output = predictor(b_x)\n",
    "        # b_y=F.sigmoid(b_y) \n",
    "        \n",
    "        #print\n",
    "        #print(output)\n",
    "        #print(b_y)\n",
    "        # compute loss\n",
    "        loss = loss_func(output,b_y)\n",
    "        #loss = criterion(output, b_y)\n",
    "        \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item())) \n",
    "\n",
    "# Save model\n",
    "torch.save(predictor.state_dict(), 'saved/models/DNN_GDSC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict = predictor(testFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
