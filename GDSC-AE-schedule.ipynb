{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "#import scipy.io as sio\n",
    "\n",
    "from models import VAE,AEBase\n",
    "from models import DNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import models\n",
    "import utils as ut\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "epochs = 500 #200,500,1000  \n",
    "#dim_au_in = 20049\n",
    "dim_au_out = 512 #8, 16, 32, 64, 128, 256,512\n",
    "dim_dnn_in = dim_au_out\n",
    "dim_dnn_out=1\n",
    "select_drug = 'Gefitinib'\n",
    "na = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r=pd.read_csv('data/GDSCexpression.csv',index_col=0)\n",
    "label_r=pd.read_csv('data/GDSClabel.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_r=label_r.fillna(na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
     ]
    }
   ],
   "source": [
    "hvg,adata = ut.highly_variable_genes(data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Erlotinib</th>\n",
       "      <th>AICAR</th>\n",
       "      <th>Camptothecin</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Cisplatin</th>\n",
       "      <th>Cytarabine</th>\n",
       "      <th>Docetaxel</th>\n",
       "      <th>Methotrexate</th>\n",
       "      <th>ATRA</th>\n",
       "      <th>Gefitinib</th>\n",
       "      <th>...</th>\n",
       "      <th>CMK</th>\n",
       "      <th>Pyrimethamine</th>\n",
       "      <th>JW-7-52-1</th>\n",
       "      <th>A-443654</th>\n",
       "      <th>GW843682X</th>\n",
       "      <th>MS-275</th>\n",
       "      <th>Parthenolide</th>\n",
       "      <th>MG-132</th>\n",
       "      <th>KIN001-135</th>\n",
       "      <th>TGX221</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BxPC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.159708</td>\n",
       "      <td>0.188393</td>\n",
       "      <td>0.436127</td>\n",
       "      <td>0.154274</td>\n",
       "      <td>0.113764</td>\n",
       "      <td>0.249987</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMOE-2</th>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.318206</td>\n",
       "      <td>0.281740</td>\n",
       "      <td>0.063808</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.101530</td>\n",
       "      <td>0.191210</td>\n",
       "      <td>0.031229</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022662</td>\n",
       "      <td>0.025217</td>\n",
       "      <td>0.270791</td>\n",
       "      <td>0.095608</td>\n",
       "      <td>0.139914</td>\n",
       "      <td>0.171747</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFM-223</th>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.334087</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.361438</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>0.186659</td>\n",
       "      <td>0.228797</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUGC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.416792</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.266409</td>\n",
       "      <td>0.237811</td>\n",
       "      <td>0.234952</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>0.153002</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OC-314</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.479915</td>\n",
       "      <td>0.390145</td>\n",
       "      <td>0.067738</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>0.110418</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.020309</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KP-N-S19s</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC-3</th>\n",
       "      <td>0.003515</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048227</td>\n",
       "      <td>0.146476</td>\n",
       "      <td>0.400087</td>\n",
       "      <td>0.205247</td>\n",
       "      <td>0.147090</td>\n",
       "      <td>0.298803</td>\n",
       "      <td>0.023796</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.069181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Panc 08.13</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.180653</td>\n",
       "      <td>0.090963</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EKVX</th>\n",
       "      <td>0.177238</td>\n",
       "      <td>0.078395</td>\n",
       "      <td>0.078938</td>\n",
       "      <td>0.072213</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>0.095642</td>\n",
       "      <td>0.111598</td>\n",
       "      <td>0.126364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.243635</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.177759</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>0.007216</td>\n",
       "      <td>0.063911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMS-114</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.127677</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.281940</td>\n",
       "      <td>0.092074</td>\n",
       "      <td>0.191972</td>\n",
       "      <td>0.185916</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.043920</td>\n",
       "      <td>0.285969</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.174841</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.080451</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.028733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Erlotinib     AICAR  Camptothecin  Vinblastine  Cisplatin  \\\n",
       "BxPC-3      -1.000000  0.159708      0.188393     0.436127   0.154274   \n",
       "KMOE-2       0.002108  0.339814      0.318206     0.281740   0.063808   \n",
       "MFM-223      0.002148  0.334087      0.122671     0.361438   0.051983   \n",
       "NUGC-3      -1.000000  0.095099      0.416792     0.422631   0.266409   \n",
       "OC-314      -1.000000  0.012740      0.479915     0.390145   0.067738   \n",
       "...               ...       ...           ...          ...        ...   \n",
       "KP-N-S19s   -1.000000 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "BC-3         0.003515 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "Panc 08.13  -1.000000  0.152818      0.196279     0.180653   0.090963   \n",
       "EKVX         0.177238  0.078395      0.078938     0.072213   0.007225   \n",
       "DMS-114      0.000322  0.127677      0.168317     0.281940   0.092074   \n",
       "\n",
       "            Cytarabine  Docetaxel  Methotrexate      ATRA  Gefitinib  ...  \\\n",
       "BxPC-3        0.113764   0.249987      0.015324  0.015324   0.020293  ...   \n",
       "KMOE-2        0.140681   0.101530      0.191210  0.031229   0.007419  ...   \n",
       "MFM-223       0.186659   0.228797      0.003639  0.023043   0.003639  ...   \n",
       "NUGC-3        0.237811   0.234952      0.011558  0.028838   0.153002  ...   \n",
       "OC-314        0.125869   0.249668      0.110418  0.002214   0.020309  ...   \n",
       "...                ...        ...           ...       ...        ...  ...   \n",
       "KP-N-S19s    -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "BC-3         -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "Panc 08.13    0.093197   0.097267      0.018269  0.010237   0.009405  ...   \n",
       "EKVX          0.003470   0.129184      0.095642  0.111598   0.126364  ...   \n",
       "DMS-114       0.191972   0.185916      0.007135  0.009329   0.020822  ...   \n",
       "\n",
       "                 CMK  Pyrimethamine  JW-7-52-1  A-443654  GW843682X    MS-275  \\\n",
       "BxPC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "KMOE-2      0.022662       0.025217   0.270791  0.095608   0.139914  0.171747   \n",
       "MFM-223     0.038997       0.006167   0.002185  0.102592   0.020338  0.010874   \n",
       "NUGC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "OC-314     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "...              ...            ...        ...       ...        ...       ...   \n",
       "KP-N-S19s  -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "BC-3        0.048227       0.146476   0.400087  0.205247   0.147090  0.298803   \n",
       "Panc 08.13 -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "EKVX        0.020472       0.044047   0.243635  0.079375   0.019859  0.177759   \n",
       "DMS-114     0.002137       0.043920   0.285969  0.075195   0.174841  0.008383   \n",
       "\n",
       "            Parthenolide    MG-132  KIN001-135    TGX221  \n",
       "BxPC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "KMOE-2          0.026670  0.013904    0.002116  0.000956  \n",
       "MFM-223         0.000970  0.035206    0.007051  0.000323  \n",
       "NUGC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "OC-314         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "...                  ...       ...         ...       ...  \n",
       "KP-N-S19s      -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "BC-3            0.023796  0.017219    0.002065  0.069181  \n",
       "Panc 08.13     -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "EKVX            0.014843  0.114431    0.007216  0.063911  \n",
       "DMS-114         0.002135  0.080451    0.002135  0.028733  \n",
       "\n",
       "[789 rows x 139 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = label_r.loc[:,select_drug]!=na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r.columns = adata.var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your is gene-cell, mine is cell-gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_r.loc[selected_idx,hvg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_r.loc[selected_idx,select_drug]\n",
    "scaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "-9.065550491767716e-18\n"
     ]
    }
   ],
   "source": [
    "print(np.std(data))\n",
    "print(np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 3462)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 139)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 3462)\n",
      "(675,)\n",
      "(432, 3462) (432,)\n",
      "(135, 3462) (135,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.375997920629374\n",
      "-7.905185436786982\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all data to AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainTensor = torch.FloatTensor(X_train).to(device)\n",
    "X_validTensor = torch.FloatTensor(X_valid).to(device)\n",
    "X_testTensor = torch.FloatTensor(X_test).to(device)\n",
    "X_allTensor = torch.FloatTensor(data).to(device)\n",
    "\n",
    "Y_trainTensor = torch.FloatTensor(Y_train.values).to(device)\n",
    "Y_validTensor = torch.FloatTensor(Y_valid.values).to(device)\n",
    "\n",
    "# construct TensorDataset\n",
    "train_dataset = TensorDataset(X_trainTensor, X_trainTensor)\n",
    "valid_dataset = TensorDataset(X_validTensor, X_validTensor)\n",
    "test_dataset = TensorDataset(X_testTensor, X_testTensor)\n",
    "all_dataset = TensorDataset(X_allTensor, X_allTensor)\n",
    "\n",
    "X_trainDataLoader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "X_validDataLoader = DataLoader(dataset=valid_dataset, batch_size=200, shuffle=True)\n",
    "X_allDataLoader = DataLoader(dataset=all_dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = X_trainDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_train = {'train':X_trainDataLoader,'val':X_validDataLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainDataLoader.dataset.tensors[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 432, 'val': 108}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{x: dataloaders_train[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEBase(input_dim=data.shape[1],latent_dim=256,hidden_dims=[1024,512,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEBase(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=3462, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (decoder_input): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=3462, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VAE(dim_au_in=data_r.shape[1],dim_au_out=128)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae_model(net,data_loaders,optimizer,loss_function,n_epochs,scheduler):\n",
    "    \n",
    "    dataset_sizes = {x: data_loaders[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}\n",
    "    loss_train = {}\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #optimizer = scheduler(optimizer, epoch)\n",
    "                net.train()  # Set model to training mode\n",
    "            else:\n",
    "                net.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # for data in data_loaders[phase]:\n",
    "            for batchidx, (x, _) in enumerate(data_loaders[phase]):\n",
    "\n",
    "                x.requires_grad_(True)\n",
    "                # encode and decode \n",
    "                output = model(x)\n",
    "                # compute loss\n",
    "                loss = loss_function(output, x)      \n",
    "\n",
    "                # zero the parameter (weight) gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    # update the weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                # print loss statistics\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Schedular\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            \n",
    "            loss_train[epoch,phase] = epoch_loss \n",
    "            print('{} Loss: {:.8f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Select best model wts\n",
    "    torch.save(best_model_wts, 'saved/models/ae.pkl')\n",
    "    net.load_state_dict(best_model_wts)           \n",
    "    \n",
    "    return net, loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00239341\n",
      "val Loss: 0.00300079\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00238665\n",
      "val Loss: 0.00300216\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00238214\n",
      "val Loss: 0.00300305\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00240891\n",
      "val Loss: 0.00300558\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00239257\n",
      "val Loss: 0.00300388\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00236371\n",
      "val Loss: 0.00300354\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00239391\n",
      "val Loss: 0.00300440\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00237672\n",
      "val Loss: 0.00300528\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00238567\n",
      "val Loss: 0.00300404\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00237047\n",
      "val Loss: 0.00300453\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00234157\n",
      "val Loss: 0.00300414\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00235259\n",
      "val Loss: 0.00300296\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00240162\n",
      "val Loss: 0.00300569\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00235590\n",
      "val Loss: 0.00300443\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00235226\n",
      "val Loss: 0.00300200\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00234642\n",
      "val Loss: 0.00300084\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00237760\n",
      "val Loss: 0.00300306\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00238500\n",
      "val Loss: 0.00300289\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00236629\n",
      "val Loss: 0.00300250\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00235446\n",
      "val Loss: 0.00300008\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00235165\n",
      "val Loss: 0.00300095\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00238079\n",
      "val Loss: 0.00300256\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00237560\n",
      "val Loss: 0.00300162\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00236359\n",
      "val Loss: 0.00300214\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00238146\n",
      "val Loss: 0.00300368\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00237937\n",
      "val Loss: 0.00300376\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00240555\n",
      "val Loss: 0.00300491\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00235526\n",
      "val Loss: 0.00300342\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00237992\n",
      "val Loss: 0.00300353\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00234920\n",
      "val Loss: 0.00300186\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00236389\n",
      "val Loss: 0.00300068\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00236187\n",
      "val Loss: 0.00300112\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00236597\n",
      "val Loss: 0.00300165\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00235047\n",
      "val Loss: 0.00300070\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00235511\n",
      "val Loss: 0.00300015\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00235301\n",
      "val Loss: 0.00300094\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00236433\n",
      "val Loss: 0.00300132\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00236486\n",
      "val Loss: 0.00300180\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00236456\n",
      "val Loss: 0.00300082\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00236432\n",
      "val Loss: 0.00300260\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00236110\n",
      "val Loss: 0.00300427\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00235654\n",
      "val Loss: 0.00300253\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00239081\n",
      "val Loss: 0.00300268\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00237291\n",
      "val Loss: 0.00300334\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00236190\n",
      "val Loss: 0.00300345\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.00238742\n",
      "val Loss: 0.00300466\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.00236525\n",
      "val Loss: 0.00300374\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.00237458\n",
      "val Loss: 0.00300376\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.00236847\n",
      "val Loss: 0.00300410\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.00237946\n",
      "val Loss: 0.00300305\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.00236333\n",
      "val Loss: 0.00300126\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.00236393\n",
      "val Loss: 0.00300257\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.00235684\n",
      "val Loss: 0.00300279\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.00236500\n",
      "val Loss: 0.00300333\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.00234294\n",
      "val Loss: 0.00300298\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.00238006\n",
      "val Loss: 0.00300278\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.00237760\n",
      "val Loss: 0.00300256\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.00239788\n",
      "val Loss: 0.00300349\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.00235735\n",
      "val Loss: 0.00300189\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.00238522\n",
      "val Loss: 0.00300492\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.00240547\n",
      "val Loss: 0.00300381\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.00236354\n",
      "val Loss: 0.00300330\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.00235585\n",
      "val Loss: 0.00300212\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.00235861\n",
      "val Loss: 0.00300343\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.00237792\n",
      "val Loss: 0.00300338\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.00236708\n",
      "val Loss: 0.00300342\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.00236339\n",
      "val Loss: 0.00300332\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.00234981\n",
      "val Loss: 0.00300355\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.00238950\n",
      "val Loss: 0.00300398\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.00237554\n",
      "val Loss: 0.00300251\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.00240659\n",
      "val Loss: 0.00300503\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.00234962\n",
      "val Loss: 0.00300559\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.00235734\n",
      "val Loss: 0.00300470\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.00236717\n",
      "val Loss: 0.00300440\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.00237038\n",
      "val Loss: 0.00300414\n",
      "Epoch 75/499\n",
      "----------\n",
      "train Loss: 0.00236537\n",
      "val Loss: 0.00300265\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.00237138\n",
      "val Loss: 0.00300292\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.00237871\n",
      "val Loss: 0.00300144\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.00237670\n",
      "val Loss: 0.00300201\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.00236080\n",
      "val Loss: 0.00300196\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.00235931\n",
      "val Loss: 0.00300212\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.00235627\n",
      "val Loss: 0.00300279\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.00236507\n",
      "val Loss: 0.00300344\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.00238564\n",
      "val Loss: 0.00300338\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.00240448\n",
      "val Loss: 0.00300446\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.00235789\n",
      "val Loss: 0.00300337\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.00242519\n",
      "val Loss: 0.00300545\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.00238189\n",
      "val Loss: 0.00300519\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.00238514\n",
      "val Loss: 0.00300651\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.00237163\n",
      "val Loss: 0.00300676\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.00238687\n",
      "val Loss: 0.00300497\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.00235130\n",
      "val Loss: 0.00300333\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.00237442\n",
      "val Loss: 0.00300325\n",
      "Epoch 93/499\n",
      "----------\n",
      "train Loss: 0.00236034\n",
      "val Loss: 0.00300387\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.00234431\n",
      "val Loss: 0.00300379\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.00238168\n",
      "val Loss: 0.00300486\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.00240210\n",
      "val Loss: 0.00300615\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.00236039\n",
      "val Loss: 0.00300431\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.00236028\n",
      "val Loss: 0.00300535\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.00241958\n",
      "val Loss: 0.00300560\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.00240964\n",
      "val Loss: 0.00300620\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.00235821\n",
      "val Loss: 0.00300463\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.00238874\n",
      "val Loss: 0.00300467\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.00236738\n",
      "val Loss: 0.00300329\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.00231981\n",
      "val Loss: 0.00300218\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.00239616\n",
      "val Loss: 0.00300272\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.00238481\n",
      "val Loss: 0.00300324\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.00233987\n",
      "val Loss: 0.00300295\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.00235691\n",
      "val Loss: 0.00300318\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.00236366\n",
      "val Loss: 0.00300344\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.00233898\n",
      "val Loss: 0.00300125\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.00235614\n",
      "val Loss: 0.00300166\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.00238862\n",
      "val Loss: 0.00300382\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.00236017\n",
      "val Loss: 0.00300379\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.00241264\n",
      "val Loss: 0.00300580\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.00234222\n",
      "val Loss: 0.00300284\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.00234901\n",
      "val Loss: 0.00300319\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.00235504\n",
      "val Loss: 0.00300244\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.00237737\n",
      "val Loss: 0.00300412\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.00240604\n",
      "val Loss: 0.00300495\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.00235732\n",
      "val Loss: 0.00300439\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.00236332\n",
      "val Loss: 0.00300543\n",
      "Epoch 122/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00237733\n",
      "val Loss: 0.00300417\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.00236094\n",
      "val Loss: 0.00300220\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.00238088\n",
      "val Loss: 0.00300221\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.00238174\n",
      "val Loss: 0.00300346\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.00234606\n",
      "val Loss: 0.00300180\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.00240663\n",
      "val Loss: 0.00300271\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.00233637\n",
      "val Loss: 0.00300117\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.00236218\n",
      "val Loss: 0.00300069\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.00240564\n",
      "val Loss: 0.00300253\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.00237236\n",
      "val Loss: 0.00300365\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.00238450\n",
      "val Loss: 0.00300383\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.00237330\n",
      "val Loss: 0.00300320\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.00236488\n",
      "val Loss: 0.00300214\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.00237539\n",
      "val Loss: 0.00300252\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.00233298\n",
      "val Loss: 0.00300183\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.00237807\n",
      "val Loss: 0.00300200\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.00238847\n",
      "val Loss: 0.00300161\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.00240527\n",
      "val Loss: 0.00300400\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.00233509\n",
      "val Loss: 0.00300427\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.00234594\n",
      "val Loss: 0.00300343\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.00237666\n",
      "val Loss: 0.00300279\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.00237498\n",
      "val Loss: 0.00300509\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.00240109\n",
      "val Loss: 0.00300517\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.00234633\n",
      "val Loss: 0.00300301\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.00241609\n",
      "val Loss: 0.00300476\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.00235565\n",
      "val Loss: 0.00300362\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.00237749\n",
      "val Loss: 0.00300504\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.00236465\n",
      "val Loss: 0.00300525\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.00240187\n",
      "val Loss: 0.00300562\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.00238455\n",
      "val Loss: 0.00300575\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.00239442\n",
      "val Loss: 0.00300598\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.00243707\n",
      "val Loss: 0.00300743\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.00235490\n",
      "val Loss: 0.00300677\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.00234990\n",
      "val Loss: 0.00300758\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.00238525\n",
      "val Loss: 0.00300516\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.00241589\n",
      "val Loss: 0.00300529\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.00238751\n",
      "val Loss: 0.00300486\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.00236836\n",
      "val Loss: 0.00300439\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.00237062\n",
      "val Loss: 0.00300326\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.00236565\n",
      "val Loss: 0.00300246\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.00235958\n",
      "val Loss: 0.00300232\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.00235001\n",
      "val Loss: 0.00300316\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.00237447\n",
      "val Loss: 0.00300424\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.00237592\n",
      "val Loss: 0.00300295\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.00238212\n",
      "val Loss: 0.00300395\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.00235687\n",
      "val Loss: 0.00300236\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.00234379\n",
      "val Loss: 0.00299976\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.00237365\n",
      "val Loss: 0.00299956\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.00237066\n",
      "val Loss: 0.00300275\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.00236589\n",
      "val Loss: 0.00300102\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.00238654\n",
      "val Loss: 0.00300094\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.00234801\n",
      "val Loss: 0.00300103\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.00237968\n",
      "val Loss: 0.00300106\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.00235643\n",
      "val Loss: 0.00299998\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.00236682\n",
      "val Loss: 0.00300059\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.00235892\n",
      "val Loss: 0.00300098\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.00238417\n",
      "val Loss: 0.00300274\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.00237622\n",
      "val Loss: 0.00300404\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.00236660\n",
      "val Loss: 0.00300520\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.00236316\n",
      "val Loss: 0.00300359\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.00235670\n",
      "val Loss: 0.00300301\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.00234896\n",
      "val Loss: 0.00300279\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.00236758\n",
      "val Loss: 0.00300252\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.00236699\n",
      "val Loss: 0.00300247\n",
      "Epoch 186/499\n",
      "----------\n",
      "train Loss: 0.00236416\n",
      "val Loss: 0.00300119\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.00237329\n",
      "val Loss: 0.00300154\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.00236220\n",
      "val Loss: 0.00300093\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.00236878\n",
      "val Loss: 0.00300303\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.00237145\n",
      "val Loss: 0.00300380\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.00233757\n",
      "val Loss: 0.00300216\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.00238275\n",
      "val Loss: 0.00300206\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.00242887\n",
      "val Loss: 0.00300353\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.00236010\n",
      "val Loss: 0.00300239\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.00236714\n",
      "val Loss: 0.00300352\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.00236295\n",
      "val Loss: 0.00300323\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.00239401\n",
      "val Loss: 0.00300325\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.00241330\n",
      "val Loss: 0.00300625\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.00237547\n",
      "val Loss: 0.00300560\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.00238811\n",
      "val Loss: 0.00300620\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.00239908\n",
      "val Loss: 0.00300699\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.00236386\n",
      "val Loss: 0.00300424\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.00235918\n",
      "val Loss: 0.00300431\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.00235217\n",
      "val Loss: 0.00300317\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.00240330\n",
      "val Loss: 0.00300375\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.00238578\n",
      "val Loss: 0.00300195\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.00237018\n",
      "val Loss: 0.00300206\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.00238026\n",
      "val Loss: 0.00300255\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.00235100\n",
      "val Loss: 0.00300171\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.00235092\n",
      "val Loss: 0.00300243\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.00238586\n",
      "val Loss: 0.00300226\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.00237720\n",
      "val Loss: 0.00300115\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.00239507\n",
      "val Loss: 0.00300273\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.00239059\n",
      "val Loss: 0.00300347\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.00235843\n",
      "val Loss: 0.00300347\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.00239003\n",
      "val Loss: 0.00300342\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.00237645\n",
      "val Loss: 0.00300368\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.00237371\n",
      "val Loss: 0.00300406\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.00239311\n",
      "val Loss: 0.00300440\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.00237285\n",
      "val Loss: 0.00300429\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.00239985\n",
      "val Loss: 0.00300614\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.00239628\n",
      "val Loss: 0.00300548\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.00239091\n",
      "val Loss: 0.00300608\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.00236013\n",
      "val Loss: 0.00300409\n",
      "Epoch 225/499\n",
      "----------\n",
      "train Loss: 0.00237602\n",
      "val Loss: 0.00300367\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.00237526\n",
      "val Loss: 0.00300289\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.00235490\n",
      "val Loss: 0.00300386\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.00237450\n",
      "val Loss: 0.00300486\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.00237946\n",
      "val Loss: 0.00300799\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.00236817\n",
      "val Loss: 0.00300477\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.00238995\n",
      "val Loss: 0.00300488\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.00234185\n",
      "val Loss: 0.00300351\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.00238545\n",
      "val Loss: 0.00300377\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.00235373\n",
      "val Loss: 0.00300097\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.00238241\n",
      "val Loss: 0.00300116\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.00235327\n",
      "val Loss: 0.00300074\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.00233274\n",
      "val Loss: 0.00300080\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.00236294\n",
      "val Loss: 0.00299971\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.00234170\n",
      "val Loss: 0.00299986\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.00239018\n",
      "val Loss: 0.00300055\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.00236829\n",
      "val Loss: 0.00300266\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.00235949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00300282\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.00239025\n",
      "val Loss: 0.00300340\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.00236517\n",
      "val Loss: 0.00300150\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.00236713\n",
      "val Loss: 0.00300138\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.00237682\n",
      "val Loss: 0.00300127\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.00236081\n",
      "val Loss: 0.00300219\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.00237723\n",
      "val Loss: 0.00300170\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.00235206\n",
      "val Loss: 0.00300306\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.00237189\n",
      "val Loss: 0.00300287\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.00239061\n",
      "val Loss: 0.00300367\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.00238389\n",
      "val Loss: 0.00300142\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.00236556\n",
      "val Loss: 0.00300279\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.00236854\n",
      "val Loss: 0.00300353\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.00235443\n",
      "val Loss: 0.00300380\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.00233533\n",
      "val Loss: 0.00300247\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.00233308\n",
      "val Loss: 0.00300282\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.00236570\n",
      "val Loss: 0.00300123\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.00236154\n",
      "val Loss: 0.00300125\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.00235111\n",
      "val Loss: 0.00300318\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.00237749\n",
      "val Loss: 0.00300400\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.00235026\n",
      "val Loss: 0.00300331\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.00235558\n",
      "val Loss: 0.00300266\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.00239729\n",
      "val Loss: 0.00300246\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.00239011\n",
      "val Loss: 0.00300247\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.00235814\n",
      "val Loss: 0.00300162\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.00238462\n",
      "val Loss: 0.00300209\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.00237929\n",
      "val Loss: 0.00300157\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.00240258\n",
      "val Loss: 0.00300291\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.00238903\n",
      "val Loss: 0.00300335\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.00239949\n",
      "val Loss: 0.00300499\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.00234522\n",
      "val Loss: 0.00300260\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.00236253\n",
      "val Loss: 0.00300246\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.00237240\n",
      "val Loss: 0.00300293\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.00236283\n",
      "val Loss: 0.00300390\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.00237142\n",
      "val Loss: 0.00300492\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.00235459\n",
      "val Loss: 0.00300290\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.00241634\n",
      "val Loss: 0.00300487\n",
      "Epoch 279/499\n",
      "----------\n",
      "train Loss: 0.00238031\n",
      "val Loss: 0.00300459\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.00237796\n",
      "val Loss: 0.00300518\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.00234157\n",
      "val Loss: 0.00300259\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.00235465\n",
      "val Loss: 0.00300159\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.00237209\n",
      "val Loss: 0.00300171\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.00237079\n",
      "val Loss: 0.00300163\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.00236009\n",
      "val Loss: 0.00300208\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.00233696\n",
      "val Loss: 0.00300068\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.00236552\n",
      "val Loss: 0.00300205\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.00236265\n",
      "val Loss: 0.00300266\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.00238201\n",
      "val Loss: 0.00300409\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.00237097\n",
      "val Loss: 0.00300421\n",
      "Epoch 291/499\n",
      "----------\n",
      "train Loss: 0.00238594\n",
      "val Loss: 0.00300457\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.00239390\n",
      "val Loss: 0.00300513\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.00236325\n",
      "val Loss: 0.00300445\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.00236264\n",
      "val Loss: 0.00300305\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.00240328\n",
      "val Loss: 0.00300322\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.00236612\n",
      "val Loss: 0.00300359\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.00242338\n",
      "val Loss: 0.00300565\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.00237474\n",
      "val Loss: 0.00300476\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.00237691\n",
      "val Loss: 0.00300430\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.00236560\n",
      "val Loss: 0.00300559\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.00239146\n",
      "val Loss: 0.00300513\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.00241433\n",
      "val Loss: 0.00300595\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.00234865\n",
      "val Loss: 0.00300445\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.00238149\n",
      "val Loss: 0.00300512\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.00239576\n",
      "val Loss: 0.00300533\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.00239371\n",
      "val Loss: 0.00300561\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.00237936\n",
      "val Loss: 0.00300434\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.00240827\n",
      "val Loss: 0.00300473\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.00237440\n",
      "val Loss: 0.00300382\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.00238020\n",
      "val Loss: 0.00300209\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.00237194\n",
      "val Loss: 0.00300344\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.00236372\n",
      "val Loss: 0.00300159\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.00238195\n",
      "val Loss: 0.00300248\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.00234232\n",
      "val Loss: 0.00300123\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.00238455\n",
      "val Loss: 0.00300244\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.00236368\n",
      "val Loss: 0.00300178\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.00236964\n",
      "val Loss: 0.00300194\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.00234891\n",
      "val Loss: 0.00300320\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.00239787\n",
      "val Loss: 0.00300363\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.00236983\n",
      "val Loss: 0.00300355\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.00235561\n",
      "val Loss: 0.00300298\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.00238918\n",
      "val Loss: 0.00300492\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.00239496\n",
      "val Loss: 0.00300567\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.00237700\n",
      "val Loss: 0.00300343\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.00238749\n",
      "val Loss: 0.00300294\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.00235524\n",
      "val Loss: 0.00300089\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.00235810\n",
      "val Loss: 0.00300124\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.00236794\n",
      "val Loss: 0.00300118\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.00237347\n",
      "val Loss: 0.00300142\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.00235885\n",
      "val Loss: 0.00300270\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.00235167\n",
      "val Loss: 0.00300117\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.00238091\n",
      "val Loss: 0.00300279\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.00236818\n",
      "val Loss: 0.00300261\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.00242158\n",
      "val Loss: 0.00300430\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.00239192\n",
      "val Loss: 0.00300444\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.00237313\n",
      "val Loss: 0.00300435\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.00235743\n",
      "val Loss: 0.00300408\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.00237938\n",
      "val Loss: 0.00300477\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.00237522\n",
      "val Loss: 0.00300459\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.00234597\n",
      "val Loss: 0.00300555\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.00239027\n",
      "val Loss: 0.00300620\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.00234871\n",
      "val Loss: 0.00300460\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.00234151\n",
      "val Loss: 0.00300286\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.00235615\n",
      "val Loss: 0.00300294\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.00238195\n",
      "val Loss: 0.00300314\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.00239069\n",
      "val Loss: 0.00300372\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.00234528\n",
      "val Loss: 0.00300258\n",
      "Epoch 348/499\n",
      "----------\n",
      "train Loss: 0.00236633\n",
      "val Loss: 0.00300278\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.00237222\n",
      "val Loss: 0.00300337\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.00238044\n",
      "val Loss: 0.00300337\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.00236328\n",
      "val Loss: 0.00300358\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.00240981\n",
      "val Loss: 0.00300389\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.00236444\n",
      "val Loss: 0.00300420\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.00236234\n",
      "val Loss: 0.00300427\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.00235319\n",
      "val Loss: 0.00300267\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.00239341\n",
      "val Loss: 0.00300216\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.00234772\n",
      "val Loss: 0.00300307\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.00237406\n",
      "val Loss: 0.00300409\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.00238247\n",
      "val Loss: 0.00300511\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.00239262\n",
      "val Loss: 0.00300499\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.00235183\n",
      "val Loss: 0.00300548\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.00233332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00300489\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.00236488\n",
      "val Loss: 0.00300463\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.00238138\n",
      "val Loss: 0.00300554\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.00234251\n",
      "val Loss: 0.00300493\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.00236695\n",
      "val Loss: 0.00300420\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.00238464\n",
      "val Loss: 0.00300515\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.00237343\n",
      "val Loss: 0.00300387\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.00239682\n",
      "val Loss: 0.00300380\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.00236651\n",
      "val Loss: 0.00300459\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.00236379\n",
      "val Loss: 0.00300276\n",
      "Epoch 372/499\n",
      "----------\n",
      "train Loss: 0.00239560\n",
      "val Loss: 0.00300426\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.00237767\n",
      "val Loss: 0.00300471\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.00237079\n",
      "val Loss: 0.00300223\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.00238227\n",
      "val Loss: 0.00300320\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.00236372\n",
      "val Loss: 0.00300131\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.00238532\n",
      "val Loss: 0.00300077\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.00238999\n",
      "val Loss: 0.00300236\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.00239549\n",
      "val Loss: 0.00300281\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.00239743\n",
      "val Loss: 0.00300460\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.00238061\n",
      "val Loss: 0.00300270\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.00238056\n",
      "val Loss: 0.00300219\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.00235913\n",
      "val Loss: 0.00300190\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.00238712\n",
      "val Loss: 0.00300371\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.00235261\n",
      "val Loss: 0.00300380\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.00237384\n",
      "val Loss: 0.00300397\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.00236818\n",
      "val Loss: 0.00300435\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.00236860\n",
      "val Loss: 0.00300350\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.00238119\n",
      "val Loss: 0.00300464\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.00238525\n",
      "val Loss: 0.00300454\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.00239038\n",
      "val Loss: 0.00300425\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.00236460\n",
      "val Loss: 0.00300315\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.00236284\n",
      "val Loss: 0.00300140\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.00239265\n",
      "val Loss: 0.00300123\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.00238234\n",
      "val Loss: 0.00300261\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.00236696\n",
      "val Loss: 0.00300412\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.00233974\n",
      "val Loss: 0.00300155\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.00238971\n",
      "val Loss: 0.00300325\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.00234163\n",
      "val Loss: 0.00300264\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.00239658\n",
      "val Loss: 0.00300328\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.00237301\n",
      "val Loss: 0.00300301\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.00239995\n",
      "val Loss: 0.00300247\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.00236682\n",
      "val Loss: 0.00300248\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.00239953\n",
      "val Loss: 0.00300410\n",
      "Epoch 405/499\n",
      "----------\n",
      "train Loss: 0.00237733\n",
      "val Loss: 0.00300377\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.00233486\n",
      "val Loss: 0.00300216\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.00234886\n",
      "val Loss: 0.00300148\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.00238379\n",
      "val Loss: 0.00300263\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.00238085\n",
      "val Loss: 0.00300209\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.00237199\n",
      "val Loss: 0.00300289\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.00239478\n",
      "val Loss: 0.00300539\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.00235780\n",
      "val Loss: 0.00300359\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.00242444\n",
      "val Loss: 0.00300524\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.00236797\n",
      "val Loss: 0.00300485\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.00237473\n",
      "val Loss: 0.00300453\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.00238351\n",
      "val Loss: 0.00300486\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.00234021\n",
      "val Loss: 0.00300282\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.00237969\n",
      "val Loss: 0.00300399\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.00236195\n",
      "val Loss: 0.00300281\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.00239316\n",
      "val Loss: 0.00300428\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.00239122\n",
      "val Loss: 0.00300567\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.00240535\n",
      "val Loss: 0.00300596\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.00234378\n",
      "val Loss: 0.00300396\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.00238334\n",
      "val Loss: 0.00300406\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.00238676\n",
      "val Loss: 0.00300274\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.00236061\n",
      "val Loss: 0.00300272\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.00237245\n",
      "val Loss: 0.00300418\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.00235082\n",
      "val Loss: 0.00300384\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.00236981\n",
      "val Loss: 0.00300345\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.00237500\n",
      "val Loss: 0.00300222\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.00238462\n",
      "val Loss: 0.00300240\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.00238669\n",
      "val Loss: 0.00300309\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.00236405\n",
      "val Loss: 0.00300275\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.00236414\n",
      "val Loss: 0.00300401\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.00237516\n",
      "val Loss: 0.00300359\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.00236186\n",
      "val Loss: 0.00300279\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.00237684\n",
      "val Loss: 0.00300411\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.00238859\n",
      "val Loss: 0.00300404\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.00237973\n",
      "val Loss: 0.00300434\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.00239649\n",
      "val Loss: 0.00300468\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.00235696\n",
      "val Loss: 0.00300460\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.00237795\n",
      "val Loss: 0.00300475\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.00240848\n",
      "val Loss: 0.00300554\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.00240821\n",
      "val Loss: 0.00300729\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.00236336\n",
      "val Loss: 0.00300664\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.00238971\n",
      "val Loss: 0.00300724\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.00237663\n",
      "val Loss: 0.00300617\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.00237098\n",
      "val Loss: 0.00300543\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.00237748\n",
      "val Loss: 0.00300538\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.00235096\n",
      "val Loss: 0.00300385\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.00237033\n",
      "val Loss: 0.00300381\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.00235294\n",
      "val Loss: 0.00300287\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.00239582\n",
      "val Loss: 0.00300293\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.00235094\n",
      "val Loss: 0.00300101\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.00236215\n",
      "val Loss: 0.00300038\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.00237598\n",
      "val Loss: 0.00300024\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.00238826\n",
      "val Loss: 0.00300185\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.00239961\n",
      "val Loss: 0.00300348\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.00235871\n",
      "val Loss: 0.00300363\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.00239105\n",
      "val Loss: 0.00300506\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.00236007\n",
      "val Loss: 0.00300444\n",
      "Epoch 462/499\n",
      "----------\n",
      "train Loss: 0.00238378\n",
      "val Loss: 0.00300284\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.00237447\n",
      "val Loss: 0.00300315\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.00238884\n",
      "val Loss: 0.00300297\n",
      "Epoch 465/499\n",
      "----------\n",
      "train Loss: 0.00237856\n",
      "val Loss: 0.00300513\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.00233498\n",
      "val Loss: 0.00300314\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.00235899\n",
      "val Loss: 0.00300319\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.00239218\n",
      "val Loss: 0.00300472\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.00239935\n",
      "val Loss: 0.00300698\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.00235124\n",
      "val Loss: 0.00300330\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.00236958\n",
      "val Loss: 0.00300443\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.00239727\n",
      "val Loss: 0.00300574\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.00235501\n",
      "val Loss: 0.00300449\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.00237764\n",
      "val Loss: 0.00300157\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.00239741\n",
      "val Loss: 0.00300306\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.00240538\n",
      "val Loss: 0.00300341\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.00238005\n",
      "val Loss: 0.00300383\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.00239756\n",
      "val Loss: 0.00300308\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.00236933\n",
      "val Loss: 0.00300258\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.00238290\n",
      "val Loss: 0.00300151\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.00237785\n",
      "val Loss: 0.00300120\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.00234937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00300125\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.00237614\n",
      "val Loss: 0.00300182\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.00238978\n",
      "val Loss: 0.00300207\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.00235152\n",
      "val Loss: 0.00300139\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.00235882\n",
      "val Loss: 0.00300141\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.00234249\n",
      "val Loss: 0.00300275\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.00234595\n",
      "val Loss: 0.00300086\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.00239838\n",
      "val Loss: 0.00300164\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.00238936\n",
      "val Loss: 0.00300218\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.00238117\n",
      "val Loss: 0.00300310\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.00237789\n",
      "val Loss: 0.00300425\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.00236517\n",
      "val Loss: 0.00300451\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.00238074\n",
      "val Loss: 0.00300431\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.00237997\n",
      "val Loss: 0.00300374\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.00239246\n",
      "val Loss: 0.00300293\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.00234506\n",
      "val Loss: 0.00300273\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.00234836\n",
      "val Loss: 0.00300348\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.00238803\n",
      "val Loss: 0.00300409\n"
     ]
    }
   ],
   "source": [
    "model,loss_report = train_ae_model(net=model,data_loaders=dataloaders_train,\n",
    "                             optimizer=optimizer,loss_function=loss_function,\n",
    "                            n_epochs=epochs,scheduler=exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'train'): 0.002393408919926043,\n",
       " (0, 'val'): 0.0030007853552147194,\n",
       " (1, 'train'): 0.002386652209140636,\n",
       " (1, 'val'): 0.003002157089886842,\n",
       " (2, 'train'): 0.0023821413654972006,\n",
       " (2, 'val'): 0.0030030528152430497,\n",
       " (3, 'train'): 0.0024089145439642446,\n",
       " (3, 'val'): 0.0030055777342231187,\n",
       " (4, 'train'): 0.0023925672112791625,\n",
       " (4, 'val'): 0.003003882313216174,\n",
       " (5, 'train'): 0.0023637062145604026,\n",
       " (5, 'val'): 0.0030035423459830106,\n",
       " (6, 'train'): 0.002393906867062604,\n",
       " (6, 'val'): 0.00300439750706708,\n",
       " (7, 'train'): 0.0023767241725215207,\n",
       " (7, 'val'): 0.003005278331262094,\n",
       " (8, 'train'): 0.0023856729407001425,\n",
       " (8, 'val'): 0.003004044570304729,\n",
       " (9, 'train'): 0.002370473411348131,\n",
       " (9, 'val'): 0.0030045305137281065,\n",
       " (10, 'train'): 0.0023415664003954995,\n",
       " (10, 'val'): 0.0030041414278524892,\n",
       " (11, 'train'): 0.0023525947773898087,\n",
       " (11, 'val'): 0.003002960924749021,\n",
       " (12, 'train'): 0.002401621874283861,\n",
       " (12, 'val'): 0.0030056925283537973,\n",
       " (13, 'train'): 0.0023559045598462777,\n",
       " (13, 'val'): 0.003004430620758622,\n",
       " (14, 'train'): 0.0023522637094612474,\n",
       " (14, 'val'): 0.0030020039390634607,\n",
       " (15, 'train'): 0.0023464234200892627,\n",
       " (15, 'val'): 0.0030008435801223473,\n",
       " (16, 'train'): 0.0023775969252542214,\n",
       " (16, 'val'): 0.0030030644050350894,\n",
       " (17, 'train'): 0.002385004182104711,\n",
       " (17, 'val'): 0.0030028913859967833,\n",
       " (18, 'train'): 0.0023662938425938287,\n",
       " (18, 'val'): 0.0030024967811725757,\n",
       " (19, 'train'): 0.0023544573535521827,\n",
       " (19, 'val'): 0.003000082517111743,\n",
       " (20, 'train'): 0.0023516543485500195,\n",
       " (20, 'val'): 0.003000951751514717,\n",
       " (21, 'train'): 0.002380788188289713,\n",
       " (21, 'val'): 0.003002561628818512,\n",
       " (22, 'train'): 0.002375599479785672,\n",
       " (22, 'val'): 0.003001624235400447,\n",
       " (23, 'train'): 0.0023635944558514487,\n",
       " (23, 'val'): 0.003002136669777058,\n",
       " (24, 'train'): 0.0023814595683857246,\n",
       " (24, 'val'): 0.003003677836170903,\n",
       " (25, 'train'): 0.0023793733368317285,\n",
       " (25, 'val'): 0.0030037606203997575,\n",
       " (26, 'train'): 0.0024055469505212925,\n",
       " (26, 'val'): 0.0030049121490231265,\n",
       " (27, 'train'): 0.0023552584289400665,\n",
       " (27, 'val'): 0.003003419273429447,\n",
       " (28, 'train'): 0.0023799184019918794,\n",
       " (28, 'val'): 0.0030035318599806893,\n",
       " (29, 'train'): 0.0023491988993353313,\n",
       " (29, 'val'): 0.003001857962873247,\n",
       " (30, 'train'): 0.0023638907544038914,\n",
       " (30, 'val'): 0.0030006758040852016,\n",
       " (31, 'train'): 0.0023618680597455415,\n",
       " (31, 'val'): 0.0030011170440249974,\n",
       " (32, 'train'): 0.002365974571417879,\n",
       " (32, 'val'): 0.0030016532098805465,\n",
       " (33, 'train'): 0.0023504690853533923,\n",
       " (33, 'val'): 0.0030006959482475563,\n",
       " (34, 'train'): 0.0023551145223555743,\n",
       " (34, 'val'): 0.0030001465369153906,\n",
       " (35, 'train'): 0.002353012078890094,\n",
       " (35, 'val'): 0.003000943197144402,\n",
       " (36, 'train'): 0.002364334891791697,\n",
       " (36, 'val'): 0.0030013173818588257,\n",
       " (37, 'train'): 0.002364855052696334,\n",
       " (37, 'val'): 0.0030018041531244912,\n",
       " (38, 'train'): 0.0023645633072764786,\n",
       " (38, 'val'): 0.0030008239878548514,\n",
       " (39, 'train'): 0.002364319438735644,\n",
       " (39, 'val'): 0.003002597501984349,\n",
       " (40, 'train'): 0.0023610957518771843,\n",
       " (40, 'val'): 0.003004265880143201,\n",
       " (41, 'train'): 0.0023565439990273227,\n",
       " (41, 'val'): 0.0030025254797052454,\n",
       " (42, 'train'): 0.0023908118406931558,\n",
       " (42, 'val'): 0.003002684701372076,\n",
       " (43, 'train'): 0.0023729147872439136,\n",
       " (43, 'val'): 0.0030033420081491823,\n",
       " (44, 'train'): 0.0023619026911479457,\n",
       " (44, 'val'): 0.0030034529390158475,\n",
       " (45, 'train'): 0.0023874161695992507,\n",
       " (45, 'val'): 0.003004656897650825,\n",
       " (46, 'train'): 0.0023652542106531284,\n",
       " (46, 'val'): 0.0030037437876065574,\n",
       " (47, 'train'): 0.002374579026191323,\n",
       " (47, 'val'): 0.0030037611722946167,\n",
       " (48, 'train'): 0.002368474517155577,\n",
       " (48, 'val'): 0.0030040953446317602,\n",
       " (49, 'train'): 0.002379463157720036,\n",
       " (49, 'val'): 0.0030030541949801976,\n",
       " (50, 'train'): 0.0023633333405962695,\n",
       " (50, 'val'): 0.003001263020215211,\n",
       " (51, 'train'): 0.0023639298009651677,\n",
       " (51, 'val'): 0.0030025737705054105,\n",
       " (52, 'train'): 0.0023568361583683225,\n",
       " (52, 'val'): 0.003002786525973567,\n",
       " (53, 'train'): 0.002365001649768264,\n",
       " (53, 'val'): 0.003003334281621156,\n",
       " (54, 'train'): 0.0023429388249361954,\n",
       " (54, 'val'): 0.0030029824486485232,\n",
       " (55, 'train'): 0.0023800562377329225,\n",
       " (55, 'val'): 0.003002781007024977,\n",
       " (56, 'train'): 0.0023776018233210953,\n",
       " (56, 'val'): 0.0030025646642402367,\n",
       " (57, 'train'): 0.0023978843043247857,\n",
       " (57, 'val'): 0.0030034935032879867,\n",
       " (58, 'train'): 0.0023573493516003646,\n",
       " (58, 'val'): 0.003001891628459648,\n",
       " (59, 'train'): 0.002385218041362586,\n",
       " (59, 'val'): 0.0030049209793408713,\n",
       " (60, 'train'): 0.0024054707890307464,\n",
       " (60, 'val'): 0.0030038075314627755,\n",
       " (61, 'train'): 0.002363540922050123,\n",
       " (61, 'val'): 0.003003304203351339,\n",
       " (62, 'train'): 0.002355852957676958,\n",
       " (62, 'val'): 0.0030021209407735754,\n",
       " (63, 'train'): 0.0023586056022732345,\n",
       " (63, 'val'): 0.0030034286556420505,\n",
       " (64, 'train'): 0.0023779164723776005,\n",
       " (64, 'val'): 0.003003377053472731,\n",
       " (65, 'train'): 0.002367076705451365,\n",
       " (65, 'val'): 0.003003415134218004,\n",
       " (66, 'train'): 0.002363386943384453,\n",
       " (66, 'val'): 0.0030033218639868276,\n",
       " (67, 'train'): 0.00234981219249743,\n",
       " (67, 'val'): 0.0030035497965636075,\n",
       " (68, 'train'): 0.002389496468283512,\n",
       " (68, 'val'): 0.0030039766872370683,\n",
       " (69, 'train'): 0.0023755433934706228,\n",
       " (69, 'val'): 0.0030025149937029237,\n",
       " (70, 'train'): 0.002406586789422565,\n",
       " (70, 'val'): 0.0030050324621023953,\n",
       " (71, 'train'): 0.0023496248931796464,\n",
       " (71, 'val'): 0.0030055934632266007,\n",
       " (72, 'train'): 0.002357336520044892,\n",
       " (72, 'val'): 0.0030046991176075405,\n",
       " (73, 'train'): 0.0023671736319859824,\n",
       " (73, 'val'): 0.0030044005424888047,\n",
       " (74, 'train'): 0.002370379244287809,\n",
       " (74, 'val'): 0.003004143359484496,\n",
       " (75, 'train'): 0.0023653745927192547,\n",
       " (75, 'val'): 0.0030026526914702523,\n",
       " (76, 'train'): 0.002371377139179795,\n",
       " (76, 'val'): 0.003002921188319171,\n",
       " (77, 'train'): 0.0023787067858157336,\n",
       " (77, 'val'): 0.003001441558202108,\n",
       " (78, 'train'): 0.0023767009929374413,\n",
       " (78, 'val'): 0.003002006146642897,\n",
       " (79, 'train'): 0.002360804972273332,\n",
       " (79, 'val'): 0.0030019628228964627,\n",
       " (80, 'train'): 0.002359310785929362,\n",
       " (80, 'val'): 0.003002122872405582,\n",
       " (81, 'train'): 0.002356274053454399,\n",
       " (81, 'val'): 0.003002788457605574,\n",
       " (82, 'train'): 0.0023650679461382053,\n",
       " (82, 'val'): 0.0030034352783803587,\n",
       " (83, 'train'): 0.0023856362396920167,\n",
       " (83, 'val'): 0.0030033831243161803,\n",
       " (84, 'train'): 0.002404475584626198,\n",
       " (84, 'val'): 0.0030044645622924523,\n",
       " (85, 'train'): 0.0023578904845096447,\n",
       " (85, 'val'): 0.0030033662915229797,\n",
       " (86, 'train'): 0.002425193441686807,\n",
       " (86, 'val'): 0.003005445279456951,\n",
       " (87, 'train'): 0.0023818908742180575,\n",
       " (87, 'val'): 0.0030051908559269374,\n",
       " (88, 'train'): 0.0023851391203977444,\n",
       " (88, 'val'): 0.00300650546948115,\n",
       " (89, 'train'): 0.002371634460157818,\n",
       " (89, 'val'): 0.0030067571335368687,\n",
       " (90, 'train'): 0.0023868684829385194,\n",
       " (90, 'val'): 0.003004969546088466,\n",
       " (91, 'train'): 0.002351301480774526,\n",
       " (91, 'val'): 0.0030033260031982704,\n",
       " (92, 'train'): 0.0023744180108662004,\n",
       " (92, 'val'): 0.0030032462543911403,\n",
       " (93, 'train'): 0.002360344485000328,\n",
       " (93, 'val'): 0.00300387348289843,\n",
       " (94, 'train'): 0.002344314491859189,\n",
       " (94, 'val'): 0.0030037915265118636,\n",
       " (95, 'train'): 0.002381679912408193,\n",
       " (95, 'val'): 0.0030048594430640892,\n",
       " (96, 'train'): 0.0024020985734683497,\n",
       " (96, 'val'): 0.003006151152981652,\n",
       " (97, 'train'): 0.002360387739759904,\n",
       " (97, 'val'): 0.0030043100317319236,\n",
       " (98, 'train'): 0.002360282948723546,\n",
       " (98, 'val'): 0.0030053451105400367,\n",
       " (99, 'train'): 0.002419576738719587,\n",
       " (99, 'val'): 0.0030055998100174796,\n",
       " (100, 'train'): 0.0024096444249153137,\n",
       " (100, 'val'): 0.003006204134888119,\n",
       " (101, 'train'): 0.002358211894278173,\n",
       " (101, 'val'): 0.003004627095328437,\n",
       " (102, 'train'): 0.0023887401653660666,\n",
       " (102, 'val'): 0.003004669867180012,\n",
       " (103, 'train'): 0.00236738452480899,\n",
       " (103, 'val'): 0.003003291509769581,\n",
       " (104, 'train'): 0.002319811670868485,\n",
       " (104, 'val'): 0.00300218082136578,\n",
       " (105, 'train'): 0.002396160046811457,\n",
       " (105, 'val'): 0.003002722782117349,\n",
       " (106, 'train'): 0.002384806258810891,\n",
       " (106, 'val'): 0.003003238527863114,\n",
       " (107, 'train'): 0.002339869599651407,\n",
       " (107, 'val'): 0.0030029485071146928,\n",
       " (108, 'train'): 0.0023569132166880147,\n",
       " (108, 'val'): 0.0030031847181143582,\n",
       " (109, 'train'): 0.002363661511076821,\n",
       " (109, 'val'): 0.003003436382170077,\n",
       " (110, 'train'): 0.002338982980560373,\n",
       " (110, 'val'): 0.0030012481190540173,\n",
       " (111, 'train'): 0.0023561431163990937,\n",
       " (111, 'val'): 0.0030016554174599826,\n",
       " (112, 'train'): 0.002388622818721665,\n",
       " (112, 'val'): 0.0030038166377279493,\n",
       " (113, 'train'): 0.0023601698792643016,\n",
       " (113, 'val'): 0.003003787387300421,\n",
       " (114, 'train'): 0.002412639696288992,\n",
       " (114, 'val'): 0.003005803735167892,\n",
       " (115, 'train'): 0.0023422192920137335,\n",
       " (115, 'val'): 0.0030028422673543296,\n",
       " (116, 'train'): 0.0023490146354392723,\n",
       " (116, 'val'): 0.0030031858219040763,\n",
       " (117, 'train'): 0.002355040223510177,\n",
       " (117, 'val'): 0.0030024371765278004,\n",
       " (118, 'train'): 0.002377370717348876,\n",
       " (118, 'val'): 0.00300411824826841,\n",
       " (119, 'train'): 0.0024060355844321073,\n",
       " (119, 'val'): 0.003004946090556957,\n",
       " (120, 'train'): 0.002357319273330547,\n",
       " (120, 'val'): 0.0030043892286441944,\n",
       " (121, 'train'): 0.0023633176115927875,\n",
       " (121, 'val'): 0.0030054347934546292,\n",
       " (122, 'train'): 0.0023773280144841584,\n",
       " (122, 'val'): 0.0030041715061223067,\n",
       " (123, 'train'): 0.0023609414282772276,\n",
       " (123, 'val'): 0.003002201517422994,\n",
       " (124, 'train'): 0.002380881803455176,\n",
       " (124, 'val'): 0.003002208416108732,\n",
       " (125, 'train'): 0.0023817433803169814,\n",
       " (125, 'val'): 0.00300345735417472,\n",
       " (126, 'train'): 0.0023460606182063065,\n",
       " (126, 'val'): 0.003001802221492485,\n",
       " (127, 'train'): 0.002406632113787863,\n",
       " (127, 'val'): 0.0030027076050087257,\n",
       " (128, 'train'): 0.0023363676887971385,\n",
       " (128, 'val'): 0.003001165058877733,\n",
       " (129, 'train'): 0.002362183329683763,\n",
       " (129, 'val'): 0.0030006907052463954,\n",
       " (130, 'train'): 0.0024056439460427674,\n",
       " (130, 'val'): 0.0030025334821807016,\n",
       " (131, 'train'): 0.0023723625474505955,\n",
       " (131, 'val'): 0.003003652173059958,\n",
       " (132, 'train'): 0.0023844966457949746,\n",
       " (132, 'val'): 0.003003826019940553,\n",
       " (133, 'train'): 0.0023732994579606587,\n",
       " (133, 'val'): 0.003003198791433264,\n",
       " (134, 'train'): 0.0023648829923735726,\n",
       " (134, 'val'): 0.003002136669777058,\n",
       " (135, 'train'): 0.0023753927261741074,\n",
       " (135, 'val'): 0.003002521892388662,\n",
       " (136, 'train'): 0.0023329821587712678,\n",
       " (136, 'val'): 0.0030018251251291346,\n",
       " (137, 'train'): 0.002378068588398121,\n",
       " (137, 'val'): 0.0030019951087457163,\n",
       " (138, 'train'): 0.002388471806490863,\n",
       " (138, 'val'): 0.0030016126456084073,\n",
       " (139, 'train'): 0.0024052701752494882,\n",
       " (139, 'val'): 0.0030040045579274497,\n",
       " (140, 'train'): 0.0023350903971327674,\n",
       " (140, 'val'): 0.00300426643203806,\n",
       " (141, 'train'): 0.0023459415468904707,\n",
       " (141, 'val'): 0.0030034264480626144,\n",
       " (142, 'train'): 0.00237666339510017,\n",
       " (142, 'val'): 0.0030027881816581444,\n",
       " (143, 'train'): 0.002374977356305829,\n",
       " (143, 'val'): 0.003005086271851151,\n",
       " (144, 'train'): 0.0024010923311666207,\n",
       " (144, 'val'): 0.0030051737471863075,\n",
       " (145, 'train'): 0.002346329391002655,\n",
       " (145, 'val'): 0.003003008111759468,\n",
       " (146, 'train'): 0.0024160946270933856,\n",
       " (146, 'val'): 0.0030047647930957653,\n",
       " (147, 'train'): 0.0023556475838025412,\n",
       " (147, 'val'): 0.003003619611263275,\n",
       " (148, 'train'): 0.002377488408927564,\n",
       " (148, 'val'): 0.0030050382569984154,\n",
       " (149, 'train'): 0.0023646480231373397,\n",
       " (149, 'val'): 0.0030052468732551293,\n",
       " (150, 'train'): 0.0024018680883778464,\n",
       " (150, 'val'): 0.0030056182984952574,\n",
       " (151, 'train'): 0.002384550317570015,\n",
       " (151, 'val'): 0.003005750201366566,\n",
       " (152, 'train'): 0.0023944217849660803,\n",
       " (152, 'val'): 0.0030059786858382047,\n",
       " (153, 'train'): 0.0024370696671583035,\n",
       " (153, 'val'): 0.0030074285136328805,\n",
       " (154, 'train'): 0.0023549048023091424,\n",
       " (154, 'val'): 0.0030067662398020425,\n",
       " (155, 'train'): 0.002349898771003441,\n",
       " (155, 'val'): 0.003007578077139678,\n",
       " (156, 'train'): 0.002385245015223821,\n",
       " (156, 'val'): 0.003005160225762261,\n",
       " (157, 'train'): 0.0024158895291663983,\n",
       " (157, 'val'): 0.0030052893691592747,\n",
       " (158, 'train'): 0.002387506059474415,\n",
       " (158, 'val'): 0.003004858339274371,\n",
       " (159, 'train'): 0.0023683582743008933,\n",
       " (159, 'val'): 0.0030043919881184897,\n",
       " (160, 'train'): 0.002370620767275492,\n",
       " (160, 'val'): 0.0030032639150266295,\n",
       " (161, 'train'): 0.002365649781293339,\n",
       " (161, 'val'): 0.003002463667481034,\n",
       " (162, 'train'): 0.0023595841808451545,\n",
       " (162, 'val'): 0.0030023198988702563,\n",
       " (163, 'train'): 0.0023500057006323777,\n",
       " (163, 'val'): 0.003003159330950843,\n",
       " (164, 'train'): 0.0023744743041418217,\n",
       " (164, 'val'): 0.0030042355259259543,\n",
       " (165, 'train'): 0.002375920268672484,\n",
       " (165, 'val'): 0.0030029460235878273,\n",
       " (166, 'train'): 0.002382122877019423,\n",
       " (166, 'val'): 0.003003951851968412,\n",
       " (167, 'train'): 0.0023568701688890105,\n",
       " (167, 'val'): 0.0030023640504589786,\n",
       " (168, 'train'): 0.002343792054388258,\n",
       " (168, 'val'): 0.002999755519407767,\n",
       " (169, 'train'): 0.0023736474966561355,\n",
       " (169, 'val'): 0.002999562632154535,\n",
       " (170, 'train'): 0.0023706600897841984,\n",
       " (170, 'val'): 0.0030027484452282943,\n",
       " (171, 'train'): 0.0023658877169644393,\n",
       " (171, 'val'): 0.0030010168751080832,\n",
       " (172, 'train'): 0.0023865381738653888,\n",
       " (172, 'val'): 0.00300094457688155,\n",
       " (173, 'train'): 0.002348008876045545,\n",
       " (173, 'val'): 0.0030010279130052637,\n",
       " (174, 'train'): 0.0023796787416493453,\n",
       " (174, 'val'): 0.003001059646959658,\n",
       " (175, 'train'): 0.0023564289289492147,\n",
       " (175, 'val'): 0.0029999773811410974,\n",
       " (176, 'train'): 0.002366819660420771,\n",
       " (176, 'val'): 0.0030005908122769107,\n",
       " (177, 'train'): 0.0023589221829617463,\n",
       " (177, 'val'): 0.0030009801740999574,\n",
       " (178, 'train'): 0.002384170475933287,\n",
       " (178, 'val'): 0.003002742374384845,\n",
       " (179, 'train'): 0.002376222155160374,\n",
       " (179, 'val'): 0.0030040354640395554,\n",
       " (180, 'train'): 0.002366602075872598,\n",
       " (180, 'val'): 0.0030051952710858096,\n",
       " (181, 'train'): 0.002363156251333378,\n",
       " (181, 'val'): 0.003003592568415183,\n",
       " (182, 'train'): 0.0023566981846535645,\n",
       " (182, 'val'): 0.0030030119750234815,\n",
       " (183, 'train'): 0.0023489579972293642,\n",
       " (183, 'val'): 0.0030027881816581444,\n",
       " (184, 'train'): 0.00236757796395708,\n",
       " (184, 'val'): 0.003002519960756655,\n",
       " (185, 'train'): 0.002366990954787643,\n",
       " (185, 'val'): 0.003002474429430785,\n",
       " (186, 'train'): 0.002364155939883656,\n",
       " (186, 'val'): 0.0030011879625143826,\n",
       " (187, 'train'): 0.002373292973196065,\n",
       " (187, 'val'): 0.003001541175224163,\n",
       " (188, 'train'): 0.0023622021630958275,\n",
       " (188, 'val'): 0.0030009255365089135,\n",
       " (189, 'train'): 0.002368781922592057,\n",
       " (189, 'val'): 0.003003025496447528,\n",
       " (190, 'train'): 0.0023714484715903245,\n",
       " (190, 'val'): 0.0030037962176181652,\n",
       " (191, 'train'): 0.0023375655076018085,\n",
       " (191, 'val'): 0.00300215819367656,\n",
       " (192, 'train'): 0.002382745345433553,\n",
       " (192, 'val'): 0.003002059128549364,\n",
       " (193, 'train'): 0.0024288725797776823,\n",
       " (193, 'val'): 0.0030035326878229776,\n",
       " (194, 'train'): 0.0023600950285240455,\n",
       " (194, 'val'): 0.00300239164520193,\n",
       " (195, 'train'): 0.0023671408632287274,\n",
       " (195, 'val'): 0.003003519442346361,\n",
       " (196, 'train'): 0.002362946738247518,\n",
       " (196, 'val'): 0.0030032250064390676,\n",
       " (197, 'train'): 0.0023940142106126856,\n",
       " (197, 'val'): 0.003003247634128288,\n",
       " (198, 'train'): 0.0024132983828032456,\n",
       " (198, 'val'): 0.00300625490921515,\n",
       " (199, 'train'): 0.0023754712332178045,\n",
       " (199, 'val'): 0.003005604777071211,\n",
       " (200, 'train'): 0.0023881102463713397,\n",
       " (200, 'val'): 0.0030061972362023814,\n",
       " (201, 'train'): 0.0023990768801282953,\n",
       " (201, 'val'): 0.0030069930685891044,\n",
       " (202, 'train'): 0.002363855502119771,\n",
       " (202, 'val'): 0.0030042366297156724,\n",
       " (203, 'train'): 0.0023591763305443303,\n",
       " (203, 'val'): 0.003004314722838225,\n",
       " (204, 'train'): 0.0023521718879540763,\n",
       " (204, 'val'): 0.003003173956164607,\n",
       " (205, 'train'): 0.0024033038428536166,\n",
       " (205, 'val'): 0.003003745167343705,\n",
       " (206, 'train'): 0.002385782422842803,\n",
       " (206, 'val'): 0.0030019459901032626,\n",
       " (207, 'train'): 0.002370177802664262,\n",
       " (207, 'val'): 0.003002056093127639,\n",
       " (208, 'train'): 0.002380260162883335,\n",
       " (208, 'val'): 0.0030025544541853444,\n",
       " (209, 'train'): 0.002351000146181495,\n",
       " (209, 'val'): 0.003001707571524161,\n",
       " (210, 'train'): 0.0023509246055726653,\n",
       " (210, 'val'): 0.0030024255867357606,\n",
       " (211, 'train'): 0.0023858636893607952,\n",
       " (211, 'val'): 0.0030022622258574876,\n",
       " (212, 'train'): 0.0023771988710871448,\n",
       " (212, 'val'): 0.003001154848822841,\n",
       " (213, 'train'): 0.0023950703304122995,\n",
       " (213, 'val'): 0.003002725265644215,\n",
       " (214, 'train'): 0.0023905938422238387,\n",
       " (214, 'val'): 0.0030034683920719006,\n",
       " (215, 'train'): 0.0023584285819972,\n",
       " (215, 'val'): 0.0030034678401770413,\n",
       " (216, 'train'): 0.0023900260114007527,\n",
       " (216, 'val'): 0.0030034209291140237,\n",
       " (217, 'train'): 0.0023764542269485967,\n",
       " (217, 'val'): 0.0030036750766966078,\n",
       " (218, 'train'): 0.0023737111025386387,\n",
       " (218, 'val'): 0.003004056711991628,\n",
       " (219, 'train'): 0.0023931053087667183,\n",
       " (219, 'val'): 0.0030043955754350733,\n",
       " (220, 'train'): 0.002372854423743707,\n",
       " (220, 'val'): 0.003004294302728441,\n",
       " (221, 'train'): 0.0023998508436812293,\n",
       " (221, 'val'): 0.003006140391031901,\n",
       " (222, 'train'): 0.0023962773934558586,\n",
       " (222, 'val'): 0.0030054781172010633,\n",
       " (223, 'train'): 0.0023909062837009078,\n",
       " (223, 'val'): 0.0030060821661242734,\n",
       " (224, 'train'): 0.002360130211821309,\n",
       " (224, 'val'): 0.0030040939648946128,\n",
       " (225, 'train'): 0.0023760234040242655,\n",
       " (225, 'val'): 0.003003669281800588,\n",
       " (226, 'train'): 0.0023752644106193824,\n",
       " (226, 'val'): 0.003002889178417347,\n",
       " (227, 'train'): 0.0023548954890833963,\n",
       " (227, 'val'): 0.0030038552703680814,\n",
       " (228, 'train'): 0.0023744996223184797,\n",
       " (228, 'val'): 0.0030048627544332434,\n",
       " (229, 'train'): 0.0023794621229171753,\n",
       " (229, 'val'): 0.0030079889628622266,\n",
       " (230, 'train'): 0.002368171802825398,\n",
       " (230, 'val'): 0.0030047744512557983,\n",
       " (231, 'train'): 0.0023899529543187884,\n",
       " (231, 'val'): 0.003004883450490457,\n",
       " (232, 'train'): 0.0023418537306564824,\n",
       " (232, 'val'): 0.003003512543660623,\n",
       " (233, 'train'): 0.002385452389717102,\n",
       " (233, 'val'): 0.0030037708304546497,\n",
       " (234, 'train'): 0.0023537294042331202,\n",
       " (234, 'val'): 0.0030009716197296425,\n",
       " (235, 'train'): 0.0023824122768861277,\n",
       " (235, 'val'): 0.003001155952612559,\n",
       " (236, 'train'): 0.002353269537841832,\n",
       " (236, 'val'): 0.0030007392719939904,\n",
       " (237, 'train'): 0.002332735392782423,\n",
       " (237, 'val'): 0.0030007974969016183,\n",
       " (238, 'train'): 0.002362944185733795,\n",
       " (238, 'val'): 0.0029997135753984804,\n",
       " (239, 'train'): 0.0023417014766622473,\n",
       " (239, 'val'): 0.0029998623110629894,\n",
       " (240, 'train'): 0.0023901779894475584,\n",
       " (240, 'val'): 0.0030005499720573425,\n",
       " (241, 'train'): 0.0023682871488509357,\n",
       " (241, 'val'): 0.0030026598661034194,\n",
       " (242, 'train'): 0.002359488289113398,\n",
       " (242, 'val'): 0.0030028177080331027,\n",
       " (243, 'train'): 0.0023902484940158,\n",
       " (243, 'val'): 0.003003398025477374,\n",
       " (244, 'train'): 0.002365167701133975,\n",
       " (244, 'val'): 0.003001497851477729,\n",
       " (245, 'train'): 0.0023671309291212646,\n",
       " (245, 'val'): 0.0030013761586613124,\n",
       " (246, 'train'): 0.0023768226857538576,\n",
       " (246, 'val'): 0.0030012707467432375,\n",
       " (247, 'train'): 0.0023608106981824945,\n",
       " (247, 'val'): 0.003002194066842397,\n",
       " (248, 'train'): 0.0023772348132398394,\n",
       " (248, 'val'): 0.003001701776628141,\n",
       " (249, 'train'): 0.00235206281973256,\n",
       " (249, 'val'): 0.0030030649569299486,\n",
       " (250, 'train'): 0.00237189129822784,\n",
       " (250, 'val'): 0.0030028734494138647,\n",
       " (251, 'train'): 0.0023906096402141782,\n",
       " (251, 'val'): 0.003003671489380024,\n",
       " (252, 'train'): 0.0023838910101740447,\n",
       " (252, 'val'): 0.003001415619143733,\n",
       " (253, 'train'): 0.002365555407272445,\n",
       " (253, 'val'): 0.003002786801920997,\n",
       " (254, 'train'): 0.002368539088854083,\n",
       " (254, 'val'): 0.0030035274448218167,\n",
       " (255, 'train'): 0.0023544305866515197,\n",
       " (255, 'val'): 0.003003803116303903,\n",
       " (256, 'train'): 0.0023353339207393153,\n",
       " (256, 'val'): 0.0030024683585873355,\n",
       " (257, 'train'): 0.0023330828795830407,\n",
       " (257, 'val'): 0.0030028204675073976,\n",
       " (258, 'train'): 0.002365704349897526,\n",
       " (258, 'val'): 0.0030012257673122265,\n",
       " (259, 'train'): 0.002361537681685554,\n",
       " (259, 'val'): 0.0030012494987911647,\n",
       " (260, 'train'): 0.0023511116289430195,\n",
       " (260, 'val'): 0.0030031841662194994,\n",
       " (261, 'train'): 0.0023774924791521495,\n",
       " (261, 'val'): 0.003003997383294282,\n",
       " (262, 'train'): 0.002350260745044108,\n",
       " (262, 'val'): 0.0030033144134062307,\n",
       " (263, 'train'): 0.0023555835639988937,\n",
       " (263, 'val'): 0.003002664005314862,\n",
       " (264, 'train'): 0.002397294466694196,\n",
       " (264, 'val'): 0.0030024589763747323,\n",
       " (265, 'train'): 0.0023901103133404694,\n",
       " (265, 'val'): 0.0030024689104821947,\n",
       " (266, 'train'): 0.0023581400789596417,\n",
       " (266, 'val'): 0.003001620924031293,\n",
       " (267, 'train'): 0.0023846160620450974,\n",
       " (267, 'val'): 0.0030020883789768924,\n",
       " (268, 'train'): 0.0023792881380628656,\n",
       " (268, 'val'): 0.003001568494019685,\n",
       " (269, 'train'): 0.002402581619443717,\n",
       " (269, 'val'): 0.00300291042636942,\n",
       " (270, 'train'): 0.002389026391837332,\n",
       " (270, 'val'): 0.0030033489068349204,\n",
       " (271, 'train'): 0.002399489352548564,\n",
       " (271, 'val'): 0.003004989414303391,\n",
       " (272, 'train'): 0.0023452171159011347,\n",
       " (272, 'val'): 0.003002601641195792,\n",
       " (273, 'train'): 0.0023625339208929626,\n",
       " (273, 'val'): 0.003002458424479873,\n",
       " (274, 'train'): 0.002372404560446739,\n",
       " (274, 'val'): 0.003002932502163781,\n",
       " (275, 'train'): 0.0023628339447357037,\n",
       " (275, 'val'): 0.003003899421956804,\n",
       " (276, 'train'): 0.002371415909793642,\n",
       " (276, 'val'): 0.00300492153123573,\n",
       " (277, 'train'): 0.0023545949823326534,\n",
       " (277, 'val'): 0.0030029010441568163,\n",
       " (278, 'train'): 0.0024163440145828106,\n",
       " (278, 'val'): 0.003004873792330424,\n",
       " (279, 'train'): 0.0023803069359726375,\n",
       " (279, 'val'): 0.0030045898424254525,\n",
       " (280, 'train'): 0.002377960072071464,\n",
       " (280, 'val'): 0.003005176506660603,\n",
       " (281, 'train'): 0.00234156743519836,\n",
       " (281, 'val'): 0.0030025892235614636,\n",
       " (282, 'train'): 0.0023546547249511437,\n",
       " (282, 'val'): 0.0030015916736037644,\n",
       " (283, 'train'): 0.002372090118350806,\n",
       " (283, 'val'): 0.003001706191787013,\n",
       " (284, 'train'): 0.0023707930274583676,\n",
       " (284, 'val'): 0.003001628650559319,\n",
       " (285, 'train'): 0.0023600893026148833,\n",
       " (285, 'val'): 0.00300207844486943,\n",
       " (286, 'train'): 0.0023369642181528937,\n",
       " (286, 'val'): 0.00300068242682351,\n",
       " (287, 'train'): 0.0023655171195665994,\n",
       " (287, 'val'): 0.003002052229863626,\n",
       " (288, 'train'): 0.002362649818813359,\n",
       " (288, 'val'): 0.003002659038261131,\n",
       " (289, 'train'): 0.0023820100835076083,\n",
       " (289, 'val'): 0.003004090929472888,\n",
       " (290, 'train'): 0.0023709688749578265,\n",
       " (290, 'val'): 0.0030042120703944455,\n",
       " (291, 'train'): 0.0023859407476804874,\n",
       " (291, 'val'): 0.003004568870420809,\n",
       " (292, 'train'): 0.002393902313930017,\n",
       " (292, 'val'): 0.0030051320791244507,\n",
       " (293, 'train'): 0.0023632540057102838,\n",
       " (293, 'val'): 0.003004453248447842,\n",
       " (294, 'train'): 0.0023626420232984754,\n",
       " (294, 'val'): 0.0030030495038738956,\n",
       " (295, 'train'): 0.0024032846645072655,\n",
       " (295, 'val'): 0.0030032153482790346,\n",
       " (296, 'train'): 0.0023661181330680847,\n",
       " (296, 'val'): 0.003003592568415183,\n",
       " (297, 'train'): 0.0024233813639040346,\n",
       " (297, 'val'): 0.0030056514121867993,\n",
       " (298, 'train'): 0.002374743145925027,\n",
       " (298, 'val'): 0.003004757066567739,\n",
       " (299, 'train'): 0.0023769058149169992,\n",
       " (299, 'val'): 0.0030043031330461854,\n",
       " (300, 'train'): 0.002365604594901756,\n",
       " (300, 'val'): 0.003005590703752306,\n",
       " (301, 'train'): 0.0023914634215610998,\n",
       " (301, 'val'): 0.0030051320791244507,\n",
       " (302, 'train'): 0.0024143286325313427,\n",
       " (302, 'val'): 0.003005945572146663,\n",
       " (303, 'train'): 0.0023486514886220298,\n",
       " (303, 'val'): 0.003004451040868406,\n",
       " (304, 'train'): 0.002381486749207532,\n",
       " (304, 'val'): 0.0030051235247541357,\n",
       " (305, 'train'): 0.0023957633723815284,\n",
       " (305, 'val'): 0.0030053315891159903,\n",
       " (306, 'train'): 0.0023937085988344968,\n",
       " (306, 'val'): 0.0030056075365455064,\n",
       " (307, 'train'): 0.002379355331261953,\n",
       " (307, 'val'): 0.0030043395581068814,\n",
       " (308, 'train'): 0.0024082718624009024,\n",
       " (308, 'val'): 0.0030047253326133446,\n",
       " (309, 'train'): 0.0023743976597432738,\n",
       " (309, 'val'): 0.0030038166377279493,\n",
       " (310, 'train'): 0.002380200627225417,\n",
       " (310, 'val'): 0.0030020894827666104,\n",
       " (311, 'train'): 0.002371942831410302,\n",
       " (311, 'val'): 0.0030034410732763784,\n",
       " (312, 'train'): 0.0023637178733392997,\n",
       " (312, 'val'): 0.0030015894660243283,\n",
       " (313, 'train'): 0.0023819544111137038,\n",
       " (313, 'val'): 0.0030024780167473685,\n",
       " (314, 'train'): 0.002342320219786079,\n",
       " (314, 'val'): 0.003001232665997964,\n",
       " (315, 'train'): 0.002384551697307163,\n",
       " (315, 'val'): 0.0030024388322123776,\n",
       " (316, 'train'): 0.002363682828015751,\n",
       " (316, 'val'): 0.0030017790419084056,\n",
       " (317, 'train'): 0.0023696412918744266,\n",
       " (317, 'val'): 0.00300194157494439,\n",
       " (318, 'train'): 0.0023489087406131956,\n",
       " (318, 'val'): 0.0030032001711704113,\n",
       " (319, 'train'): 0.002397870437966453,\n",
       " (319, 'val'): 0.0030036298213181674,\n",
       " (320, 'train'): 0.002369830039916215,\n",
       " (320, 'val'): 0.0030035509003533255,\n",
       " (321, 'train'): 0.0023556086752149793,\n",
       " (321, 'val'): 0.0030029785853845103,\n",
       " (322, 'train'): 0.0023891841647801578,\n",
       " (322, 'val'): 0.00300491601228714,\n",
       " (323, 'train'): 0.002394958847650775,\n",
       " (323, 'val'): 0.003005665209558275,\n",
       " (324, 'train'): 0.002376997498450456,\n",
       " (324, 'val'): 0.0030034330708009226,\n",
       " (325, 'train'): 0.002387491503247508,\n",
       " (325, 'val'): 0.0030029424362712437,\n",
       " (326, 'train'): 0.002355239595528002,\n",
       " (326, 'val'): 0.003000886351973922,\n",
       " (327, 'train'): 0.002358095582436632,\n",
       " (327, 'val'): 0.0030012354254722595,\n",
       " (328, 'train'): 0.002367938420286885,\n",
       " (328, 'val'): 0.003001183271408081,\n",
       " (329, 'train'): 0.0023734739257229697,\n",
       " (329, 'val'): 0.003001419206460317,\n",
       " (330, 'train'): 0.002358851264472361,\n",
       " (330, 'val'): 0.003002704569587001,\n",
       " (331, 'train'): 0.0023516685598426396,\n",
       " (331, 'val'): 0.003001173613248048,\n",
       " (332, 'train'): 0.0023809136753832854,\n",
       " (332, 'val'): 0.0030027909411324393,\n",
       " (333, 'train'): 0.0023681833926174375,\n",
       " (333, 'val'): 0.0030026057804072343,\n",
       " (334, 'train'): 0.0024215779094784347,\n",
       " (334, 'val'): 0.003004303960888474,\n",
       " (335, 'train'): 0.0023919177690037976,\n",
       " (335, 'val'): 0.0030044444181300976,\n",
       " (336, 'train'): 0.002373130578133795,\n",
       " (336, 'val'): 0.003004352527636069,\n",
       " (337, 'train'): 0.0023574336535400814,\n",
       " (337, 'val'): 0.003004082926997432,\n",
       " (338, 'train'): 0.002379380856399183,\n",
       " (338, 'val'): 0.0030047744512557983,\n",
       " (339, 'train'): 0.0023752181894249385,\n",
       " (339, 'val'): 0.003004591774057459,\n",
       " (340, 'train'): 0.002345969279607137,\n",
       " (340, 'val'): 0.0030055493116378784,\n",
       " (341, 'train'): 0.0023902730533370267,\n",
       " (341, 'val'): 0.0030062022032561124,\n",
       " (342, 'train'): 0.0023487129559119544,\n",
       " (342, 'val'): 0.0030046022600597804,\n",
       " (343, 'train'): 0.002341505553987291,\n",
       " (343, 'val'): 0.003002864067201261,\n",
       " (344, 'train'): 0.0023561522916511254,\n",
       " (344, 'val'): 0.0030029385730072303,\n",
       " (345, 'train'): 0.0023819510997445496,\n",
       " (345, 'val'): 0.0030031441538422195,\n",
       " (346, 'train'): 0.0023906860086652967,\n",
       " (346, 'val'): 0.003003716468811035,\n",
       " (347, 'train'): 0.0023452793420464906,\n",
       " (347, 'val'): 0.0030025762540322764,\n",
       " (348, 'train'): 0.002366329370825379,\n",
       " (348, 'val'): 0.003002784318394131,\n",
       " (349, 'train'): 0.002372221952235257,\n",
       " (349, 'val'): 0.0030033693269447045,\n",
       " (350, 'train'): 0.002380438976817661,\n",
       " (350, 'val'): 0.0030033687750498452,\n",
       " (351, 'train'): 0.0023632755985966434,\n",
       " (351, 'val'): 0.0030035823583602905,\n",
       " (352, 'train'): 0.0024098090275570198,\n",
       " (352, 'val'): 0.0030038911435339185,\n",
       " (353, 'train'): 0.0023644442359606424,\n",
       " (353, 'val'): 0.0030042029641292713,\n",
       " (354, 'train'): 0.002362338274165436,\n",
       " (354, 'val'): 0.0030042672598803483,\n",
       " (355, 'train'): 0.0023531856498232593,\n",
       " (355, 'val'): 0.0030026742153697545,\n",
       " (356, 'train'): 0.002393411334466051,\n",
       " (356, 'val'): 0.0030021601253085667,\n",
       " (357, 'train'): 0.0023477205799685586,\n",
       " (357, 'val'): 0.0030030743391425523,\n",
       " (358, 'train'): 0.0023740626595638416,\n",
       " (358, 'val'): 0.0030040870662088747,\n",
       " (359, 'train'): 0.002382469673951467,\n",
       " (359, 'val'): 0.0030051091754878007,\n",
       " (360, 'train'): 0.002392624056449643,\n",
       " (360, 'val'): 0.003004993553514834,\n",
       " (361, 'train'): 0.0023518322656551995,\n",
       " (361, 'val'): 0.003005480600727929,\n",
       " (362, 'train'): 0.0023333202633592817,\n",
       " (362, 'val'): 0.0030048873137544702,\n",
       " (363, 'train'): 0.0023648840961632907,\n",
       " (363, 'val'): 0.0030046279231707254,\n",
       " (364, 'train'): 0.0023813779569334453,\n",
       " (364, 'val'): 0.003005542137004711,\n",
       " (365, 'train'): 0.002342506484301002,\n",
       " (365, 'val'): 0.0030049253944997435,\n",
       " (366, 'train'): 0.0023669508044366484,\n",
       " (366, 'val'): 0.00300419799707554,\n",
       " (367, 'train'): 0.0023846372410103126,\n",
       " (367, 'val'): 0.0030051475321805038,\n",
       " (368, 'train'): 0.0023734257728965195,\n",
       " (368, 'val'): 0.003003873758845859,\n",
       " (369, 'train'): 0.002396817629535993,\n",
       " (369, 'val'): 0.003003801460619326,\n",
       " (370, 'train'): 0.002366511427142002,\n",
       " (370, 'val'): 0.003004585979161439,\n",
       " (371, 'train'): 0.002363793827869274,\n",
       " (371, 'val'): 0.0030027622425997697,\n",
       " (372, 'train'): 0.0023956006323849715,\n",
       " (372, 'val'): 0.0030042559460357384,\n",
       " (373, 'train'): 0.002377671362073333,\n",
       " (373, 'val'): 0.0030047112592944394,\n",
       " (374, 'train'): 0.0023707888882469248,\n",
       " (374, 'val'): 0.003002226076744221,\n",
       " (375, 'train'): 0.002382267956380491,\n",
       " (375, 'val'): 0.0030032029306447067,\n",
       " (376, 'train'): 0.0023637203568661652,\n",
       " (376, 'val'): 0.003001310207225658,\n",
       " (377, 'train'): 0.0023853246260572363,\n",
       " (377, 'val'): 0.0030007734894752502,\n",
       " (378, 'train'): 0.0023899858610497583,\n",
       " (378, 'val'): 0.0030023554960886636,\n",
       " (379, 'train'): 0.00239549328883489,\n",
       " (379, 'val'): 0.00300281329287423,\n",
       " (380, 'train'): 0.002397428991066085,\n",
       " (380, 'val'): 0.0030045981208483377,\n",
       " (381, 'train'): 0.002380614203435403,\n",
       " (381, 'val'): 0.0030026976709012633,\n",
       " (382, 'train'): 0.0023805588759757855,\n",
       " (382, 'val'): 0.0030021874441040885,\n",
       " (383, 'train'): 0.0023591273498755915,\n",
       " (383, 'val'): 0.0030018985271453857,\n",
       " (384, 'train'): 0.0023871221476131016,\n",
       " (384, 'val'): 0.0030037076384932908,\n",
       " (385, 'train'): 0.002352609885511575,\n",
       " (385, 'val'): 0.0030038028403564735,\n",
       " (386, 'train'): 0.002373840866817368,\n",
       " (386, 'val'): 0.003003970616393619,\n",
       " (387, 'train'): 0.002368184358433441,\n",
       " (387, 'val'): 0.0030043453530029015,\n",
       " (388, 'train'): 0.002368602694736587,\n",
       " (388, 'val'): 0.003003497918446859,\n",
       " (389, 'train'): 0.0023811921063396665,\n",
       " (389, 'val'): 0.003004636201593611,\n",
       " (390, 'train'): 0.002385245705092395,\n",
       " (390, 'val'): 0.003004541551625287,\n",
       " (391, 'train'): 0.00239037639564938,\n",
       " (391, 'val'): 0.0030042462878757054,\n",
       " (392, 'train'): 0.00236460221586404,\n",
       " (392, 'val'): 0.003003146085474226,\n",
       " (393, 'train'): 0.002362839256723722,\n",
       " (393, 'val'): 0.0030014034774568346,\n",
       " (394, 'train'): 0.0023926454423754302,\n",
       " (394, 'val'): 0.003001226043259656,\n",
       " (395, 'train'): 0.0023823409444755977,\n",
       " (395, 'val'): 0.003002607160144382,\n",
       " (396, 'train'): 0.0023669574271749567,\n",
       " (396, 'val'): 0.0030041160406889737,\n",
       " (397, 'train'): 0.0023397372828589547,\n",
       " (397, 'val'): 0.003001553592858491,\n",
       " (398, 'train'): 0.0023897139148579705,\n",
       " (398, 'val'): 0.0030032517733397305,\n",
       " (399, 'train'): 0.002341631110067721,\n",
       " (399, 'val'): 0.0030026377903090585,\n",
       " (400, 'train'): 0.0023965774862854568,\n",
       " (400, 'val'): 0.003003284059188984,\n",
       " (401, 'train'): 0.002373007160645944,\n",
       " (401, 'val'): 0.003003008111759468,\n",
       " (402, 'train'): 0.0023999493569135666,\n",
       " (402, 'val'): 0.003002474429430785,\n",
       " (403, 'train'): 0.002366816073104187,\n",
       " (403, 'val'): 0.0030024810521690933,\n",
       " (404, 'train'): 0.0023995282611361255,\n",
       " (404, 'val'): 0.003004095068684331,\n",
       " (405, 'train'): 0.0023773307739584533,\n",
       " (405, 'val'): 0.0030037744177712333,\n",
       " (406, 'train'): 0.0023348583253445447,\n",
       " (406, 'val'): 0.003002162608835432,\n",
       " (407, 'train'): 0.0023488571384438764,\n",
       " (407, 'val'): 0.0030014771554205152,\n",
       " (408, 'train'): 0.0023837862191376864,\n",
       " (408, 'val'): 0.003002628132149025,\n",
       " (409, 'train'): 0.0023808543466859395,\n",
       " (409, 'val'): 0.0030020850676077382,\n",
       " (410, 'train'): 0.002371991398157897,\n",
       " (410, 'val'): 0.003002893869523649,\n",
       " (411, 'train'): 0.0023947767913341522,\n",
       " (411, 'val'): 0.0030053920216030544,\n",
       " (412, 'train'): 0.0023577955585938914,\n",
       " (412, 'val'): 0.00300358842920374,\n",
       " (413, 'train'): 0.002424437207756219,\n",
       " (413, 'val'): 0.0030052427340436865,\n",
       " (414, 'train'): 0.0023679729137155744,\n",
       " (414, 'val'): 0.003004854476010358,\n",
       " (415, 'train'): 0.002374731418159273,\n",
       " (415, 'val'): 0.003004527478306382,\n",
       " (416, 'train'): 0.002383508891971023,\n",
       " (416, 'val'): 0.00300486468606525,\n",
       " (417, 'train'): 0.0023402140510303004,\n",
       " (417, 'val'): 0.0030028152245062368,\n",
       " (418, 'train'): 0.0023796947466002572,\n",
       " (418, 'val'): 0.003003994623819987,\n",
       " (419, 'train'): 0.0023619458079338074,\n",
       " (419, 'val'): 0.0030028055663462038,\n",
       " (420, 'train'): 0.0023931616020423396,\n",
       " (420, 'val'): 0.0030042763661455225,\n",
       " (421, 'train'): 0.0023912152758351077,\n",
       " (421, 'val'): 0.003005673212033731,\n",
       " (422, 'train'): 0.002405345508897746,\n",
       " (422, 'val'): 0.003005959921412998,\n",
       " (423, 'train'): 0.0023437774291744937,\n",
       " (423, 'val'): 0.0030039559911798548,\n",
       " (424, 'train'): 0.0023833424266841677,\n",
       " (424, 'val'): 0.003004059471465923,\n",
       " (425, 'train'): 0.0023867610704015802,\n",
       " (425, 'val'): 0.0030027440300694217,\n",
       " (426, 'train'): 0.002360609946427522,\n",
       " (426, 'val'): 0.003002720022643054,\n",
       " (427, 'train'): 0.0023724453316794503,\n",
       " (427, 'val'): 0.0030041764731760377,\n",
       " (428, 'train'): 0.002350822574010602,\n",
       " (428, 'val'): 0.0030038370578377334,\n",
       " (429, 'train'): 0.002369807757161282,\n",
       " (429, 'val'): 0.003003446316277539,\n",
       " (430, 'train'): 0.002374996879586467,\n",
       " (430, 'val'): 0.00300221503884704,\n",
       " (431, 'train'): 0.0023846157171108105,\n",
       " (431, 'val'): 0.003002401579309393,\n",
       " (432, 'train'): 0.002386690220899052,\n",
       " (432, 'val'): 0.0030030936554626183,\n",
       " (433, 'train'): 0.0023640479064650005,\n",
       " (433, 'val'): 0.0030027545160717433,\n",
       " (434, 'train'): 0.0023641403488538883,\n",
       " (434, 'val'): 0.0030040147679823415,\n",
       " (435, 'train'): 0.0023751556873321533,\n",
       " (435, 'val'): 0.0030035856697294447,\n",
       " (436, 'train'): 0.0023618582636117935,\n",
       " (436, 'val'): 0.003002786525973567,\n",
       " (437, 'train'): 0.0023768386907047695,\n",
       " (437, 'val'): 0.0030041072103712293,\n",
       " (438, 'train'): 0.002388594327149568,\n",
       " (438, 'val'): 0.00300403849946128,\n",
       " (439, 'train'): 0.0023797288950946596,\n",
       " (439, 'val'): 0.003004342869476036,\n",
       " (440, 'train'): 0.0023964885622262955,\n",
       " (440, 'val'): 0.0030046792493926156,\n",
       " (441, 'train'): 0.002356962059383039,\n",
       " (441, 'val'): 0.0030046000524803444,\n",
       " (442, 'train'): 0.002377954070214872,\n",
       " (442, 'val'): 0.003004749615987142,\n",
       " (443, 'train'): 0.0024084841349610578,\n",
       " (443, 'val'): 0.0030055366180561207,\n",
       " (444, 'train'): 0.00240821218876927,\n",
       " (444, 'val'): 0.003007292471550129,\n",
       " (445, 'train'): 0.002363361418247223,\n",
       " (445, 'val'): 0.0030066426153536195,\n",
       " (446, 'train'): 0.002389708188948808,\n",
       " (446, 'val'): 0.003007237282064226,\n",
       " (447, 'train'): 0.0023766269010526164,\n",
       " (447, 'val'): 0.003006166606037705,\n",
       " (448, 'train'): 0.002370978602104717,\n",
       " (448, 'val'): 0.0030054278947688915,\n",
       " (449, 'train'): 0.002377477784951528,\n",
       " (449, 'val'): 0.0030053804318110147,\n",
       " (450, 'train'): 0.0023509630312522254,\n",
       " (450, 'val'): 0.0030038500273669205,\n",
       " (451, 'train'): 0.002370329435776781,\n",
       " (451, 'val'): 0.003003805323883339,\n",
       " (452, 'train'): 0.002352939987624133,\n",
       " (452, 'val'): 0.0030028731734664353,\n",
       " (453, 'train'): 0.0023958175270645705,\n",
       " (453, 'val'): 0.0030029267072677612,\n",
       " (454, 'train'): 0.002350940058628718,\n",
       " (454, 'val'): 0.0030010116321069224,\n",
       " (455, 'train'): 0.0023621477324653555,\n",
       " (455, 'val'): 0.003000384403599633,\n",
       " (456, 'train'): 0.0023759841505024168,\n",
       " (456, 'val'): 0.003000235116040265,\n",
       " (457, 'train'): 0.0023882552567455503,\n",
       " (457, 'val'): 0.003001846097133778,\n",
       " (458, 'train'): 0.0023996062852718212,\n",
       " (458, 'val'): 0.003003479705916511,\n",
       " (459, 'train'): 0.0023587106692570226,\n",
       " (459, 'val'): 0.0030036328567398917,\n",
       " (460, 'train'): 0.002391048534600823,\n",
       " (460, 'val'): 0.0030050628163196423,\n",
       " (461, 'train'): 0.002360070676163391,\n",
       " (461, 'val'): 0.0030044366916020713,\n",
       " (462, 'train'): 0.0023837795963993777,\n",
       " (462, 'val'): 0.0030028364724583095,\n",
       " (463, 'train'): 0.0023744705098646657,\n",
       " (463, 'val'): 0.0030031488449485215,\n",
       " (464, 'train'): 0.002388842886796704,\n",
       " (464, 'val'): 0.003002972514541061,\n",
       " (465, 'train'): 0.002378558877993513,\n",
       " (465, 'val'): 0.0030051331829141687,\n",
       " (466, 'train'): 0.002334975809962661,\n",
       " (466, 'val'): 0.003003140566525636,\n",
       " (467, 'train'): 0.0023589866856733956,\n",
       " (467, 'val'): 0.003003190237062949,\n",
       " (468, 'train'): 0.00239217695262697,\n",
       " (468, 'val'): 0.0030047242288236266,\n",
       " (469, 'train'): 0.0023993516547812354,\n",
       " (469, 'val'): 0.0030069823066393533,\n",
       " (470, 'train'): 0.0023512419451166082,\n",
       " (470, 'val'): 0.0030033047552461977,\n",
       " (471, 'train'): 0.002369580790400505,\n",
       " (471, 'val'): 0.0030044342080752053,\n",
       " (472, 'train'): 0.0023972650093060954,\n",
       " (472, 'val'): 0.003005744406470546,\n",
       " (473, 'train'): 0.0023550080066477813,\n",
       " (473, 'val'): 0.003004486914034243,\n",
       " (474, 'train'): 0.002377644043277811,\n",
       " (474, 'val'): 0.003001566286440249,\n",
       " (475, 'train'): 0.0023974074671665826,\n",
       " (475, 'val'): 0.0030030594379813584,\n",
       " (476, 'train'): 0.0024053813820635833,\n",
       " (476, 'val'): 0.0030034143063757154,\n",
       " (477, 'train'): 0.0023800547890089176,\n",
       " (477, 'val'): 0.003003831538889143,\n",
       " (478, 'train'): 0.0023975586173711,\n",
       " (478, 'val'): 0.003003077926459136,\n",
       " (479, 'train'): 0.0023693300231739326,\n",
       " (479, 'val'): 0.003002580393243719,\n",
       " (480, 'train'): 0.0023828998070072245,\n",
       " (480, 'val'): 0.0030015141323760704,\n",
       " (481, 'train'): 0.002377851003849948,\n",
       " (481, 'val'): 0.0030011976206744157,\n",
       " (482, 'train'): 0.0023493655025959015,\n",
       " (482, 'val'): 0.00300124673931687,\n",
       " (483, 'train'): 0.002376135852601793,\n",
       " (483, 'val'): 0.0030018154669691015,\n",
       " (484, 'train'): 0.0023897766239113277,\n",
       " (484, 'val'): 0.0030020726499734103,\n",
       " (485, 'train'): 0.002351518996335842,\n",
       " (485, 'val'): 0.0030013852649264866,\n",
       " (486, 'train'): 0.0023588200134259684,\n",
       " (486, 'val'): 0.003001411203984861,\n",
       " (487, 'train'): 0.002342487374941508,\n",
       " (487, 'val'): 0.003002747893333435,\n",
       " (488, 'train'): 0.0023459476867207776,\n",
       " (488, 'val'): 0.0030008634483372726,\n",
       " (489, 'train'): 0.0023983823894350616,\n",
       " (489, 'val'): 0.003001635273297628,\n",
       " (490, 'train'): 0.0023893587705161837,\n",
       " (490, 'val'): 0.0030021788897337735,\n",
       " (491, 'train'): 0.0023811746526647497,\n",
       " (491, 'val'): 0.0030031044174123694,\n",
       " (492, 'train'): 0.002377888532700362,\n",
       " (492, 'val'): 0.0030042451840859873,\n",
       " (493, 'train'): 0.0023651709435162723,\n",
       " (493, 'val'): 0.0030045065063017385,\n",
       " (494, 'train'): 0.0023807369310546805,\n",
       " (494, 'val'): 0.0030043111355216416,\n",
       " (495, 'train'): 0.0023799704180823434,\n",
       " (495, 'val'): 0.003003735785131101,\n",
       " (496, 'train'): 0.0023924635930193793,\n",
       " (496, 'val'): 0.0030029316743214927,\n",
       " (497, 'train'): 0.0023450649308937566,\n",
       " (497, 'val'): 0.0030027332681196706,\n",
       " (498, 'train'): 0.0023483576046095956,\n",
       " (498, 'val'): 0.003003479429969081,\n",
       " (499, 'train'): 0.0023880285659322034,\n",
       " (499, 'val'): 0.003004086238366586}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ä¸éœ€è¦labelï¼Œæ‰€ä»¥ç”¨ä¸€ä¸ªå ä½ç¬¦\"_\"ä»£æ›¿\n",
    "    for batchidx, (x, _) in enumerate(X_allDataLoader):\n",
    "        x.requires_grad_(True)\n",
    "        # encode and decode \n",
    "        output = model(x)\n",
    "        # compute loss\n",
    "        print(output.shape)\n",
    "        loss = loss_function(output, x)      \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "           \n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'saved/models/ae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_batch = model(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(recon_batch,trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = model.encode(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = train_embeddings.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.001)\n",
    "clf.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFrg = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "RFrg.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeature = model.encode(testData)\n",
    "lasso = clf.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfresult = RFrg.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(lasso,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(rfresult,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=\"label\")\n",
    "plt.legend()\n",
    "plt.savefig(\"saved/figures/tsne_vae_gdsc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data type conversion\n",
    "B_feature = torch.FloatTensor(feature).to(device)\n",
    "y = torch.FloatTensor(Y_train.values).to(device)\n",
    "# construct TensorDataset\n",
    "b_data = TensorDataset(B_feature, y)\n",
    "trainDataLoader2 = DataLoader(dataset=b_data, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization DNN model\n",
    "\n",
    "predictor = DNN(128, dim_dnn_out).to(device)\n",
    "optimizer = optim.Adam(predictor.parameters(), lr=1e-3,betas=(0.9,0.99))\n",
    "#loss1-softmax\n",
    "loss_func = nn.MSELoss().to(device)\n",
    "#loss2-sigmoid\n",
    "#loss_func = nn.BCELoss()\n",
    "#loss3-sigmoid\n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = torch.nn.MSELoss(size_average=True)\n",
    "#criterion = torch.nn.BCELoss(size_average=True) # Defined loss function\n",
    "#optimizer = optim.Adm(model.parameters(), lr=0.01) # Defined optimizer\n",
    "loss_train = np.zeros((epochs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ',epoch)\n",
    "    for step,(batch_x,batch_y) in enumerate(trainDataLoader2):\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "        # predict label\n",
    "        output = predictor(b_x)\n",
    "        # b_y=F.sigmoid(b_y) \n",
    "        \n",
    "        #print\n",
    "        #print(output)\n",
    "        #print(b_y)\n",
    "        # compute loss\n",
    "        loss = loss_func(output,b_y)\n",
    "        #loss = criterion(output, b_y)\n",
    "        \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item())) \n",
    "\n",
    "# Save model\n",
    "torch.save(predictor.state_dict(), 'saved/models/DNN_GDSC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict = predictor(testFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
