{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In D:\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "#import scipy.io as sio\n",
    "\n",
    "from models import VAE,AEBase\n",
    "from models import DNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import models\n",
    "import utils as ut\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "epochs = 500 #200,500,1000  \n",
    "#dim_au_in = 20049\n",
    "dim_au_out = 512 #8, 16, 32, 64, 128, 256,512\n",
    "dim_dnn_in = dim_au_out\n",
    "dim_dnn_out=1\n",
    "select_drug = 'Gefitinib'\n",
    "na = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r=pd.read_csv('data/GDSCexpression.csv',index_col=0)\n",
    "label_r=pd.read_csv('data/GDSClabel.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_r=label_r.fillna(na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n"
     ]
    }
   ],
   "source": [
    "hvg,adata = ut.highly_variable_genes(data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Erlotinib</th>\n",
       "      <th>AICAR</th>\n",
       "      <th>Camptothecin</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Cisplatin</th>\n",
       "      <th>Cytarabine</th>\n",
       "      <th>Docetaxel</th>\n",
       "      <th>Methotrexate</th>\n",
       "      <th>ATRA</th>\n",
       "      <th>Gefitinib</th>\n",
       "      <th>...</th>\n",
       "      <th>CMK</th>\n",
       "      <th>Pyrimethamine</th>\n",
       "      <th>JW-7-52-1</th>\n",
       "      <th>A-443654</th>\n",
       "      <th>GW843682X</th>\n",
       "      <th>MS-275</th>\n",
       "      <th>Parthenolide</th>\n",
       "      <th>MG-132</th>\n",
       "      <th>KIN001-135</th>\n",
       "      <th>TGX221</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BxPC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.159708</td>\n",
       "      <td>0.188393</td>\n",
       "      <td>0.436127</td>\n",
       "      <td>0.154274</td>\n",
       "      <td>0.113764</td>\n",
       "      <td>0.249987</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMOE-2</th>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.318206</td>\n",
       "      <td>0.281740</td>\n",
       "      <td>0.063808</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.101530</td>\n",
       "      <td>0.191210</td>\n",
       "      <td>0.031229</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022662</td>\n",
       "      <td>0.025217</td>\n",
       "      <td>0.270791</td>\n",
       "      <td>0.095608</td>\n",
       "      <td>0.139914</td>\n",
       "      <td>0.171747</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFM-223</th>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.334087</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.361438</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>0.186659</td>\n",
       "      <td>0.228797</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUGC-3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.416792</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.266409</td>\n",
       "      <td>0.237811</td>\n",
       "      <td>0.234952</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>0.153002</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OC-314</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.479915</td>\n",
       "      <td>0.390145</td>\n",
       "      <td>0.067738</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>0.110418</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.020309</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KP-N-S19s</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC-3</th>\n",
       "      <td>0.003515</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048227</td>\n",
       "      <td>0.146476</td>\n",
       "      <td>0.400087</td>\n",
       "      <td>0.205247</td>\n",
       "      <td>0.147090</td>\n",
       "      <td>0.298803</td>\n",
       "      <td>0.023796</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.069181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Panc 08.13</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.180653</td>\n",
       "      <td>0.090963</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EKVX</th>\n",
       "      <td>0.177238</td>\n",
       "      <td>0.078395</td>\n",
       "      <td>0.078938</td>\n",
       "      <td>0.072213</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>0.095642</td>\n",
       "      <td>0.111598</td>\n",
       "      <td>0.126364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.243635</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.177759</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>0.007216</td>\n",
       "      <td>0.063911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMS-114</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.127677</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.281940</td>\n",
       "      <td>0.092074</td>\n",
       "      <td>0.191972</td>\n",
       "      <td>0.185916</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.043920</td>\n",
       "      <td>0.285969</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.174841</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.080451</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.028733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Erlotinib     AICAR  Camptothecin  Vinblastine  Cisplatin  \\\n",
       "BxPC-3      -1.000000  0.159708      0.188393     0.436127   0.154274   \n",
       "KMOE-2       0.002108  0.339814      0.318206     0.281740   0.063808   \n",
       "MFM-223      0.002148  0.334087      0.122671     0.361438   0.051983   \n",
       "NUGC-3      -1.000000  0.095099      0.416792     0.422631   0.266409   \n",
       "OC-314      -1.000000  0.012740      0.479915     0.390145   0.067738   \n",
       "...               ...       ...           ...          ...        ...   \n",
       "KP-N-S19s   -1.000000 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "BC-3         0.003515 -1.000000     -1.000000    -1.000000  -1.000000   \n",
       "Panc 08.13  -1.000000  0.152818      0.196279     0.180653   0.090963   \n",
       "EKVX         0.177238  0.078395      0.078938     0.072213   0.007225   \n",
       "DMS-114      0.000322  0.127677      0.168317     0.281940   0.092074   \n",
       "\n",
       "            Cytarabine  Docetaxel  Methotrexate      ATRA  Gefitinib  ...  \\\n",
       "BxPC-3        0.113764   0.249987      0.015324  0.015324   0.020293  ...   \n",
       "KMOE-2        0.140681   0.101530      0.191210  0.031229   0.007419  ...   \n",
       "MFM-223       0.186659   0.228797      0.003639  0.023043   0.003639  ...   \n",
       "NUGC-3        0.237811   0.234952      0.011558  0.028838   0.153002  ...   \n",
       "OC-314        0.125869   0.249668      0.110418  0.002214   0.020309  ...   \n",
       "...                ...        ...           ...       ...        ...  ...   \n",
       "KP-N-S19s    -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "BC-3         -1.000000  -1.000000     -1.000000 -1.000000  -1.000000  ...   \n",
       "Panc 08.13    0.093197   0.097267      0.018269  0.010237   0.009405  ...   \n",
       "EKVX          0.003470   0.129184      0.095642  0.111598   0.126364  ...   \n",
       "DMS-114       0.191972   0.185916      0.007135  0.009329   0.020822  ...   \n",
       "\n",
       "                 CMK  Pyrimethamine  JW-7-52-1  A-443654  GW843682X    MS-275  \\\n",
       "BxPC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "KMOE-2      0.022662       0.025217   0.270791  0.095608   0.139914  0.171747   \n",
       "MFM-223     0.038997       0.006167   0.002185  0.102592   0.020338  0.010874   \n",
       "NUGC-3     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "OC-314     -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "...              ...            ...        ...       ...        ...       ...   \n",
       "KP-N-S19s  -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "BC-3        0.048227       0.146476   0.400087  0.205247   0.147090  0.298803   \n",
       "Panc 08.13 -1.000000      -1.000000  -1.000000 -1.000000  -1.000000 -1.000000   \n",
       "EKVX        0.020472       0.044047   0.243635  0.079375   0.019859  0.177759   \n",
       "DMS-114     0.002137       0.043920   0.285969  0.075195   0.174841  0.008383   \n",
       "\n",
       "            Parthenolide    MG-132  KIN001-135    TGX221  \n",
       "BxPC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "KMOE-2          0.026670  0.013904    0.002116  0.000956  \n",
       "MFM-223         0.000970  0.035206    0.007051  0.000323  \n",
       "NUGC-3         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "OC-314         -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "...                  ...       ...         ...       ...  \n",
       "KP-N-S19s      -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "BC-3            0.023796  0.017219    0.002065  0.069181  \n",
       "Panc 08.13     -1.000000 -1.000000   -1.000000 -1.000000  \n",
       "EKVX            0.014843  0.114431    0.007216  0.063911  \n",
       "DMS-114         0.002135  0.080451    0.002135  0.028733  \n",
       "\n",
       "[789 rows x 139 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = label_r.loc[:,select_drug]!=na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r.columns = adata.var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your is gene-cell, mine is cell-gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data_r.loc[selected_idx,:]\n",
    "data = data_r.loc[selected_idx,hvg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = label_r.loc[selected_idx,select_drug]\n",
    "#sscaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "mmscaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "data = mmscaler.fit_transform(data)\n",
    "#data = sscaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25383929199289434\n",
      "0.2727100504700379\n"
     ]
    }
   ],
   "source": [
    "print(np.std(data))\n",
    "print(np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49458949, 0.09860014, 0.21159545, ..., 0.21276445, 0.22738253,\n",
       "       0.27747885])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000004\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(data.max())\n",
    "print(data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 3462)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 139)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_all, X_test, Y_train_all, Y_test = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_all, Y_train_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 3462)\n",
      "(675,)\n",
      "(432, 3462) (432,)\n",
      "(135, 3462) (135,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all data to AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainTensor = torch.FloatTensor(X_train).to(device)\n",
    "X_validTensor = torch.FloatTensor(X_valid).to(device)\n",
    "X_testTensor = torch.FloatTensor(X_test).to(device)\n",
    "X_allTensor = torch.FloatTensor(data).to(device)\n",
    "#X_alltrainTensor = torch.FloatTensor(X_train_all).to(device)\n",
    "\n",
    "\n",
    "Y_trainTensor = torch.FloatTensor(Y_train.values).to(device)\n",
    "Y_validTensor = torch.FloatTensor(Y_valid.values).to(device)\n",
    "\n",
    "# construct TensorDataset\n",
    "train_dataset = TensorDataset(X_trainTensor, X_trainTensor)\n",
    "valid_dataset = TensorDataset(X_validTensor, X_validTensor)\n",
    "test_dataset = TensorDataset(X_testTensor, X_testTensor)\n",
    "all_dataset = TensorDataset(X_allTensor, X_allTensor)\n",
    "\n",
    "X_trainDataLoader = DataLoader(dataset=train_dataset, batch_size=200, shuffle=True)\n",
    "X_validDataLoader = DataLoader(dataset=valid_dataset, batch_size=200, shuffle=True)\n",
    "X_allDataLoader = DataLoader(dataset=all_dataset, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = X_trainDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_train = {'train':X_trainDataLoader,'val':X_validDataLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainDataLoader.dataset.tensors[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 432, 'val': 108}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{x: dataloaders_train[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEBase(input_dim=data.shape[1],latent_dim=512,hidden_dims=[2048,1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEBase(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=3462, out_features=2048, bias=True)\n",
      "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (decoder_input): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=3462, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VAE(dim_au_in=data_r.shape[1],dim_au_out=128)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae_model(net,data_loaders,optimizer,loss_function,n_epochs,scheduler):\n",
    "    \n",
    "    dataset_sizes = {x: data_loaders[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}\n",
    "    loss_train = {}\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #optimizer = scheduler(optimizer, epoch)\n",
    "                net.train()  # Set model to training mode\n",
    "            else:\n",
    "                net.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # for data in data_loaders[phase]:\n",
    "            for batchidx, (x, _) in enumerate(data_loaders[phase]):\n",
    "\n",
    "                x.requires_grad_(True)\n",
    "                # encode and decode \n",
    "                output = model(x)\n",
    "                # compute loss\n",
    "                loss = loss_function(output, x)      \n",
    "\n",
    "                # zero the parameter (weight) gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    # update the weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                # print loss statistics\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            # Schedular\n",
    "#             if phase == 'train':\n",
    "#                 scheduler.step()\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)\n",
    "                \n",
    "            last_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            loss_train[epoch,phase] = epoch_loss\n",
    "            print('{} Loss: {:.8f}. Learning rate = {}'.format(phase, epoch_loss,last_lr))\n",
    "            \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Select best model wts\n",
    "    torch.save(best_model_wts, 'saved/models/ae.pkl')\n",
    "    net.load_state_dict(best_model_wts)           \n",
    "    \n",
    "    return net, loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/499\n",
      "----------\n",
      "train Loss: 0.00137978. Learning rate = 0.01\n",
      "val Loss: 0.00364676. Learning rate = 0.01\n",
      "Epoch 1/499\n",
      "----------\n",
      "train Loss: 0.00179373. Learning rate = 0.01\n",
      "val Loss: 0.00209717. Learning rate = 0.01\n",
      "Epoch 2/499\n",
      "----------\n",
      "train Loss: 0.00126729. Learning rate = 0.01\n",
      "val Loss: 0.00199299. Learning rate = 0.01\n",
      "Epoch 3/499\n",
      "----------\n",
      "train Loss: 0.00118850. Learning rate = 0.01\n",
      "val Loss: 0.00184282. Learning rate = 0.01\n",
      "Epoch 4/499\n",
      "----------\n",
      "train Loss: 0.00110372. Learning rate = 0.01\n",
      "val Loss: 0.00183227. Learning rate = 0.01\n",
      "Epoch 5/499\n",
      "----------\n",
      "train Loss: 0.00103471. Learning rate = 0.01\n",
      "val Loss: 0.00167147. Learning rate = 0.01\n",
      "Epoch 6/499\n",
      "----------\n",
      "train Loss: 0.00099253. Learning rate = 0.01\n",
      "val Loss: 0.00165997. Learning rate = 0.01\n",
      "Epoch 7/499\n",
      "----------\n",
      "train Loss: 0.00098848. Learning rate = 0.01\n",
      "val Loss: 0.00158414. Learning rate = 0.01\n",
      "Epoch 8/499\n",
      "----------\n",
      "train Loss: 0.00094238. Learning rate = 0.01\n",
      "val Loss: 0.00139948. Learning rate = 0.01\n",
      "Epoch 9/499\n",
      "----------\n",
      "train Loss: 0.00098456. Learning rate = 0.01\n",
      "val Loss: 0.00130868. Learning rate = 0.01\n",
      "Epoch 10/499\n",
      "----------\n",
      "train Loss: 0.00090732. Learning rate = 0.01\n",
      "val Loss: 0.00126319. Learning rate = 0.01\n",
      "Epoch 11/499\n",
      "----------\n",
      "train Loss: 0.00090839. Learning rate = 0.01\n",
      "val Loss: 0.00115323. Learning rate = 0.01\n",
      "Epoch 12/499\n",
      "----------\n",
      "train Loss: 0.00090886. Learning rate = 0.01\n",
      "val Loss: 0.00109034. Learning rate = 0.01\n",
      "Epoch 13/499\n",
      "----------\n",
      "train Loss: 0.00085676. Learning rate = 0.01\n",
      "val Loss: 0.00106028. Learning rate = 0.01\n",
      "Epoch 14/499\n",
      "----------\n",
      "train Loss: 0.00087808. Learning rate = 0.01\n",
      "val Loss: 0.00100773. Learning rate = 0.01\n",
      "Epoch 15/499\n",
      "----------\n",
      "train Loss: 0.00081708. Learning rate = 0.01\n",
      "val Loss: 0.00098772. Learning rate = 0.01\n",
      "Epoch 16/499\n",
      "----------\n",
      "train Loss: 0.00081746. Learning rate = 0.01\n",
      "val Loss: 0.00094821. Learning rate = 0.01\n",
      "Epoch 17/499\n",
      "----------\n",
      "train Loss: 0.00075450. Learning rate = 0.01\n",
      "val Loss: 0.00093754. Learning rate = 0.01\n",
      "Epoch 18/499\n",
      "----------\n",
      "train Loss: 0.00075466. Learning rate = 0.01\n",
      "val Loss: 0.00089861. Learning rate = 0.01\n",
      "Epoch 19/499\n",
      "----------\n",
      "train Loss: 0.00081337. Learning rate = 0.01\n",
      "val Loss: 0.00086920. Learning rate = 0.01\n",
      "Epoch 20/499\n",
      "----------\n",
      "train Loss: 0.00079455. Learning rate = 0.01\n",
      "val Loss: 0.00086727. Learning rate = 0.01\n",
      "Epoch 21/499\n",
      "----------\n",
      "train Loss: 0.00073723. Learning rate = 0.01\n",
      "val Loss: 0.00084049. Learning rate = 0.01\n",
      "Epoch 22/499\n",
      "----------\n",
      "train Loss: 0.00067018. Learning rate = 0.01\n",
      "val Loss: 0.00083870. Learning rate = 0.01\n",
      "Epoch 23/499\n",
      "----------\n",
      "train Loss: 0.00067980. Learning rate = 0.01\n",
      "val Loss: 0.00079974. Learning rate = 0.01\n",
      "Epoch 24/499\n",
      "----------\n",
      "train Loss: 0.00062009. Learning rate = 0.01\n",
      "val Loss: 0.00080443. Learning rate = 0.01\n",
      "Epoch 25/499\n",
      "----------\n",
      "train Loss: 0.00067669. Learning rate = 0.01\n",
      "val Loss: 0.00076360. Learning rate = 0.01\n",
      "Epoch 26/499\n",
      "----------\n",
      "train Loss: 0.00064552. Learning rate = 0.01\n",
      "val Loss: 0.00074703. Learning rate = 0.01\n",
      "Epoch 27/499\n",
      "----------\n",
      "train Loss: 0.00059674. Learning rate = 0.01\n",
      "val Loss: 0.00075069. Learning rate = 0.01\n",
      "Epoch 28/499\n",
      "----------\n",
      "train Loss: 0.00061317. Learning rate = 0.01\n",
      "val Loss: 0.00072101. Learning rate = 0.01\n",
      "Epoch 29/499\n",
      "----------\n",
      "train Loss: 0.00061362. Learning rate = 0.01\n",
      "val Loss: 0.00069921. Learning rate = 0.01\n",
      "Epoch 30/499\n",
      "----------\n",
      "train Loss: 0.00066453. Learning rate = 0.01\n",
      "val Loss: 0.00069655. Learning rate = 0.01\n",
      "Epoch 31/499\n",
      "----------\n",
      "train Loss: 0.00058040. Learning rate = 0.01\n",
      "val Loss: 0.00071391. Learning rate = 0.01\n",
      "Epoch 32/499\n",
      "----------\n",
      "train Loss: 0.00066906. Learning rate = 0.01\n",
      "val Loss: 0.00068871. Learning rate = 0.01\n",
      "Epoch 33/499\n",
      "----------\n",
      "train Loss: 0.00061568. Learning rate = 0.01\n",
      "val Loss: 0.00068774. Learning rate = 0.01\n",
      "Epoch 34/499\n",
      "----------\n",
      "train Loss: 0.00054460. Learning rate = 0.01\n",
      "val Loss: 0.00069549. Learning rate = 0.01\n",
      "Epoch 35/499\n",
      "----------\n",
      "train Loss: 0.00053937. Learning rate = 0.01\n",
      "val Loss: 0.00066815. Learning rate = 0.01\n",
      "Epoch 36/499\n",
      "----------\n",
      "train Loss: 0.00054305. Learning rate = 0.01\n",
      "val Loss: 0.00064705. Learning rate = 0.01\n",
      "Epoch 37/499\n",
      "----------\n",
      "train Loss: 0.00055602. Learning rate = 0.01\n",
      "val Loss: 0.00062216. Learning rate = 0.01\n",
      "Epoch 38/499\n",
      "----------\n",
      "train Loss: 0.00051921. Learning rate = 0.01\n",
      "val Loss: 0.00062582. Learning rate = 0.01\n",
      "Epoch 39/499\n",
      "----------\n",
      "train Loss: 0.00050746. Learning rate = 0.01\n",
      "val Loss: 0.00060709. Learning rate = 0.01\n",
      "Epoch 40/499\n",
      "----------\n",
      "train Loss: 0.00047887. Learning rate = 0.01\n",
      "val Loss: 0.00059120. Learning rate = 0.01\n",
      "Epoch 41/499\n",
      "----------\n",
      "train Loss: 0.00047743. Learning rate = 0.01\n",
      "val Loss: 0.00058124. Learning rate = 0.01\n",
      "Epoch 42/499\n",
      "----------\n",
      "train Loss: 0.00047154. Learning rate = 0.01\n",
      "val Loss: 0.00057727. Learning rate = 0.01\n",
      "Epoch 43/499\n",
      "----------\n",
      "train Loss: 0.00046852. Learning rate = 0.01\n",
      "val Loss: 0.00054960. Learning rate = 0.01\n",
      "Epoch 44/499\n",
      "----------\n",
      "train Loss: 0.00047556. Learning rate = 0.01\n",
      "val Loss: 0.00051091. Learning rate = 0.01\n",
      "Epoch 45/499\n",
      "----------\n",
      "train Loss: 0.00043157. Learning rate = 0.01\n",
      "val Loss: 0.00049978. Learning rate = 0.01\n",
      "Epoch 46/499\n",
      "----------\n",
      "train Loss: 0.00046983. Learning rate = 0.01\n",
      "val Loss: 0.00046435. Learning rate = 0.01\n",
      "Epoch 47/499\n",
      "----------\n",
      "train Loss: 0.00037697. Learning rate = 0.01\n",
      "val Loss: 0.00045501. Learning rate = 0.01\n",
      "Epoch 48/499\n",
      "----------\n",
      "train Loss: 0.00042081. Learning rate = 0.01\n",
      "val Loss: 0.00042891. Learning rate = 0.01\n",
      "Epoch 49/499\n",
      "----------\n",
      "train Loss: 0.00035817. Learning rate = 0.01\n",
      "val Loss: 0.00040233. Learning rate = 0.01\n",
      "Epoch 50/499\n",
      "----------\n",
      "train Loss: 0.00032653. Learning rate = 0.01\n",
      "val Loss: 0.00039106. Learning rate = 0.01\n",
      "Epoch 51/499\n",
      "----------\n",
      "train Loss: 0.00035904. Learning rate = 0.01\n",
      "val Loss: 0.00037941. Learning rate = 0.01\n",
      "Epoch 52/499\n",
      "----------\n",
      "train Loss: 0.00031298. Learning rate = 0.01\n",
      "val Loss: 0.00037783. Learning rate = 0.01\n",
      "Epoch 53/499\n",
      "----------\n",
      "train Loss: 0.00030365. Learning rate = 0.01\n",
      "val Loss: 0.00036601. Learning rate = 0.01\n",
      "Epoch 54/499\n",
      "----------\n",
      "train Loss: 0.00028913. Learning rate = 0.01\n",
      "val Loss: 0.00035494. Learning rate = 0.01\n",
      "Epoch 55/499\n",
      "----------\n",
      "train Loss: 0.00027638. Learning rate = 0.01\n",
      "val Loss: 0.00035459. Learning rate = 0.01\n",
      "Epoch 56/499\n",
      "----------\n",
      "train Loss: 0.00028682. Learning rate = 0.01\n",
      "val Loss: 0.00035376. Learning rate = 0.01\n",
      "Epoch 57/499\n",
      "----------\n",
      "train Loss: 0.00025919. Learning rate = 0.01\n",
      "val Loss: 0.00033709. Learning rate = 0.01\n",
      "Epoch 58/499\n",
      "----------\n",
      "train Loss: 0.00025494. Learning rate = 0.01\n",
      "val Loss: 0.00032852. Learning rate = 0.01\n",
      "Epoch 59/499\n",
      "----------\n",
      "train Loss: 0.00026156. Learning rate = 0.01\n",
      "val Loss: 0.00032519. Learning rate = 0.01\n",
      "Epoch 60/499\n",
      "----------\n",
      "train Loss: 0.00025577. Learning rate = 0.01\n",
      "val Loss: 0.00032733. Learning rate = 0.01\n",
      "Epoch 61/499\n",
      "----------\n",
      "train Loss: 0.00025422. Learning rate = 0.01\n",
      "val Loss: 0.00031974. Learning rate = 0.01\n",
      "Epoch 62/499\n",
      "----------\n",
      "train Loss: 0.00025736. Learning rate = 0.01\n",
      "val Loss: 0.00032141. Learning rate = 0.01\n",
      "Epoch 63/499\n",
      "----------\n",
      "train Loss: 0.00025721. Learning rate = 0.01\n",
      "val Loss: 0.00031449. Learning rate = 0.01\n",
      "Epoch 64/499\n",
      "----------\n",
      "train Loss: 0.00024336. Learning rate = 0.01\n",
      "val Loss: 0.00030829. Learning rate = 0.01\n",
      "Epoch 65/499\n",
      "----------\n",
      "train Loss: 0.00025757. Learning rate = 0.01\n",
      "val Loss: 0.00030223. Learning rate = 0.01\n",
      "Epoch 66/499\n",
      "----------\n",
      "train Loss: 0.00025534. Learning rate = 0.01\n",
      "val Loss: 0.00031651. Learning rate = 0.01\n",
      "Epoch 67/499\n",
      "----------\n",
      "train Loss: 0.00025495. Learning rate = 0.01\n",
      "val Loss: 0.00031158. Learning rate = 0.01\n",
      "Epoch 68/499\n",
      "----------\n",
      "train Loss: 0.00024633. Learning rate = 0.01\n",
      "val Loss: 0.00030349. Learning rate = 0.01\n",
      "Epoch 69/499\n",
      "----------\n",
      "train Loss: 0.00023920. Learning rate = 0.01\n",
      "val Loss: 0.00030278. Learning rate = 0.01\n",
      "Epoch 70/499\n",
      "----------\n",
      "train Loss: 0.00025020. Learning rate = 0.01\n",
      "val Loss: 0.00029339. Learning rate = 0.01\n",
      "Epoch 71/499\n",
      "----------\n",
      "train Loss: 0.00023751. Learning rate = 0.01\n",
      "val Loss: 0.00030262. Learning rate = 0.01\n",
      "Epoch 72/499\n",
      "----------\n",
      "train Loss: 0.00023705. Learning rate = 0.01\n",
      "val Loss: 0.00029396. Learning rate = 0.01\n",
      "Epoch 73/499\n",
      "----------\n",
      "train Loss: 0.00022779. Learning rate = 0.01\n",
      "val Loss: 0.00028691. Learning rate = 0.01\n",
      "Epoch 74/499\n",
      "----------\n",
      "train Loss: 0.00028834. Learning rate = 0.01\n",
      "val Loss: 0.00028634. Learning rate = 0.01\n",
      "Epoch 75/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00024096. Learning rate = 0.01\n",
      "val Loss: 0.00033422. Learning rate = 0.01\n",
      "Epoch 76/499\n",
      "----------\n",
      "train Loss: 0.00026034. Learning rate = 0.01\n",
      "val Loss: 0.00032989. Learning rate = 0.01\n",
      "Epoch 77/499\n",
      "----------\n",
      "train Loss: 0.00025672. Learning rate = 0.01\n",
      "val Loss: 0.00031021. Learning rate = 0.01\n",
      "Epoch 78/499\n",
      "----------\n",
      "train Loss: 0.00024812. Learning rate = 0.01\n",
      "val Loss: 0.00030526. Learning rate = 0.01\n",
      "Epoch 79/499\n",
      "----------\n",
      "train Loss: 0.00023394. Learning rate = 0.01\n",
      "val Loss: 0.00029488. Learning rate = 0.01\n",
      "Epoch 80/499\n",
      "----------\n",
      "train Loss: 0.00023871. Learning rate = 0.01\n",
      "val Loss: 0.00029169. Learning rate = 0.01\n",
      "Epoch 81/499\n",
      "----------\n",
      "train Loss: 0.00023021. Learning rate = 0.01\n",
      "val Loss: 0.00029095. Learning rate = 0.01\n",
      "Epoch 82/499\n",
      "----------\n",
      "train Loss: 0.00022611. Learning rate = 0.01\n",
      "val Loss: 0.00028766. Learning rate = 0.01\n",
      "Epoch 83/499\n",
      "----------\n",
      "train Loss: 0.00022493. Learning rate = 0.01\n",
      "val Loss: 0.00028437. Learning rate = 0.01\n",
      "Epoch 84/499\n",
      "----------\n",
      "train Loss: 0.00022544. Learning rate = 0.01\n",
      "val Loss: 0.00028371. Learning rate = 0.01\n",
      "Epoch 85/499\n",
      "----------\n",
      "train Loss: 0.00023322. Learning rate = 0.01\n",
      "val Loss: 0.00027865. Learning rate = 0.01\n",
      "Epoch 86/499\n",
      "----------\n",
      "train Loss: 0.00022371. Learning rate = 0.01\n",
      "val Loss: 0.00027662. Learning rate = 0.01\n",
      "Epoch 87/499\n",
      "----------\n",
      "train Loss: 0.00021707. Learning rate = 0.01\n",
      "val Loss: 0.00027520. Learning rate = 0.01\n",
      "Epoch 88/499\n",
      "----------\n",
      "train Loss: 0.00021456. Learning rate = 0.01\n",
      "val Loss: 0.00027274. Learning rate = 0.01\n",
      "Epoch 89/499\n",
      "----------\n",
      "train Loss: 0.00021378. Learning rate = 0.01\n",
      "val Loss: 0.00027151. Learning rate = 0.01\n",
      "Epoch 90/499\n",
      "----------\n",
      "train Loss: 0.00021801. Learning rate = 0.01\n",
      "val Loss: 0.00027338. Learning rate = 0.01\n",
      "Epoch 91/499\n",
      "----------\n",
      "train Loss: 0.00023731. Learning rate = 0.01\n",
      "val Loss: 0.00026806. Learning rate = 0.01\n",
      "Epoch 92/499\n",
      "----------\n",
      "train Loss: 0.00021923. Learning rate = 0.01\n",
      "val Loss: 0.00027011. Learning rate = 0.01\n",
      "Epoch 93/499\n",
      "----------\n",
      "train Loss: 0.00021807. Learning rate = 0.01\n",
      "val Loss: 0.00026308. Learning rate = 0.01\n",
      "Epoch 94/499\n",
      "----------\n",
      "train Loss: 0.00020992. Learning rate = 0.01\n",
      "val Loss: 0.00026216. Learning rate = 0.01\n",
      "Epoch 95/499\n",
      "----------\n",
      "train Loss: 0.00020759. Learning rate = 0.01\n",
      "val Loss: 0.00026364. Learning rate = 0.01\n",
      "Epoch 96/499\n",
      "----------\n",
      "train Loss: 0.00020667. Learning rate = 0.01\n",
      "val Loss: 0.00026475. Learning rate = 0.01\n",
      "Epoch 97/499\n",
      "----------\n",
      "train Loss: 0.00020306. Learning rate = 0.01\n",
      "val Loss: 0.00026174. Learning rate = 0.01\n",
      "Epoch 98/499\n",
      "----------\n",
      "train Loss: 0.00020289. Learning rate = 0.01\n",
      "val Loss: 0.00026198. Learning rate = 0.01\n",
      "Epoch 99/499\n",
      "----------\n",
      "train Loss: 0.00020813. Learning rate = 0.01\n",
      "val Loss: 0.00025899. Learning rate = 0.01\n",
      "Epoch 100/499\n",
      "----------\n",
      "train Loss: 0.00020200. Learning rate = 0.01\n",
      "val Loss: 0.00026114. Learning rate = 0.01\n",
      "Epoch 101/499\n",
      "----------\n",
      "train Loss: 0.00020952. Learning rate = 0.01\n",
      "val Loss: 0.00025646. Learning rate = 0.01\n",
      "Epoch 102/499\n",
      "----------\n",
      "train Loss: 0.00020440. Learning rate = 0.01\n",
      "val Loss: 0.00025869. Learning rate = 0.01\n",
      "Epoch 103/499\n",
      "----------\n",
      "train Loss: 0.00021201. Learning rate = 0.01\n",
      "val Loss: 0.00025471. Learning rate = 0.01\n",
      "Epoch 104/499\n",
      "----------\n",
      "train Loss: 0.00020314. Learning rate = 0.01\n",
      "val Loss: 0.00025482. Learning rate = 0.01\n",
      "Epoch 105/499\n",
      "----------\n",
      "train Loss: 0.00020455. Learning rate = 0.01\n",
      "val Loss: 0.00025444. Learning rate = 0.01\n",
      "Epoch 106/499\n",
      "----------\n",
      "train Loss: 0.00019940. Learning rate = 0.01\n",
      "val Loss: 0.00025563. Learning rate = 0.01\n",
      "Epoch 107/499\n",
      "----------\n",
      "train Loss: 0.00019956. Learning rate = 0.01\n",
      "val Loss: 0.00025259. Learning rate = 0.01\n",
      "Epoch 108/499\n",
      "----------\n",
      "train Loss: 0.00020131. Learning rate = 0.01\n",
      "val Loss: 0.00025217. Learning rate = 0.01\n",
      "Epoch 109/499\n",
      "----------\n",
      "train Loss: 0.00022707. Learning rate = 0.01\n",
      "val Loss: 0.00025328. Learning rate = 0.01\n",
      "Epoch 110/499\n",
      "----------\n",
      "train Loss: 0.00020508. Learning rate = 0.01\n",
      "val Loss: 0.00026026. Learning rate = 0.01\n",
      "Epoch 111/499\n",
      "----------\n",
      "train Loss: 0.00020197. Learning rate = 0.01\n",
      "val Loss: 0.00025023. Learning rate = 0.01\n",
      "Epoch 112/499\n",
      "----------\n",
      "train Loss: 0.00019889. Learning rate = 0.01\n",
      "val Loss: 0.00025261. Learning rate = 0.01\n",
      "Epoch 113/499\n",
      "----------\n",
      "train Loss: 0.00020292. Learning rate = 0.01\n",
      "val Loss: 0.00024907. Learning rate = 0.01\n",
      "Epoch 114/499\n",
      "----------\n",
      "train Loss: 0.00020072. Learning rate = 0.01\n",
      "val Loss: 0.00025145. Learning rate = 0.01\n",
      "Epoch 115/499\n",
      "----------\n",
      "train Loss: 0.00019830. Learning rate = 0.01\n",
      "val Loss: 0.00025093. Learning rate = 0.01\n",
      "Epoch 116/499\n",
      "----------\n",
      "train Loss: 0.00019926. Learning rate = 0.01\n",
      "val Loss: 0.00024883. Learning rate = 0.01\n",
      "Epoch 117/499\n",
      "----------\n",
      "train Loss: 0.00019660. Learning rate = 0.01\n",
      "val Loss: 0.00024638. Learning rate = 0.01\n",
      "Epoch 118/499\n",
      "----------\n",
      "train Loss: 0.00020246. Learning rate = 0.01\n",
      "val Loss: 0.00024353. Learning rate = 0.01\n",
      "Epoch 119/499\n",
      "----------\n",
      "train Loss: 0.00019263. Learning rate = 0.01\n",
      "val Loss: 0.00024784. Learning rate = 0.01\n",
      "Epoch 120/499\n",
      "----------\n",
      "train Loss: 0.00019808. Learning rate = 0.01\n",
      "val Loss: 0.00024275. Learning rate = 0.01\n",
      "Epoch 121/499\n",
      "----------\n",
      "train Loss: 0.00019343. Learning rate = 0.01\n",
      "val Loss: 0.00024200. Learning rate = 0.01\n",
      "Epoch 122/499\n",
      "----------\n",
      "train Loss: 0.00019671. Learning rate = 0.01\n",
      "val Loss: 0.00023983. Learning rate = 0.01\n",
      "Epoch 123/499\n",
      "----------\n",
      "train Loss: 0.00019087. Learning rate = 0.01\n",
      "val Loss: 0.00024124. Learning rate = 0.01\n",
      "Epoch 124/499\n",
      "----------\n",
      "train Loss: 0.00019410. Learning rate = 0.01\n",
      "val Loss: 0.00024077. Learning rate = 0.01\n",
      "Epoch 125/499\n",
      "----------\n",
      "train Loss: 0.00019029. Learning rate = 0.01\n",
      "val Loss: 0.00024046. Learning rate = 0.01\n",
      "Epoch 126/499\n",
      "----------\n",
      "train Loss: 0.00018654. Learning rate = 0.01\n",
      "val Loss: 0.00023998. Learning rate = 0.01\n",
      "Epoch 127/499\n",
      "----------\n",
      "train Loss: 0.00019213. Learning rate = 0.01\n",
      "val Loss: 0.00023781. Learning rate = 0.01\n",
      "Epoch 128/499\n",
      "----------\n",
      "train Loss: 0.00019002. Learning rate = 0.01\n",
      "val Loss: 0.00023940. Learning rate = 0.01\n",
      "Epoch 129/499\n",
      "----------\n",
      "train Loss: 0.00018729. Learning rate = 0.01\n",
      "val Loss: 0.00024179. Learning rate = 0.01\n",
      "Epoch 130/499\n",
      "----------\n",
      "train Loss: 0.00018335. Learning rate = 0.01\n",
      "val Loss: 0.00023583. Learning rate = 0.01\n",
      "Epoch 131/499\n",
      "----------\n",
      "train Loss: 0.00018722. Learning rate = 0.01\n",
      "val Loss: 0.00023799. Learning rate = 0.01\n",
      "Epoch 132/499\n",
      "----------\n",
      "train Loss: 0.00018011. Learning rate = 0.01\n",
      "val Loss: 0.00023566. Learning rate = 0.01\n",
      "Epoch 133/499\n",
      "----------\n",
      "train Loss: 0.00018415. Learning rate = 0.01\n",
      "val Loss: 0.00023598. Learning rate = 0.01\n",
      "Epoch 134/499\n",
      "----------\n",
      "train Loss: 0.00018420. Learning rate = 0.01\n",
      "val Loss: 0.00023429. Learning rate = 0.01\n",
      "Epoch 135/499\n",
      "----------\n",
      "train Loss: 0.00017737. Learning rate = 0.01\n",
      "val Loss: 0.00023288. Learning rate = 0.01\n",
      "Epoch 136/499\n",
      "----------\n",
      "train Loss: 0.00017987. Learning rate = 0.01\n",
      "val Loss: 0.00023196. Learning rate = 0.01\n",
      "Epoch 137/499\n",
      "----------\n",
      "train Loss: 0.00017974. Learning rate = 0.01\n",
      "val Loss: 0.00023191. Learning rate = 0.01\n",
      "Epoch 138/499\n",
      "----------\n",
      "train Loss: 0.00017824. Learning rate = 0.01\n",
      "val Loss: 0.00023119. Learning rate = 0.01\n",
      "Epoch 139/499\n",
      "----------\n",
      "train Loss: 0.00017702. Learning rate = 0.01\n",
      "val Loss: 0.00023004. Learning rate = 0.01\n",
      "Epoch 140/499\n",
      "----------\n",
      "train Loss: 0.00017809. Learning rate = 0.01\n",
      "val Loss: 0.00023120. Learning rate = 0.01\n",
      "Epoch 141/499\n",
      "----------\n",
      "train Loss: 0.00017907. Learning rate = 0.01\n",
      "val Loss: 0.00023008. Learning rate = 0.01\n",
      "Epoch 142/499\n",
      "----------\n",
      "train Loss: 0.00017887. Learning rate = 0.01\n",
      "val Loss: 0.00022873. Learning rate = 0.01\n",
      "Epoch 143/499\n",
      "----------\n",
      "train Loss: 0.00017746. Learning rate = 0.01\n",
      "val Loss: 0.00022885. Learning rate = 0.01\n",
      "Epoch 144/499\n",
      "----------\n",
      "train Loss: 0.00017624. Learning rate = 0.01\n",
      "val Loss: 0.00022869. Learning rate = 0.01\n",
      "Epoch 145/499\n",
      "----------\n",
      "train Loss: 0.00017342. Learning rate = 0.01\n",
      "val Loss: 0.00022676. Learning rate = 0.01\n",
      "Epoch 146/499\n",
      "----------\n",
      "train Loss: 0.00017324. Learning rate = 0.01\n",
      "val Loss: 0.00022671. Learning rate = 0.01\n",
      "Epoch 147/499\n",
      "----------\n",
      "train Loss: 0.00017874. Learning rate = 0.01\n",
      "val Loss: 0.00022634. Learning rate = 0.01\n",
      "Epoch 148/499\n",
      "----------\n",
      "train Loss: 0.00017131. Learning rate = 0.01\n",
      "val Loss: 0.00022554. Learning rate = 0.01\n",
      "Epoch 149/499\n",
      "----------\n",
      "train Loss: 0.00018022. Learning rate = 0.01\n",
      "val Loss: 0.00022534. Learning rate = 0.01\n",
      "Epoch 150/499\n",
      "----------\n",
      "train Loss: 0.00017452. Learning rate = 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.00022599. Learning rate = 0.01\n",
      "Epoch 151/499\n",
      "----------\n",
      "train Loss: 0.00017774. Learning rate = 0.01\n",
      "val Loss: 0.00022601. Learning rate = 0.01\n",
      "Epoch 152/499\n",
      "----------\n",
      "train Loss: 0.00017832. Learning rate = 0.01\n",
      "val Loss: 0.00022525. Learning rate = 0.01\n",
      "Epoch 153/499\n",
      "----------\n",
      "train Loss: 0.00017270. Learning rate = 0.01\n",
      "val Loss: 0.00022418. Learning rate = 0.01\n",
      "Epoch 154/499\n",
      "----------\n",
      "train Loss: 0.00017186. Learning rate = 0.01\n",
      "val Loss: 0.00022401. Learning rate = 0.01\n",
      "Epoch 155/499\n",
      "----------\n",
      "train Loss: 0.00017544. Learning rate = 0.01\n",
      "val Loss: 0.00022398. Learning rate = 0.01\n",
      "Epoch 156/499\n",
      "----------\n",
      "train Loss: 0.00016777. Learning rate = 0.01\n",
      "val Loss: 0.00022298. Learning rate = 0.01\n",
      "Epoch 157/499\n",
      "----------\n",
      "train Loss: 0.00017066. Learning rate = 0.01\n",
      "val Loss: 0.00022374. Learning rate = 0.01\n",
      "Epoch 158/499\n",
      "----------\n",
      "train Loss: 0.00017189. Learning rate = 0.01\n",
      "val Loss: 0.00022140. Learning rate = 0.01\n",
      "Epoch 159/499\n",
      "----------\n",
      "train Loss: 0.00016701. Learning rate = 0.01\n",
      "val Loss: 0.00022058. Learning rate = 0.01\n",
      "Epoch 160/499\n",
      "----------\n",
      "train Loss: 0.00016903. Learning rate = 0.01\n",
      "val Loss: 0.00022221. Learning rate = 0.01\n",
      "Epoch 161/499\n",
      "----------\n",
      "train Loss: 0.00016458. Learning rate = 0.01\n",
      "val Loss: 0.00022057. Learning rate = 0.01\n",
      "Epoch 162/499\n",
      "----------\n",
      "train Loss: 0.00016643. Learning rate = 0.01\n",
      "val Loss: 0.00022011. Learning rate = 0.01\n",
      "Epoch 163/499\n",
      "----------\n",
      "train Loss: 0.00016202. Learning rate = 0.01\n",
      "val Loss: 0.00021951. Learning rate = 0.01\n",
      "Epoch 164/499\n",
      "----------\n",
      "train Loss: 0.00016377. Learning rate = 0.01\n",
      "val Loss: 0.00021868. Learning rate = 0.01\n",
      "Epoch 165/499\n",
      "----------\n",
      "train Loss: 0.00016555. Learning rate = 0.01\n",
      "val Loss: 0.00021950. Learning rate = 0.01\n",
      "Epoch 166/499\n",
      "----------\n",
      "train Loss: 0.00016533. Learning rate = 0.01\n",
      "val Loss: 0.00021854. Learning rate = 0.01\n",
      "Epoch 167/499\n",
      "----------\n",
      "train Loss: 0.00016334. Learning rate = 0.01\n",
      "val Loss: 0.00021870. Learning rate = 0.01\n",
      "Epoch 168/499\n",
      "----------\n",
      "train Loss: 0.00016106. Learning rate = 0.01\n",
      "val Loss: 0.00021883. Learning rate = 0.01\n",
      "Epoch 169/499\n",
      "----------\n",
      "train Loss: 0.00016288. Learning rate = 0.01\n",
      "val Loss: 0.00021856. Learning rate = 0.01\n",
      "Epoch 170/499\n",
      "----------\n",
      "train Loss: 0.00016267. Learning rate = 0.01\n",
      "val Loss: 0.00021909. Learning rate = 0.01\n",
      "Epoch 171/499\n",
      "----------\n",
      "train Loss: 0.00016287. Learning rate = 0.01\n",
      "val Loss: 0.00021868. Learning rate = 0.01\n",
      "Epoch 172/499\n",
      "----------\n",
      "train Loss: 0.00016145. Learning rate = 0.01\n",
      "val Loss: 0.00021711. Learning rate = 0.01\n",
      "Epoch 173/499\n",
      "----------\n",
      "train Loss: 0.00016033. Learning rate = 0.01\n",
      "val Loss: 0.00021708. Learning rate = 0.01\n",
      "Epoch 174/499\n",
      "----------\n",
      "train Loss: 0.00016292. Learning rate = 0.01\n",
      "val Loss: 0.00021747. Learning rate = 0.01\n",
      "Epoch 175/499\n",
      "----------\n",
      "train Loss: 0.00016031. Learning rate = 0.01\n",
      "val Loss: 0.00021840. Learning rate = 0.01\n",
      "Epoch 176/499\n",
      "----------\n",
      "train Loss: 0.00017381. Learning rate = 0.01\n",
      "val Loss: 0.00021923. Learning rate = 0.01\n",
      "Epoch 177/499\n",
      "----------\n",
      "train Loss: 0.00016245. Learning rate = 0.01\n",
      "val Loss: 0.00022080. Learning rate = 0.01\n",
      "Epoch 178/499\n",
      "----------\n",
      "train Loss: 0.00016417. Learning rate = 0.01\n",
      "val Loss: 0.00021807. Learning rate = 0.01\n",
      "Epoch 179/499\n",
      "----------\n",
      "train Loss: 0.00016118. Learning rate = 0.01\n",
      "val Loss: 0.00021683. Learning rate = 0.01\n",
      "Epoch 180/499\n",
      "----------\n",
      "train Loss: 0.00016351. Learning rate = 0.01\n",
      "val Loss: 0.00021701. Learning rate = 0.01\n",
      "Epoch 181/499\n",
      "----------\n",
      "train Loss: 0.00016055. Learning rate = 0.01\n",
      "val Loss: 0.00021486. Learning rate = 0.01\n",
      "Epoch 182/499\n",
      "----------\n",
      "train Loss: 0.00015861. Learning rate = 0.01\n",
      "val Loss: 0.00021505. Learning rate = 0.01\n",
      "Epoch 183/499\n",
      "----------\n",
      "train Loss: 0.00015924. Learning rate = 0.01\n",
      "val Loss: 0.00021502. Learning rate = 0.01\n",
      "Epoch 184/499\n",
      "----------\n",
      "train Loss: 0.00015992. Learning rate = 0.01\n",
      "val Loss: 0.00021505. Learning rate = 0.01\n",
      "Epoch 185/499\n",
      "----------\n",
      "train Loss: 0.00015655. Learning rate = 0.01\n",
      "val Loss: 0.00021292. Learning rate = 0.01\n",
      "Epoch 186/499\n",
      "----------\n",
      "train Loss: 0.00015276. Learning rate = 0.01\n",
      "val Loss: 0.00021266. Learning rate = 0.01\n",
      "Epoch 187/499\n",
      "----------\n",
      "train Loss: 0.00015574. Learning rate = 0.01\n",
      "val Loss: 0.00021026. Learning rate = 0.01\n",
      "Epoch 188/499\n",
      "----------\n",
      "train Loss: 0.00015575. Learning rate = 0.01\n",
      "val Loss: 0.00021042. Learning rate = 0.01\n",
      "Epoch 189/499\n",
      "----------\n",
      "train Loss: 0.00015411. Learning rate = 0.01\n",
      "val Loss: 0.00020747. Learning rate = 0.01\n",
      "Epoch 190/499\n",
      "----------\n",
      "train Loss: 0.00015630. Learning rate = 0.01\n",
      "val Loss: 0.00020819. Learning rate = 0.01\n",
      "Epoch 191/499\n",
      "----------\n",
      "train Loss: 0.00015379. Learning rate = 0.01\n",
      "val Loss: 0.00020865. Learning rate = 0.01\n",
      "Epoch 192/499\n",
      "----------\n",
      "train Loss: 0.00015350. Learning rate = 0.01\n",
      "val Loss: 0.00020713. Learning rate = 0.01\n",
      "Epoch 193/499\n",
      "----------\n",
      "train Loss: 0.00015458. Learning rate = 0.01\n",
      "val Loss: 0.00020600. Learning rate = 0.01\n",
      "Epoch 194/499\n",
      "----------\n",
      "train Loss: 0.00015307. Learning rate = 0.01\n",
      "val Loss: 0.00020633. Learning rate = 0.01\n",
      "Epoch 195/499\n",
      "----------\n",
      "train Loss: 0.00015329. Learning rate = 0.01\n",
      "val Loss: 0.00020613. Learning rate = 0.01\n",
      "Epoch 196/499\n",
      "----------\n",
      "train Loss: 0.00015379. Learning rate = 0.01\n",
      "val Loss: 0.00020608. Learning rate = 0.01\n",
      "Epoch 197/499\n",
      "----------\n",
      "train Loss: 0.00015048. Learning rate = 0.01\n",
      "val Loss: 0.00020663. Learning rate = 0.01\n",
      "Epoch 198/499\n",
      "----------\n",
      "train Loss: 0.00015416. Learning rate = 0.01\n",
      "val Loss: 0.00020610. Learning rate = 0.01\n",
      "Epoch 199/499\n",
      "----------\n",
      "train Loss: 0.00015104. Learning rate = 0.01\n",
      "val Loss: 0.00020674. Learning rate = 0.01\n",
      "Epoch 200/499\n",
      "----------\n",
      "train Loss: 0.00015546. Learning rate = 0.01\n",
      "val Loss: 0.00020656. Learning rate = 0.01\n",
      "Epoch 201/499\n",
      "----------\n",
      "train Loss: 0.00014826. Learning rate = 0.01\n",
      "val Loss: 0.00020652. Learning rate = 0.01\n",
      "Epoch 202/499\n",
      "----------\n",
      "train Loss: 0.00014946. Learning rate = 0.01\n",
      "val Loss: 0.00020653. Learning rate = 0.01\n",
      "Epoch 203/499\n",
      "----------\n",
      "train Loss: 0.00014868. Learning rate = 0.01\n",
      "val Loss: 0.00020606. Learning rate = 0.01\n",
      "Epoch 204/499\n",
      "----------\n",
      "train Loss: 0.00014949. Learning rate = 0.01\n",
      "val Loss: 0.00020555. Learning rate = 0.01\n",
      "Epoch 205/499\n",
      "----------\n",
      "train Loss: 0.00015109. Learning rate = 0.01\n",
      "val Loss: 0.00020470. Learning rate = 0.01\n",
      "Epoch 206/499\n",
      "----------\n",
      "train Loss: 0.00014984. Learning rate = 0.01\n",
      "val Loss: 0.00020508. Learning rate = 0.01\n",
      "Epoch 207/499\n",
      "----------\n",
      "train Loss: 0.00015074. Learning rate = 0.01\n",
      "val Loss: 0.00020537. Learning rate = 0.01\n",
      "Epoch 208/499\n",
      "----------\n",
      "train Loss: 0.00015039. Learning rate = 0.01\n",
      "val Loss: 0.00020578. Learning rate = 0.01\n",
      "Epoch 209/499\n",
      "----------\n",
      "train Loss: 0.00015080. Learning rate = 0.01\n",
      "val Loss: 0.00020567. Learning rate = 0.01\n",
      "Epoch 210/499\n",
      "----------\n",
      "train Loss: 0.00015125. Learning rate = 0.01\n",
      "val Loss: 0.00020565. Learning rate = 0.01\n",
      "Epoch 211/499\n",
      "----------\n",
      "train Loss: 0.00014934. Learning rate = 0.01\n",
      "val Loss: 0.00020545. Learning rate = 0.01\n",
      "Epoch 212/499\n",
      "----------\n",
      "train Loss: 0.00015239. Learning rate = 0.001\n",
      "val Loss: 0.00020587. Learning rate = 0.001\n",
      "Epoch 213/499\n",
      "----------\n",
      "train Loss: 0.00015171. Learning rate = 0.001\n",
      "val Loss: 0.00020554. Learning rate = 0.001\n",
      "Epoch 214/499\n",
      "----------\n",
      "train Loss: 0.00014905. Learning rate = 0.001\n",
      "val Loss: 0.00020510. Learning rate = 0.001\n",
      "Epoch 215/499\n",
      "----------\n",
      "train Loss: 0.00014973. Learning rate = 0.001\n",
      "val Loss: 0.00020435. Learning rate = 0.001\n",
      "Epoch 216/499\n",
      "----------\n",
      "train Loss: 0.00014821. Learning rate = 0.001\n",
      "val Loss: 0.00020388. Learning rate = 0.001\n",
      "Epoch 217/499\n",
      "----------\n",
      "train Loss: 0.00014986. Learning rate = 0.001\n",
      "val Loss: 0.00020380. Learning rate = 0.001\n",
      "Epoch 218/499\n",
      "----------\n",
      "train Loss: 0.00015007. Learning rate = 0.001\n",
      "val Loss: 0.00020370. Learning rate = 0.001\n",
      "Epoch 219/499\n",
      "----------\n",
      "train Loss: 0.00014792. Learning rate = 0.001\n",
      "val Loss: 0.00020373. Learning rate = 0.001\n",
      "Epoch 220/499\n",
      "----------\n",
      "train Loss: 0.00014925. Learning rate = 0.001\n",
      "val Loss: 0.00020372. Learning rate = 0.001\n",
      "Epoch 221/499\n",
      "----------\n",
      "train Loss: 0.00014587. Learning rate = 0.001\n",
      "val Loss: 0.00020369. Learning rate = 0.001\n",
      "Epoch 222/499\n",
      "----------\n",
      "train Loss: 0.00014781. Learning rate = 0.001\n",
      "val Loss: 0.00020367. Learning rate = 0.001\n",
      "Epoch 223/499\n",
      "----------\n",
      "train Loss: 0.00014691. Learning rate = 0.001\n",
      "val Loss: 0.00020368. Learning rate = 0.001\n",
      "Epoch 224/499\n",
      "----------\n",
      "train Loss: 0.00015017. Learning rate = 0.001\n",
      "val Loss: 0.00020367. Learning rate = 0.001\n",
      "Epoch 225/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00014764. Learning rate = 0.001\n",
      "val Loss: 0.00020362. Learning rate = 0.001\n",
      "Epoch 226/499\n",
      "----------\n",
      "train Loss: 0.00014856. Learning rate = 0.001\n",
      "val Loss: 0.00020358. Learning rate = 0.001\n",
      "Epoch 227/499\n",
      "----------\n",
      "train Loss: 0.00014790. Learning rate = 0.001\n",
      "val Loss: 0.00020353. Learning rate = 0.001\n",
      "Epoch 228/499\n",
      "----------\n",
      "train Loss: 0.00014898. Learning rate = 0.001\n",
      "val Loss: 0.00020345. Learning rate = 0.001\n",
      "Epoch 229/499\n",
      "----------\n",
      "train Loss: 0.00015020. Learning rate = 0.001\n",
      "val Loss: 0.00020349. Learning rate = 0.001\n",
      "Epoch 230/499\n",
      "----------\n",
      "train Loss: 0.00014813. Learning rate = 0.001\n",
      "val Loss: 0.00020347. Learning rate = 0.001\n",
      "Epoch 231/499\n",
      "----------\n",
      "train Loss: 0.00014647. Learning rate = 0.001\n",
      "val Loss: 0.00020355. Learning rate = 0.001\n",
      "Epoch 232/499\n",
      "----------\n",
      "train Loss: 0.00014510. Learning rate = 0.001\n",
      "val Loss: 0.00020349. Learning rate = 0.001\n",
      "Epoch 233/499\n",
      "----------\n",
      "train Loss: 0.00014751. Learning rate = 0.001\n",
      "val Loss: 0.00020345. Learning rate = 0.001\n",
      "Epoch 234/499\n",
      "----------\n",
      "train Loss: 0.00014791. Learning rate = 0.001\n",
      "val Loss: 0.00020356. Learning rate = 0.001\n",
      "Epoch 235/499\n",
      "----------\n",
      "train Loss: 0.00015002. Learning rate = 0.001\n",
      "val Loss: 0.00020344. Learning rate = 0.001\n",
      "Epoch 236/499\n",
      "----------\n",
      "train Loss: 0.00014845. Learning rate = 0.001\n",
      "val Loss: 0.00020339. Learning rate = 0.001\n",
      "Epoch 237/499\n",
      "----------\n",
      "train Loss: 0.00014784. Learning rate = 0.001\n",
      "val Loss: 0.00020337. Learning rate = 0.001\n",
      "Epoch 238/499\n",
      "----------\n",
      "train Loss: 0.00015192. Learning rate = 0.001\n",
      "val Loss: 0.00020344. Learning rate = 0.001\n",
      "Epoch 239/499\n",
      "----------\n",
      "train Loss: 0.00014732. Learning rate = 0.001\n",
      "val Loss: 0.00020342. Learning rate = 0.001\n",
      "Epoch 240/499\n",
      "----------\n",
      "train Loss: 0.00014855. Learning rate = 0.001\n",
      "val Loss: 0.00020354. Learning rate = 0.001\n",
      "Epoch 241/499\n",
      "----------\n",
      "train Loss: 0.00015216. Learning rate = 0.001\n",
      "val Loss: 0.00020361. Learning rate = 0.001\n",
      "Epoch 242/499\n",
      "----------\n",
      "train Loss: 0.00014651. Learning rate = 0.001\n",
      "val Loss: 0.00020340. Learning rate = 0.001\n",
      "Epoch 243/499\n",
      "----------\n",
      "train Loss: 0.00014893. Learning rate = 0.0001\n",
      "val Loss: 0.00020339. Learning rate = 0.0001\n",
      "Epoch 244/499\n",
      "----------\n",
      "train Loss: 0.00014933. Learning rate = 0.0001\n",
      "val Loss: 0.00020346. Learning rate = 0.0001\n",
      "Epoch 245/499\n",
      "----------\n",
      "train Loss: 0.00014687. Learning rate = 0.0001\n",
      "val Loss: 0.00020344. Learning rate = 0.0001\n",
      "Epoch 246/499\n",
      "----------\n",
      "train Loss: 0.00014666. Learning rate = 0.0001\n",
      "val Loss: 0.00020340. Learning rate = 0.0001\n",
      "Epoch 247/499\n",
      "----------\n",
      "train Loss: 0.00015195. Learning rate = 0.0001\n",
      "val Loss: 0.00020342. Learning rate = 0.0001\n",
      "Epoch 248/499\n",
      "----------\n",
      "train Loss: 0.00014833. Learning rate = 0.0001\n",
      "val Loss: 0.00020342. Learning rate = 0.0001\n",
      "Epoch 249/499\n",
      "----------\n",
      "train Loss: 0.00014842. Learning rate = 0.0001\n",
      "val Loss: 0.00020332. Learning rate = 0.0001\n",
      "Epoch 250/499\n",
      "----------\n",
      "train Loss: 0.00014961. Learning rate = 0.0001\n",
      "val Loss: 0.00020332. Learning rate = 0.0001\n",
      "Epoch 251/499\n",
      "----------\n",
      "train Loss: 0.00014949. Learning rate = 0.0001\n",
      "val Loss: 0.00020325. Learning rate = 0.0001\n",
      "Epoch 252/499\n",
      "----------\n",
      "train Loss: 0.00014847. Learning rate = 0.0001\n",
      "val Loss: 0.00020335. Learning rate = 0.0001\n",
      "Epoch 253/499\n",
      "----------\n",
      "train Loss: 0.00014693. Learning rate = 0.0001\n",
      "val Loss: 0.00020333. Learning rate = 0.0001\n",
      "Epoch 254/499\n",
      "----------\n",
      "train Loss: 0.00014804. Learning rate = 1e-05\n",
      "val Loss: 0.00020331. Learning rate = 1e-05\n",
      "Epoch 255/499\n",
      "----------\n",
      "train Loss: 0.00014884. Learning rate = 1e-05\n",
      "val Loss: 0.00020330. Learning rate = 1e-05\n",
      "Epoch 256/499\n",
      "----------\n",
      "train Loss: 0.00014792. Learning rate = 1e-05\n",
      "val Loss: 0.00020327. Learning rate = 1e-05\n",
      "Epoch 257/499\n",
      "----------\n",
      "train Loss: 0.00014895. Learning rate = 1e-05\n",
      "val Loss: 0.00020328. Learning rate = 1e-05\n",
      "Epoch 258/499\n",
      "----------\n",
      "train Loss: 0.00014868. Learning rate = 1e-05\n",
      "val Loss: 0.00020332. Learning rate = 1e-05\n",
      "Epoch 259/499\n",
      "----------\n",
      "train Loss: 0.00014827. Learning rate = 1e-05\n",
      "val Loss: 0.00020330. Learning rate = 1e-05\n",
      "Epoch 260/499\n",
      "----------\n",
      "train Loss: 0.00015004. Learning rate = 1e-05\n",
      "val Loss: 0.00020335. Learning rate = 1e-05\n",
      "Epoch 261/499\n",
      "----------\n",
      "train Loss: 0.00014791. Learning rate = 1e-05\n",
      "val Loss: 0.00020339. Learning rate = 1e-05\n",
      "Epoch 262/499\n",
      "----------\n",
      "train Loss: 0.00014986. Learning rate = 1e-05\n",
      "val Loss: 0.00020336. Learning rate = 1e-05\n",
      "Epoch 263/499\n",
      "----------\n",
      "train Loss: 0.00014947. Learning rate = 1e-05\n",
      "val Loss: 0.00020330. Learning rate = 1e-05\n",
      "Epoch 264/499\n",
      "----------\n",
      "train Loss: 0.00014596. Learning rate = 1e-05\n",
      "val Loss: 0.00020332. Learning rate = 1e-05\n",
      "Epoch 265/499\n",
      "----------\n",
      "train Loss: 0.00014736. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020337. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 266/499\n",
      "----------\n",
      "train Loss: 0.00014877. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 267/499\n",
      "----------\n",
      "train Loss: 0.00014617. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 268/499\n",
      "----------\n",
      "train Loss: 0.00014948. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020336. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 269/499\n",
      "----------\n",
      "train Loss: 0.00014717. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020336. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 270/499\n",
      "----------\n",
      "train Loss: 0.00014765. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020338. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 271/499\n",
      "----------\n",
      "train Loss: 0.00014928. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020338. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 272/499\n",
      "----------\n",
      "train Loss: 0.00014803. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 273/499\n",
      "----------\n",
      "train Loss: 0.00014941. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020330. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 274/499\n",
      "----------\n",
      "train Loss: 0.00014755. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020324. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 275/499\n",
      "----------\n",
      "train Loss: 0.00014568. Learning rate = 1.0000000000000002e-06\n",
      "val Loss: 0.00020322. Learning rate = 1.0000000000000002e-06\n",
      "Epoch 276/499\n",
      "----------\n",
      "train Loss: 0.00014999. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020324. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 277/499\n",
      "----------\n",
      "train Loss: 0.00014879. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 278/499\n",
      "----------\n",
      "train Loss: 0.00015293. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020345. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 279/499\n",
      "----------\n",
      "train Loss: 0.00014797. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020343. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 280/499\n",
      "----------\n",
      "train Loss: 0.00014789. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 281/499\n",
      "----------\n",
      "train Loss: 0.00014838. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 282/499\n",
      "----------\n",
      "train Loss: 0.00014932. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020339. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 283/499\n",
      "----------\n",
      "train Loss: 0.00014611. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 284/499\n",
      "----------\n",
      "train Loss: 0.00014774. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 285/499\n",
      "----------\n",
      "train Loss: 0.00014779. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 286/499\n",
      "----------\n",
      "train Loss: 0.00014610. Learning rate = 1.0000000000000002e-07\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000002e-07\n",
      "Epoch 287/499\n",
      "----------\n",
      "train Loss: 0.00014691. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 288/499\n",
      "----------\n",
      "train Loss: 0.00014956. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 289/499\n",
      "----------\n",
      "train Loss: 0.00014743. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 290/499\n",
      "----------\n",
      "train Loss: 0.00014733. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020327. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 291/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00014658. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 292/499\n",
      "----------\n",
      "train Loss: 0.00014628. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 293/499\n",
      "----------\n",
      "train Loss: 0.00015223. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 294/499\n",
      "----------\n",
      "train Loss: 0.00015105. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 295/499\n",
      "----------\n",
      "train Loss: 0.00014752. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020323. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 296/499\n",
      "----------\n",
      "train Loss: 0.00015015. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 297/499\n",
      "----------\n",
      "train Loss: 0.00015074. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020342. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 298/499\n",
      "----------\n",
      "train Loss: 0.00014831. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 299/499\n",
      "----------\n",
      "train Loss: 0.00014823. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 300/499\n",
      "----------\n",
      "train Loss: 0.00014968. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020340. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 301/499\n",
      "----------\n",
      "train Loss: 0.00015102. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020342. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 302/499\n",
      "----------\n",
      "train Loss: 0.00014786. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020342. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 303/499\n",
      "----------\n",
      "train Loss: 0.00015087. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020338. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 304/499\n",
      "----------\n",
      "train Loss: 0.00014692. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020336. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 305/499\n",
      "----------\n",
      "train Loss: 0.00015030. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 306/499\n",
      "----------\n",
      "train Loss: 0.00015159. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020345. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 307/499\n",
      "----------\n",
      "train Loss: 0.00014752. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020337. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 308/499\n",
      "----------\n",
      "train Loss: 0.00014894. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 309/499\n",
      "----------\n",
      "train Loss: 0.00014821. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 310/499\n",
      "----------\n",
      "train Loss: 0.00014714. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 311/499\n",
      "----------\n",
      "train Loss: 0.00014840. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020327. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 312/499\n",
      "----------\n",
      "train Loss: 0.00014655. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020327. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 313/499\n",
      "----------\n",
      "train Loss: 0.00014803. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020327. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 314/499\n",
      "----------\n",
      "train Loss: 0.00014852. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 315/499\n",
      "----------\n",
      "train Loss: 0.00014881. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 316/499\n",
      "----------\n",
      "train Loss: 0.00015002. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020330. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 317/499\n",
      "----------\n",
      "train Loss: 0.00014795. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 318/499\n",
      "----------\n",
      "train Loss: 0.00014705. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 319/499\n",
      "----------\n",
      "train Loss: 0.00014710. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 320/499\n",
      "----------\n",
      "train Loss: 0.00014651. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 321/499\n",
      "----------\n",
      "train Loss: 0.00014574. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 322/499\n",
      "----------\n",
      "train Loss: 0.00014718. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 323/499\n",
      "----------\n",
      "train Loss: 0.00014779. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020339. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 324/499\n",
      "----------\n",
      "train Loss: 0.00014682. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020343. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 325/499\n",
      "----------\n",
      "train Loss: 0.00014944. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020337. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 326/499\n",
      "----------\n",
      "train Loss: 0.00014904. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 327/499\n",
      "----------\n",
      "train Loss: 0.00014893. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020336. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 328/499\n",
      "----------\n",
      "train Loss: 0.00015029. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 329/499\n",
      "----------\n",
      "train Loss: 0.00015050. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 330/499\n",
      "----------\n",
      "train Loss: 0.00014924. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 331/499\n",
      "----------\n",
      "train Loss: 0.00014704. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020342. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 332/499\n",
      "----------\n",
      "train Loss: 0.00014932. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020339. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 333/499\n",
      "----------\n",
      "train Loss: 0.00014659. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 334/499\n",
      "----------\n",
      "train Loss: 0.00014787. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 335/499\n",
      "----------\n",
      "train Loss: 0.00015005. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 336/499\n",
      "----------\n",
      "train Loss: 0.00015117. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 337/499\n",
      "----------\n",
      "train Loss: 0.00014952. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020321. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 338/499\n",
      "----------\n",
      "train Loss: 0.00014757. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020320. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 339/499\n",
      "----------\n",
      "train Loss: 0.00014990. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020327. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 340/499\n",
      "----------\n",
      "train Loss: 0.00014722. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 341/499\n",
      "----------\n",
      "train Loss: 0.00014505. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 342/499\n",
      "----------\n",
      "train Loss: 0.00014787. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 343/499\n",
      "----------\n",
      "train Loss: 0.00014782. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 344/499\n",
      "----------\n",
      "train Loss: 0.00014872. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 345/499\n",
      "----------\n",
      "train Loss: 0.00015437. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020370. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 346/499\n",
      "----------\n",
      "train Loss: 0.00014948. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020363. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 347/499\n",
      "----------\n",
      "train Loss: 0.00014911. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020369. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 348/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00014893. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020354. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 349/499\n",
      "----------\n",
      "train Loss: 0.00014790. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020345. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 350/499\n",
      "----------\n",
      "train Loss: 0.00014891. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020344. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 351/499\n",
      "----------\n",
      "train Loss: 0.00014791. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 352/499\n",
      "----------\n",
      "train Loss: 0.00014814. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 353/499\n",
      "----------\n",
      "train Loss: 0.00014623. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 354/499\n",
      "----------\n",
      "train Loss: 0.00014713. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 355/499\n",
      "----------\n",
      "train Loss: 0.00014541. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 356/499\n",
      "----------\n",
      "train Loss: 0.00014932. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020327. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 357/499\n",
      "----------\n",
      "train Loss: 0.00014831. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 358/499\n",
      "----------\n",
      "train Loss: 0.00014997. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 359/499\n",
      "----------\n",
      "train Loss: 0.00014909. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 360/499\n",
      "----------\n",
      "train Loss: 0.00014824. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 361/499\n",
      "----------\n",
      "train Loss: 0.00014878. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020339. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 362/499\n",
      "----------\n",
      "train Loss: 0.00015092. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 363/499\n",
      "----------\n",
      "train Loss: 0.00014575. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 364/499\n",
      "----------\n",
      "train Loss: 0.00014924. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 365/499\n",
      "----------\n",
      "train Loss: 0.00014753. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020330. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 366/499\n",
      "----------\n",
      "train Loss: 0.00014925. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 367/499\n",
      "----------\n",
      "train Loss: 0.00014982. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 368/499\n",
      "----------\n",
      "train Loss: 0.00015405. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020321. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 369/499\n",
      "----------\n",
      "train Loss: 0.00015036. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020323. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 370/499\n",
      "----------\n",
      "train Loss: 0.00014483. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 371/499\n",
      "----------\n",
      "train Loss: 0.00014801. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 372/499\n",
      "----------\n",
      "train Loss: 0.00014740. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 373/499\n",
      "----------\n",
      "train Loss: 0.00014864. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 374/499\n",
      "----------\n",
      "train Loss: 0.00014546. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 375/499\n",
      "----------\n",
      "train Loss: 0.00014715. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 376/499\n",
      "----------\n",
      "train Loss: 0.00014989. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 377/499\n",
      "----------\n",
      "train Loss: 0.00014707. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020341. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 378/499\n",
      "----------\n",
      "train Loss: 0.00014795. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020336. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 379/499\n",
      "----------\n",
      "train Loss: 0.00014757. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 380/499\n",
      "----------\n",
      "train Loss: 0.00015054. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020327. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 381/499\n",
      "----------\n",
      "train Loss: 0.00014925. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020319. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 382/499\n",
      "----------\n",
      "train Loss: 0.00014744. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020319. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 383/499\n",
      "----------\n",
      "train Loss: 0.00015015. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020323. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 384/499\n",
      "----------\n",
      "train Loss: 0.00014918. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020336. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 385/499\n",
      "----------\n",
      "train Loss: 0.00014874. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 386/499\n",
      "----------\n",
      "train Loss: 0.00014943. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 387/499\n",
      "----------\n",
      "train Loss: 0.00015041. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 388/499\n",
      "----------\n",
      "train Loss: 0.00014686. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 389/499\n",
      "----------\n",
      "train Loss: 0.00014557. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020330. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 390/499\n",
      "----------\n",
      "train Loss: 0.00015180. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 391/499\n",
      "----------\n",
      "train Loss: 0.00014895. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 392/499\n",
      "----------\n",
      "train Loss: 0.00014725. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020341. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 393/499\n",
      "----------\n",
      "train Loss: 0.00014789. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 394/499\n",
      "----------\n",
      "train Loss: 0.00014931. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 395/499\n",
      "----------\n",
      "train Loss: 0.00014964. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020337. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 396/499\n",
      "----------\n",
      "train Loss: 0.00014831. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 397/499\n",
      "----------\n",
      "train Loss: 0.00014418. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020339. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 398/499\n",
      "----------\n",
      "train Loss: 0.00014779. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020337. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 399/499\n",
      "----------\n",
      "train Loss: 0.00014750. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 400/499\n",
      "----------\n",
      "train Loss: 0.00014786. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 401/499\n",
      "----------\n",
      "train Loss: 0.00014998. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 402/499\n",
      "----------\n",
      "train Loss: 0.00015009. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 403/499\n",
      "----------\n",
      "train Loss: 0.00014932. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 404/499\n",
      "----------\n",
      "train Loss: 0.00014821. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 405/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00015457. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 406/499\n",
      "----------\n",
      "train Loss: 0.00014818. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 407/499\n",
      "----------\n",
      "train Loss: 0.00015086. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 408/499\n",
      "----------\n",
      "train Loss: 0.00015051. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 409/499\n",
      "----------\n",
      "train Loss: 0.00014964. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020330. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 410/499\n",
      "----------\n",
      "train Loss: 0.00014922. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 411/499\n",
      "----------\n",
      "train Loss: 0.00014827. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020341. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 412/499\n",
      "----------\n",
      "train Loss: 0.00014983. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020358. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 413/499\n",
      "----------\n",
      "train Loss: 0.00014738. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020348. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 414/499\n",
      "----------\n",
      "train Loss: 0.00014814. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 415/499\n",
      "----------\n",
      "train Loss: 0.00014921. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 416/499\n",
      "----------\n",
      "train Loss: 0.00014598. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 417/499\n",
      "----------\n",
      "train Loss: 0.00015025. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020339. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 418/499\n",
      "----------\n",
      "train Loss: 0.00015004. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 419/499\n",
      "----------\n",
      "train Loss: 0.00014680. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 420/499\n",
      "----------\n",
      "train Loss: 0.00015058. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 421/499\n",
      "----------\n",
      "train Loss: 0.00014700. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 422/499\n",
      "----------\n",
      "train Loss: 0.00014983. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020330. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 423/499\n",
      "----------\n",
      "train Loss: 0.00015105. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 424/499\n",
      "----------\n",
      "train Loss: 0.00014701. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 425/499\n",
      "----------\n",
      "train Loss: 0.00015093. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020321. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 426/499\n",
      "----------\n",
      "train Loss: 0.00014913. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020324. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 427/499\n",
      "----------\n",
      "train Loss: 0.00014935. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 428/499\n",
      "----------\n",
      "train Loss: 0.00014669. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020338. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 429/499\n",
      "----------\n",
      "train Loss: 0.00014395. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 430/499\n",
      "----------\n",
      "train Loss: 0.00014776. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020338. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 431/499\n",
      "----------\n",
      "train Loss: 0.00014545. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 432/499\n",
      "----------\n",
      "train Loss: 0.00014768. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 433/499\n",
      "----------\n",
      "train Loss: 0.00014581. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 434/499\n",
      "----------\n",
      "train Loss: 0.00014631. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 435/499\n",
      "----------\n",
      "train Loss: 0.00014873. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 436/499\n",
      "----------\n",
      "train Loss: 0.00014764. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 437/499\n",
      "----------\n",
      "train Loss: 0.00015000. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 438/499\n",
      "----------\n",
      "train Loss: 0.00014812. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 439/499\n",
      "----------\n",
      "train Loss: 0.00014873. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020324. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 440/499\n",
      "----------\n",
      "train Loss: 0.00014744. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020322. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 441/499\n",
      "----------\n",
      "train Loss: 0.00014918. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 442/499\n",
      "----------\n",
      "train Loss: 0.00014697. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 443/499\n",
      "----------\n",
      "train Loss: 0.00014815. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020327. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 444/499\n",
      "----------\n",
      "train Loss: 0.00015174. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020324. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 445/499\n",
      "----------\n",
      "train Loss: 0.00015002. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020330. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 446/499\n",
      "----------\n",
      "train Loss: 0.00014783. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020323. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 447/499\n",
      "----------\n",
      "train Loss: 0.00014691. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020322. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 448/499\n",
      "----------\n",
      "train Loss: 0.00014598. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 449/499\n",
      "----------\n",
      "train Loss: 0.00014493. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 450/499\n",
      "----------\n",
      "train Loss: 0.00015344. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020322. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 451/499\n",
      "----------\n",
      "train Loss: 0.00014765. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 452/499\n",
      "----------\n",
      "train Loss: 0.00015098. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020322. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 453/499\n",
      "----------\n",
      "train Loss: 0.00014599. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 454/499\n",
      "----------\n",
      "train Loss: 0.00014718. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020322. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 455/499\n",
      "----------\n",
      "train Loss: 0.00015006. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020320. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 456/499\n",
      "----------\n",
      "train Loss: 0.00014803. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020323. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 457/499\n",
      "----------\n",
      "train Loss: 0.00014679. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 458/499\n",
      "----------\n",
      "train Loss: 0.00015230. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020320. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 459/499\n",
      "----------\n",
      "train Loss: 0.00014625. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020326. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 460/499\n",
      "----------\n",
      "train Loss: 0.00015020. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020320. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 461/499\n",
      "----------\n",
      "train Loss: 0.00014767. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 462/499\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.00014830. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 463/499\n",
      "----------\n",
      "train Loss: 0.00014783. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020328. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 464/499\n",
      "----------\n",
      "train Loss: 0.00014716. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020325. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 465/499\n",
      "----------\n",
      "train Loss: 0.00014953. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020329. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 466/499\n",
      "----------\n",
      "train Loss: 0.00015013. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020341. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 467/499\n",
      "----------\n",
      "train Loss: 0.00014932. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020341. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 468/499\n",
      "----------\n",
      "train Loss: 0.00015026. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020343. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 469/499\n",
      "----------\n",
      "train Loss: 0.00014862. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020339. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 470/499\n",
      "----------\n",
      "train Loss: 0.00014649. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020347. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 471/499\n",
      "----------\n",
      "train Loss: 0.00014749. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020341. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 472/499\n",
      "----------\n",
      "train Loss: 0.00014607. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020336. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 473/499\n",
      "----------\n",
      "train Loss: 0.00014667. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020338. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 474/499\n",
      "----------\n",
      "train Loss: 0.00014866. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 475/499\n",
      "----------\n",
      "train Loss: 0.00014666. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 476/499\n",
      "----------\n",
      "train Loss: 0.00014728. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 477/499\n",
      "----------\n",
      "train Loss: 0.00014893. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020346. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 478/499\n",
      "----------\n",
      "train Loss: 0.00015236. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020350. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 479/499\n",
      "----------\n",
      "train Loss: 0.00015079. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020360. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 480/499\n",
      "----------\n",
      "train Loss: 0.00014776. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020346. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 481/499\n",
      "----------\n",
      "train Loss: 0.00014832. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020347. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 482/499\n",
      "----------\n",
      "train Loss: 0.00014797. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 483/499\n",
      "----------\n",
      "train Loss: 0.00014999. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020336. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 484/499\n",
      "----------\n",
      "train Loss: 0.00015035. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020353. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 485/499\n",
      "----------\n",
      "train Loss: 0.00014960. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020343. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 486/499\n",
      "----------\n",
      "train Loss: 0.00014588. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020333. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 487/499\n",
      "----------\n",
      "train Loss: 0.00014684. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 488/499\n",
      "----------\n",
      "train Loss: 0.00015027. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020338. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 489/499\n",
      "----------\n",
      "train Loss: 0.00014796. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 490/499\n",
      "----------\n",
      "train Loss: 0.00014828. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 491/499\n",
      "----------\n",
      "train Loss: 0.00014940. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 492/499\n",
      "----------\n",
      "train Loss: 0.00014769. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 493/499\n",
      "----------\n",
      "train Loss: 0.00014877. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020338. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 494/499\n",
      "----------\n",
      "train Loss: 0.00015069. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020332. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 495/499\n",
      "----------\n",
      "train Loss: 0.00014682. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020324. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 496/499\n",
      "----------\n",
      "train Loss: 0.00015012. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020331. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 497/499\n",
      "----------\n",
      "train Loss: 0.00014939. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020334. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 498/499\n",
      "----------\n",
      "train Loss: 0.00015141. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020335. Learning rate = 1.0000000000000004e-08\n",
      "Epoch 499/499\n",
      "----------\n",
      "train Loss: 0.00014704. Learning rate = 1.0000000000000004e-08\n",
      "val Loss: 0.00020330. Learning rate = 1.0000000000000004e-08\n"
     ]
    }
   ],
   "source": [
    "model,loss_report = train_ae_model(net=model,data_loaders=dataloaders_train,\n",
    "                             optimizer=optimizer,loss_function=loss_function,\n",
    "                            n_epochs=epochs,scheduler=exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'train'): 0.0013797760561660485,\n",
       " (0, 'val'): 0.0036467586954434714,\n",
       " (1, 'train'): 0.001793733142592289,\n",
       " (1, 'val'): 0.002097165005074607,\n",
       " (2, 'train'): 0.0012672857070962589,\n",
       " (2, 'val'): 0.001992992642852995,\n",
       " (3, 'train'): 0.0011884989561858,\n",
       " (3, 'val'): 0.001842819016288828,\n",
       " (4, 'train'): 0.001103721455567413,\n",
       " (4, 'val'): 0.0018322684422687249,\n",
       " (5, 'train'): 0.001034714109091847,\n",
       " (5, 'val'): 0.0016714729092739246,\n",
       " (6, 'train'): 0.0009925312328117866,\n",
       " (6, 'val'): 0.0016599682470162709,\n",
       " (7, 'train'): 0.0009884778065262017,\n",
       " (7, 'val'): 0.0015841446540973804,\n",
       " (8, 'train'): 0.000942378960273884,\n",
       " (8, 'val'): 0.001399482289950053,\n",
       " (9, 'train'): 0.0009845559899177817,\n",
       " (9, 'val'): 0.0013086842717947784,\n",
       " (10, 'train'): 0.0009073166659584751,\n",
       " (10, 'val'): 0.0012631921304596795,\n",
       " (11, 'train'): 0.0009083885147615716,\n",
       " (11, 'val'): 0.0011532302531931135,\n",
       " (12, 'train'): 0.000908859315569754,\n",
       " (12, 'val'): 0.0010903419719802008,\n",
       " (13, 'train'): 0.0008567588541794706,\n",
       " (13, 'val'): 0.001060275912836746,\n",
       " (14, 'train'): 0.000878078932011569,\n",
       " (14, 'val'): 0.0010077292444529357,\n",
       " (15, 'train'): 0.0008170774930881129,\n",
       " (15, 'val'): 0.0009877161571273097,\n",
       " (16, 'train'): 0.0008174561274548372,\n",
       " (16, 'val'): 0.0009482079358012588,\n",
       " (17, 'train'): 0.0007544963931043943,\n",
       " (17, 'val'): 0.0009375391872944656,\n",
       " (18, 'train'): 0.0007546572877025163,\n",
       " (18, 'val'): 0.0008986139049132665,\n",
       " (19, 'train'): 0.0008133736059621528,\n",
       " (19, 'val'): 0.000869199702585185,\n",
       " (20, 'train'): 0.0007945508006270285,\n",
       " (20, 'val'): 0.000867273313579736,\n",
       " (21, 'train'): 0.0007372318111636021,\n",
       " (21, 'val'): 0.0008404949610983884,\n",
       " (22, 'train'): 0.0006701763788307155,\n",
       " (22, 'val'): 0.0008387024065962544,\n",
       " (23, 'train'): 0.0006797966133389208,\n",
       " (23, 'val'): 0.0007997408371280741,\n",
       " (24, 'train'): 0.0006200909545576131,\n",
       " (24, 'val'): 0.0008044256656258194,\n",
       " (25, 'train'): 0.0006766865823279928,\n",
       " (25, 'val'): 0.000763597242810108,\n",
       " (26, 'train'): 0.0006455189755393399,\n",
       " (26, 'val'): 0.0007470272895362643,\n",
       " (27, 'train'): 0.0005967393000092772,\n",
       " (27, 'val'): 0.0007506946998613852,\n",
       " (28, 'train'): 0.0006131738148353718,\n",
       " (28, 'val'): 0.0007210140702901063,\n",
       " (29, 'train'): 0.0006136186593384655,\n",
       " (29, 'val'): 0.0006992091183309202,\n",
       " (30, 'train'): 0.000664534133479551,\n",
       " (30, 'val'): 0.0006965489161235315,\n",
       " (31, 'train'): 0.0005803989867369334,\n",
       " (31, 'val'): 0.0007139095967566525,\n",
       " (32, 'train'): 0.0006690614304884717,\n",
       " (32, 'val'): 0.0006887072490321265,\n",
       " (33, 'train'): 0.000615683453226531,\n",
       " (33, 'val'): 0.0006877357071196591,\n",
       " (34, 'train'): 0.000544597774192139,\n",
       " (34, 'val'): 0.000695485414730178,\n",
       " (35, 'train'): 0.0005393669319649538,\n",
       " (35, 'val'): 0.0006681516490600727,\n",
       " (36, 'train'): 0.000543052813521138,\n",
       " (36, 'val'): 0.0006470515358227271,\n",
       " (37, 'train'): 0.0005560173929013588,\n",
       " (37, 'val'): 0.0006221588011141177,\n",
       " (38, 'train'): 0.0005192053159353909,\n",
       " (38, 'val'): 0.0006258208994512204,\n",
       " (39, 'train'): 0.0005074589754696246,\n",
       " (39, 'val'): 0.000607087794277403,\n",
       " (40, 'train'): 0.00047886665669028407,\n",
       " (40, 'val'): 0.0005912012937996122,\n",
       " (41, 'train'): 0.00047742560747321004,\n",
       " (41, 'val'): 0.0005812386947649497,\n",
       " (42, 'train'): 0.0004715350180588387,\n",
       " (42, 'val'): 0.0005772671213856449,\n",
       " (43, 'train'): 0.0004685178778513714,\n",
       " (43, 'val'): 0.0005496032880964103,\n",
       " (44, 'train'): 0.0004755579090366761,\n",
       " (44, 'val'): 0.0005109096980757184,\n",
       " (45, 'train'): 0.0004315742515717392,\n",
       " (45, 'val'): 0.000499781324631638,\n",
       " (46, 'train'): 0.0004698338107792316,\n",
       " (46, 'val'): 0.00046435405534726604,\n",
       " (47, 'train'): 0.00037697221462925273,\n",
       " (47, 'val'): 0.0004550147180755933,\n",
       " (48, 'train'): 0.0004208149405679217,\n",
       " (48, 'val'): 0.00042890508969624835,\n",
       " (49, 'train'): 0.0003581680443689779,\n",
       " (49, 'val'): 0.0004023298000295957,\n",
       " (50, 'train'): 0.0003265333189456551,\n",
       " (50, 'val'): 0.00039106231458761075,\n",
       " (51, 'train'): 0.0003590350453224447,\n",
       " (51, 'val'): 0.0003794129179031761,\n",
       " (52, 'train'): 0.00031298269621200033,\n",
       " (52, 'val'): 0.00037782694454546327,\n",
       " (53, 'train'): 0.0003036465150890527,\n",
       " (53, 'val'): 0.00036601335914046676,\n",
       " (54, 'train'): 0.00028912679947636744,\n",
       " (54, 'val'): 0.00035493914037942886,\n",
       " (55, 'train'): 0.00027637955456696173,\n",
       " (55, 'val'): 0.0003545891010650882,\n",
       " (56, 'train'): 0.0002868247339157043,\n",
       " (56, 'val'): 0.0003537578784205295,\n",
       " (57, 'train'): 0.0002591910565065013,\n",
       " (57, 'val'): 0.0003370936546060774,\n",
       " (58, 'train'): 0.00025494109528760117,\n",
       " (58, 'val'): 0.00032852176162931655,\n",
       " (59, 'train'): 0.00026155785760945745,\n",
       " (59, 'val'): 0.0003251908691944899,\n",
       " (60, 'train'): 0.00025577281808687583,\n",
       " (60, 'val'): 0.00032732739216751524,\n",
       " (61, 'train'): 0.00025422079488635063,\n",
       " (61, 'val'): 0.0003197374926121147,\n",
       " (62, 'train'): 0.00025736251673488705,\n",
       " (62, 'val'): 0.00032141473558213975,\n",
       " (63, 'train'): 0.0002572063822299242,\n",
       " (63, 'val'): 0.00031448535069271373,\n",
       " (64, 'train'): 0.00024335946094382693,\n",
       " (64, 'val'): 0.0003082916071569478,\n",
       " (65, 'train'): 0.00025756533809558106,\n",
       " (65, 'val'): 0.00030223369874336103,\n",
       " (66, 'train'): 0.00025533690738181275,\n",
       " (66, 'val'): 0.0003165094940750687,\n",
       " (67, 'train'): 0.0002549541338036458,\n",
       " (67, 'val'): 0.0003115780720556224,\n",
       " (68, 'train'): 0.00024632630973226495,\n",
       " (68, 'val'): 0.000303487534876223,\n",
       " (69, 'train'): 0.00023919575054336478,\n",
       " (69, 'val'): 0.00030278086800266193,\n",
       " (70, 'train'): 0.00025019907668508867,\n",
       " (70, 'val'): 0.00029338609979108525,\n",
       " (71, 'train'): 0.00023750638313315534,\n",
       " (71, 'val'): 0.0003026170587098157,\n",
       " (72, 'train'): 0.00023704684442943998,\n",
       " (72, 'val'): 0.0002939631058662026,\n",
       " (73, 'train'): 0.00022778605731825033,\n",
       " (73, 'val'): 0.00028691373558508023,\n",
       " (74, 'train'): 0.00028834172041603814,\n",
       " (74, 'val'): 0.0002863419897578381,\n",
       " (75, 'train'): 0.0002409573040764641,\n",
       " (75, 'val'): 0.00033422311147054035,\n",
       " (76, 'train'): 0.0002603381527242837,\n",
       " (76, 'val'): 0.00032988545933255444,\n",
       " (77, 'train'): 0.0002567192401599001,\n",
       " (77, 'val'): 0.00031020506112663834,\n",
       " (78, 'train'): 0.00024812446941656097,\n",
       " (78, 'val'): 0.0003052562889125612,\n",
       " (79, 'train'): 0.00023393988333366537,\n",
       " (79, 'val'): 0.00029488383895821043,\n",
       " (80, 'train'): 0.00023870651299754778,\n",
       " (80, 'val'): 0.00029168923006013585,\n",
       " (81, 'train'): 0.00023021132478283512,\n",
       " (81, 'val'): 0.00029095151910075437,\n",
       " (82, 'train'): 0.00022611158244587756,\n",
       " (82, 'val'): 0.00028766412287950516,\n",
       " (83, 'train'): 0.0002249301221497633,\n",
       " (83, 'val'): 0.0002843653265800741,\n",
       " (84, 'train'): 0.00022544241855265918,\n",
       " (84, 'val'): 0.0002837077093621095,\n",
       " (85, 'train'): 0.00023321805228651674,\n",
       " (85, 'val'): 0.0002786521109993811,\n",
       " (86, 'train'): 0.00022371154692437913,\n",
       " (86, 'val'): 0.00027662255214872184,\n",
       " (87, 'train'): 0.00021706754341721535,\n",
       " (87, 'val'): 0.00027520426859458286,\n",
       " (88, 'train'): 0.00021455536544736888,\n",
       " (88, 'val'): 0.00027273745379514166,\n",
       " (89, 'train'): 0.00021377574066045108,\n",
       " (89, 'val'): 0.00027151317853066657,\n",
       " (90, 'train'): 0.0002180137024778459,\n",
       " (90, 'val'): 0.00027337789328561886,\n",
       " (91, 'train'): 0.00023730691625840135,\n",
       " (91, 'val'): 0.00026806155909542685,\n",
       " (92, 'train'): 0.00021922800051807253,\n",
       " (92, 'val'): 0.00027011088268072517,\n",
       " (93, 'train'): 0.00021807194463218803,\n",
       " (93, 'val'): 0.0002630830880392481,\n",
       " (94, 'train'): 0.00020991615450906533,\n",
       " (94, 'val'): 0.00026216233770052594,\n",
       " (95, 'train'): 0.00020758709352877404,\n",
       " (95, 'val'): 0.000263635258845709,\n",
       " (96, 'train'): 0.00020666918458624018,\n",
       " (96, 'val'): 0.00026474655088451173,\n",
       " (97, 'train'): 0.0002030612150621083,\n",
       " (97, 'val'): 0.00026174458778566786,\n",
       " (98, 'train'): 0.00020288591083415127,\n",
       " (98, 'val'): 0.0002619766768206049,\n",
       " (99, 'train'): 0.00020813490091650574,\n",
       " (99, 'val'): 0.0002589927537849656,\n",
       " (100, 'train'): 0.0002020026462290574,\n",
       " (100, 'val'): 0.00026113570978244144,\n",
       " (101, 'train'): 0.00020951535380272953,\n",
       " (101, 'val'): 0.00025646157424758984,\n",
       " (102, 'train'): 0.00020439868913618502,\n",
       " (102, 'val'): 0.0002586913502050771,\n",
       " (103, 'train'): 0.00021200602511978813,\n",
       " (103, 'val'): 0.00025470777311258845,\n",
       " (104, 'train'): 0.00020314227893120714,\n",
       " (104, 'val'): 0.0002548247230825601,\n",
       " (105, 'train'): 0.00020454679098394181,\n",
       " (105, 'val'): 0.00025443712042437657,\n",
       " (106, 'train'): 0.0001994010742270836,\n",
       " (106, 'val'): 0.00025562641935216054,\n",
       " (107, 'train'): 0.00019956086934716613,\n",
       " (107, 'val'): 0.0002525850302643246,\n",
       " (108, 'train'): 0.0002013114194765135,\n",
       " (108, 'val'): 0.00025216764253046777,\n",
       " (109, 'train'): 0.00022706919763651158,\n",
       " (109, 'val'): 0.0002532808489545628,\n",
       " (110, 'train'): 0.0002050780975777242,\n",
       " (110, 'val'): 0.00026026093918416236,\n",
       " (111, 'train'): 0.00020197163232498698,\n",
       " (111, 'val'): 0.0002502303708482672,\n",
       " (112, 'train'): 0.00019888838546143638,\n",
       " (112, 'val'): 0.00025260648517696946,\n",
       " (113, 'train'): 0.00020291737315279466,\n",
       " (113, 'val'): 0.0002490691150780077,\n",
       " (114, 'train'): 0.00020072188797510334,\n",
       " (114, 'val'): 0.00025144983427944007,\n",
       " (115, 'train'): 0.0001983016867328573,\n",
       " (115, 'val'): 0.00025092672418665005,\n",
       " (116, 'train'): 0.00019925788338123648,\n",
       " (116, 'val'): 0.000248832490157198,\n",
       " (117, 'train'): 0.000196597616498669,\n",
       " (117, 'val'): 0.00024637788603151285,\n",
       " (118, 'train'): 0.00020245896842054748,\n",
       " (118, 'val'): 0.00024353410979663885,\n",
       " (119, 'train'): 0.00019263242440367186,\n",
       " (119, 'val'): 0.0002478405453816608,\n",
       " (120, 'train'): 0.00019807614282601408,\n",
       " (120, 'val'): 0.0002427480735436634,\n",
       " (121, 'train'): 0.00019343424571195133,\n",
       " (121, 'val'): 0.0002420011355921074,\n",
       " (122, 'train'): 0.00019671074632141326,\n",
       " (122, 'val'): 0.00023983206806911362,\n",
       " (123, 'train'): 0.0001908721729975056,\n",
       " (123, 'val'): 0.00024123952068664408,\n",
       " (124, 'train'): 0.0001941021721534155,\n",
       " (124, 'val'): 0.00024076552923630784,\n",
       " (125, 'train'): 0.00019028774221186285,\n",
       " (125, 'val'): 0.00024045822728011344,\n",
       " (126, 'train'): 0.0001865415833890438,\n",
       " (126, 'val'): 0.00023997595740689171,\n",
       " (127, 'train'): 0.0001921265739602623,\n",
       " (127, 'val'): 0.0002378120121580583,\n",
       " (128, 'train'): 0.0001900214227606301,\n",
       " (128, 'val'): 0.00023939829595662928,\n",
       " (129, 'train'): 0.00018729084102367914,\n",
       " (129, 'val'): 0.00024179112235153162,\n",
       " (130, 'train'): 0.0001833519242979862,\n",
       " (130, 'val'): 0.00023583362430885986,\n",
       " (131, 'train'): 0.00018722417816105817,\n",
       " (131, 'val'): 0.0002379920333623886,\n",
       " (132, 'train'): 0.00018011458666512259,\n",
       " (132, 'val'): 0.0002356578630429727,\n",
       " (133, 'train'): 0.0001841517751691518,\n",
       " (133, 'val'): 0.0002359822047529397,\n",
       " (134, 'train'): 0.00018419533174622943,\n",
       " (134, 'val'): 0.00023428925002614656,\n",
       " (135, 'train'): 0.00017737027223187465,\n",
       " (135, 'val'): 0.00023287565757830939,\n",
       " (136, 'train'): 0.00017986963589296297,\n",
       " (136, 'val'): 0.00023195720105259507,\n",
       " (137, 'train'): 0.00017974476105774993,\n",
       " (137, 'val'): 0.0002319097898348614,\n",
       " (138, 'train'): 0.00017824388298861408,\n",
       " (138, 'val'): 0.00023118597972724173,\n",
       " (139, 'train'): 0.00017702065546203543,\n",
       " (139, 'val'): 0.00023004454043176438,\n",
       " (140, 'train'): 0.00017808659295379012,\n",
       " (140, 'val'): 0.00023120193293801062,\n",
       " (141, 'train'): 0.00017907271696323597,\n",
       " (141, 'val'): 0.00023008193130846376,\n",
       " (142, 'train'): 0.00017887187466301299,\n",
       " (142, 'val'): 0.00022872589114639495,\n",
       " (143, 'train'): 0.0001774608390405774,\n",
       " (143, 'val'): 0.0002288494811013893,\n",
       " (144, 'train'): 0.00017623632663378008,\n",
       " (144, 'val'): 0.0002286878966346935,\n",
       " (145, 'train'): 0.00017341749777120572,\n",
       " (145, 'val'): 0.00022676402564953874,\n",
       " (146, 'train'): 0.0001732367349581586,\n",
       " (146, 'val'): 0.00022670635263676996,\n",
       " (147, 'train'): 0.00017874340819953768,\n",
       " (147, 'val'): 0.00022633720396293534,\n",
       " (148, 'train'): 0.00017130836689223847,\n",
       " (148, 'val'): 0.00022554150954992683,\n",
       " (149, 'train'): 0.00018022069707512856,\n",
       " (149, 'val'): 0.00022534237898610258,\n",
       " (150, 'train'): 0.0001745186099368665,\n",
       " (150, 'val'): 0.00022599297679132887,\n",
       " (151, 'train'): 0.0001777391622049941,\n",
       " (151, 'val'): 0.0002260069983700911,\n",
       " (152, 'train'): 0.0001783162027735401,\n",
       " (152, 'val'): 0.00022524821192578033,\n",
       " (153, 'train'): 0.00017270150042518421,\n",
       " (153, 'val'): 0.00022417919158383652,\n",
       " (154, 'train'): 0.00017185561807343252,\n",
       " (154, 'val'): 0.0002240131919582685,\n",
       " (155, 'train'): 0.0001754407486360934,\n",
       " (155, 'val'): 0.00022398099234258688,\n",
       " (156, 'train'): 0.00016776511997536377,\n",
       " (156, 'val'): 0.00022298437370746224,\n",
       " (157, 'train'): 0.0001706637148917825,\n",
       " (157, 'val'): 0.00022374362581306032,\n",
       " (158, 'train'): 0.00017188565753814246,\n",
       " (158, 'val'): 0.0002214021946269053,\n",
       " (159, 'train'): 0.0001670115334154279,\n",
       " (159, 'val'): 0.00022058432093924947,\n",
       " (160, 'train'): 0.00016902889021568827,\n",
       " (160, 'val'): 0.00022220708153865957,\n",
       " (161, 'train'): 0.00016457751755499176,\n",
       " (161, 'val'): 0.0002205729208610676,\n",
       " (162, 'train'): 0.00016642638689113988,\n",
       " (162, 'val'): 0.0002201107434100575,\n",
       " (163, 'train'): 0.0001620246459626489,\n",
       " (163, 'val'): 0.00021950862611885423,\n",
       " (164, 'train'): 0.0001637696167799058,\n",
       " (164, 'val'): 0.0002186808873105932,\n",
       " (165, 'train'): 0.00016555159042278925,\n",
       " (165, 'val'): 0.0002195044869074115,\n",
       " (166, 'train'): 0.00016533171637328687,\n",
       " (166, 'val'): 0.00021854332751697965,\n",
       " (167, 'train'): 0.00016334207504297847,\n",
       " (167, 'val'): 0.00021869601267907355,\n",
       " (168, 'train'): 0.00016105550996683263,\n",
       " (168, 'val'): 0.00021883088198524934,\n",
       " (169, 'train'): 0.0001628813592510091,\n",
       " (169, 'val'): 0.00021855760779645707,\n",
       " (170, 'train'): 0.0001626673533960625,\n",
       " (170, 'val'): 0.00021908889283184653,\n",
       " (171, 'train'): 0.00016286544053366892,\n",
       " (171, 'val'): 0.00021868190486673956,\n",
       " (172, 'train'): 0.00016144774336781766,\n",
       " (172, 'val'): 0.00021711019454178986,\n",
       " (173, 'train'): 0.00016032780210177103,\n",
       " (173, 'val'): 0.0002170805991799743,\n",
       " (174, 'train'): 0.00016291576644612683,\n",
       " (174, 'val'): 0.00021747173741459846,\n",
       " (175, 'train'): 0.00016031403922372393,\n",
       " (175, 'val'): 0.00021840197344621023,\n",
       " (176, 'train'): 0.00017381314170995243,\n",
       " (176, 'val'): 0.00021923307536376847,\n",
       " (177, 'train'): 0.00016245102785803652,\n",
       " (177, 'val'): 0.00022079517926882814,\n",
       " (178, 'train'): 0.0001641709952511721,\n",
       " (178, 'val'): 0.00021806743892806548,\n",
       " (179, 'train'): 0.00016118106173558367,\n",
       " (179, 'val'): 0.0002168337986977012,\n",
       " (180, 'train'): 0.0001635149086791056,\n",
       " (180, 'val'): 0.00021701194001016793,\n",
       " (181, 'train'): 0.00016054678794548467,\n",
       " (181, 'val'): 0.00021486153343209513,\n",
       " (182, 'train'): 0.00015860742078749118,\n",
       " (182, 'val'): 0.00021505014350016913,\n",
       " (183, 'train'): 0.00015924045281415737,\n",
       " (183, 'val'): 0.00021501646066705385,\n",
       " (184, 'train'): 0.0001599158082778255,\n",
       " (184, 'val'): 0.00021505176469131752,\n",
       " (185, 'train'): 0.00015655202635874352,\n",
       " (185, 'val'): 0.00021291675942915458,\n",
       " (186, 'train'): 0.00015276236701066846,\n",
       " (186, 'val'): 0.00021265916250370168,\n",
       " (187, 'train'): 0.00015573662326291755,\n",
       " (187, 'val'): 0.00021026259357178652,\n",
       " (188, 'train'): 0.00015575088198400207,\n",
       " (188, 'val'): 0.00021041736558631615,\n",
       " (189, 'train'): 0.00015411146105853495,\n",
       " (189, 'val'): 0.0002074735584082427,\n",
       " (190, 'train'): 0.0001562953952492939,\n",
       " (190, 'val'): 0.0002081909009979831,\n",
       " (191, 'train'): 0.00015378933555136123,\n",
       " (191, 'val'): 0.00020865131928413002,\n",
       " (192, 'train'): 0.00015349902592047497,\n",
       " (192, 'val'): 0.00020713222868464612,\n",
       " (193, 'train'): 0.00015458062774053327,\n",
       " (193, 'val'): 0.0002059998611609141,\n",
       " (194, 'train'): 0.00015307414017755677,\n",
       " (194, 'val'): 0.0002063321708529084,\n",
       " (195, 'train'): 0.00015328862031714784,\n",
       " (195, 'val'): 0.00020613216070665253,\n",
       " (196, 'train'): 0.00015379071097683023,\n",
       " (196, 'val'): 0.00020608017910961752,\n",
       " (197, 'train'): 0.00015048450290190953,\n",
       " (197, 'val'): 0.00020662827969149307,\n",
       " (198, 'train'): 0.0001541649086262893,\n",
       " (198, 'val'): 0.00020610475567755875,\n",
       " (199, 'train'): 0.00015103788529005315,\n",
       " (199, 'val'): 0.00020674086624273546,\n",
       " (200, 'train'): 0.00015546096471586712,\n",
       " (200, 'val'): 0.00020655960327497236,\n",
       " (201, 'train'): 0.0001482642083256333,\n",
       " (201, 'val'): 0.00020652095338812582,\n",
       " (202, 'train'): 0.00014946089747051397,\n",
       " (202, 'val'): 0.00020652502361271117,\n",
       " (203, 'train'): 0.00014867837092390768,\n",
       " (203, 'val'): 0.00020605955203926123,\n",
       " (204, 'train'): 0.00014948539642824067,\n",
       " (204, 'val'): 0.00020554753158379485,\n",
       " (205, 'train'): 0.00015109356199563654,\n",
       " (205, 'val'): 0.00020469759625417214,\n",
       " (206, 'train'): 0.00014983537106740254,\n",
       " (206, 'val'): 0.00020508299133291952,\n",
       " (207, 'train'): 0.00015074082357050092,\n",
       " (207, 'val'): 0.00020536602716203088,\n",
       " (208, 'train'): 0.0001503879040548647,\n",
       " (208, 'val'): 0.00020578441520531973,\n",
       " (209, 'train'): 0.00015079898380294994,\n",
       " (209, 'val'): 0.00020567346709194007,\n",
       " (210, 'train'): 0.0001512511883413902,\n",
       " (210, 'val'): 0.00020564956314585828,\n",
       " (211, 'train'): 0.0001493440294224355,\n",
       " (211, 'val'): 0.00020544743165373802,\n",
       " (212, 'train'): 0.00015239140312014906,\n",
       " (212, 'val'): 0.0002058707695040438,\n",
       " (213, 'train'): 0.00015171280096250552,\n",
       " (213, 'val'): 0.00020554097783234384,\n",
       " (214, 'train'): 0.00014904983928082165,\n",
       " (214, 'val'): 0.00020510272157412988,\n",
       " (215, 'train'): 0.00014973466319066506,\n",
       " (215, 'val'): 0.00020435135121698732,\n",
       " (216, 'train'): 0.00014821251561106355,\n",
       " (216, 'val'): 0.00020388148173137947,\n",
       " (217, 'train'): 0.00014985536432100667,\n",
       " (217, 'val'): 0.00020380342310225523,\n",
       " (218, 'train'): 0.00015007476408586458,\n",
       " (218, 'val'): 0.0002037017709679074,\n",
       " (219, 'train'): 0.0001479248835325793,\n",
       " (219, 'val'): 0.00020372684769056463,\n",
       " (220, 'train'): 0.00014925471300052272,\n",
       " (220, 'val'): 0.00020372389850241166,\n",
       " (221, 'train'): 0.000145870058245405,\n",
       " (221, 'val'): 0.0002036874734417156,\n",
       " (222, 'train'): 0.00014780826125018024,\n",
       " (222, 'val'): 0.000203672434306807,\n",
       " (223, 'train'): 0.00014691110441668166,\n",
       " (223, 'val'): 0.00020368428279956183,\n",
       " (224, 'train'): 0.00015016995301401175,\n",
       " (224, 'val'): 0.00020367400375781235,\n",
       " (225, 'train'): 0.00014764115352321555,\n",
       " (225, 'val'): 0.00020361724482090385,\n",
       " (226, 'train'): 0.00014856368889687238,\n",
       " (226, 'val'): 0.0002035785087004856,\n",
       " (227, 'train'): 0.00014790037163981684,\n",
       " (227, 'val'): 0.00020352925208431703,\n",
       " (228, 'train'): 0.0001489801453081546,\n",
       " (228, 'val'): 0.000203445502039459,\n",
       " (229, 'train'): 0.00015020229922676527,\n",
       " (229, 'val'): 0.00020348718734803024,\n",
       " (230, 'train'): 0.00014812501440583556,\n",
       " (230, 'val'): 0.00020346819871553667,\n",
       " (231, 'train'): 0.00014647102431842573,\n",
       " (231, 'val'): 0.00020354755084823678,\n",
       " (232, 'train'): 0.00014510114797977385,\n",
       " (232, 'val'): 0.00020348685966045768,\n",
       " (233, 'train'): 0.00014750584442582395,\n",
       " (233, 'val'): 0.00020345124519533582,\n",
       " (234, 'train'): 0.00014791470365943732,\n",
       " (234, 'val'): 0.00020355979601542154,\n",
       " (235, 'train'): 0.00015001910893867412,\n",
       " (235, 'val'): 0.00020343962090986747,\n",
       " (236, 'train'): 0.00014845104629380835,\n",
       " (236, 'val'): 0.0002033939861037113,\n",
       " (237, 'train'): 0.00014783843006524775,\n",
       " (237, 'val'): 0.00020337210002320785,\n",
       " (238, 'train'): 0.00015191827400553006,\n",
       " (238, 'val'): 0.00020343918974200884,\n",
       " (239, 'train'): 0.00014731765690225143,\n",
       " (239, 'val'): 0.00020342252941595184,\n",
       " (240, 'train'): 0.0001485509521983288,\n",
       " (240, 'val'): 0.00020354070390264192,\n",
       " (241, 'train'): 0.00015215587336570024,\n",
       " (241, 'val'): 0.0002036102426548799,\n",
       " (242, 'train'): 0.0001465122267189953,\n",
       " (242, 'val'): 0.00020339878069029914,\n",
       " (243, 'train'): 0.00014892803004908341,\n",
       " (243, 'val'): 0.00020338803598726236,\n",
       " (244, 'train'): 0.0001493290420276699,\n",
       " (244, 'val'): 0.00020345793692050156,\n",
       " (245, 'train'): 0.00014687440772023465,\n",
       " (245, 'val'): 0.0002034367407085719,\n",
       " (246, 'train'): 0.00014665548655169982,\n",
       " (246, 'val'): 0.00020339795284801058,\n",
       " (247, 'train'): 0.00015195350904293633,\n",
       " (247, 'val'): 0.00020341840745122345,\n",
       " (248, 'train'): 0.00014832597312138037,\n",
       " (248, 'val'): 0.00020341832121765173,\n",
       " (249, 'train'): 0.00014841889841826978,\n",
       " (249, 'val'): 0.00020332039437360235,\n",
       " (250, 'train'): 0.00014960960295327282,\n",
       " (250, 'val'): 0.00020332013567288718,\n",
       " (251, 'train'): 0.00014948789720182066,\n",
       " (251, 'val'): 0.00020325432221094766,\n",
       " (252, 'train'): 0.00014846955201830024,\n",
       " (252, 'val'): 0.00020334566081011736,\n",
       " (253, 'train'): 0.00014692584604576782,\n",
       " (253, 'val'): 0.00020333386405750557,\n",
       " (254, 'train'): 0.00014804183350255093,\n",
       " (254, 'val'): 0.0002033051137846929,\n",
       " (255, 'train'): 0.00014884192582771734,\n",
       " (255, 'val'): 0.0002032964731808062,\n",
       " (256, 'train'): 0.00014792491802600798,\n",
       " (256, 'val'): 0.0002032738454915859,\n",
       " (257, 'train'): 0.00014894861400265385,\n",
       " (257, 'val'): 0.0002032754666827343,\n",
       " (258, 'train'): 0.0001486843512221067,\n",
       " (258, 'val'): 0.00020331511687901284,\n",
       " (259, 'train'): 0.00014827265921566222,\n",
       " (259, 'val'): 0.0002033002502112477,\n",
       " (260, 'train'): 0.0001500447677379405,\n",
       " (260, 'val'): 0.00020335387024614547,\n",
       " (261, 'train'): 0.00014790923213931146,\n",
       " (261, 'val'): 0.00020339198548484732,\n",
       " (262, 'train'): 0.00014986174560531422,\n",
       " (262, 'val'): 0.0002033613208267424,\n",
       " (263, 'train'): 0.00014947081001958362,\n",
       " (263, 'val'): 0.0002032953866378025,\n",
       " (264, 'train'): 0.00014595553727337607,\n",
       " (264, 'val'): 0.00020331773837959326,\n",
       " (265, 'train'): 0.00014736126090779349,\n",
       " (265, 'val'): 0.00020336663281476057,\n",
       " (266, 'train'): 0.000148765764337171,\n",
       " (266, 'val'): 0.00020334638517211984,\n",
       " (267, 'train'): 0.000146167007861314,\n",
       " (267, 'val'): 0.0002033102360588533,\n",
       " (268, 'train'): 0.00014948327077069767,\n",
       " (268, 'val'): 0.0002033624073697461,\n",
       " (269, 'train'): 0.00014717426771918932,\n",
       " (269, 'val'): 0.00020335664696715498,\n",
       " (270, 'train'): 0.00014765225609557496,\n",
       " (270, 'val'): 0.000203380412939522,\n",
       " (271, 'train'): 0.0001492839246229441,\n",
       " (271, 'val'): 0.00020338196514381302,\n",
       " (272, 'train'): 0.00014802575525310304,\n",
       " (272, 'val'): 0.00020332398169018603,\n",
       " (273, 'train'): 0.00014941463747096282,\n",
       " (273, 'val'): 0.0002033005951455346,\n",
       " (274, 'train'): 0.00014754540407685218,\n",
       " (274, 'val'): 0.0002032368340426021,\n",
       " (275, 'train'): 0.0001456815861510458,\n",
       " (275, 'val'): 0.0002032151549226708,\n",
       " (276, 'train'): 0.00014999077258700573,\n",
       " (276, 'val'): 0.00020324399142905518,\n",
       " (277, 'train'): 0.00014878839633806988,\n",
       " (277, 'val'): 0.00020331087418728403,\n",
       " (278, 'train'): 0.00015292689635383863,\n",
       " (278, 'val'): 0.00020345464279806173,\n",
       " (279, 'train'): 0.00014797423931735534,\n",
       " (279, 'val'): 0.0002034311872665529,\n",
       " (280, 'train'): 0.00014788755301938013,\n",
       " (280, 'val'): 0.00020332191208446466,\n",
       " (281, 'train'): 0.0001483840341852219,\n",
       " (281, 'val'): 0.0002032776570154561,\n",
       " (282, 'train'): 0.00014932122495439317,\n",
       " (282, 'val'): 0.0002033887086091218,\n",
       " (283, 'train'): 0.00014610599329764091,\n",
       " (283, 'val'): 0.00020333446769250763,\n",
       " (284, 'train'): 0.00014773611824407622,\n",
       " (284, 'val'): 0.00020325459815837718,\n",
       " (285, 'train'): 0.00014778724181707258,\n",
       " (285, 'val'): 0.0002033341055115064,\n",
       " (286, 'train'): 0.00014610017253154957,\n",
       " (286, 'val'): 0.00020334383265839683,\n",
       " (287, 'train'): 0.00014691050509335819,\n",
       " (287, 'val'): 0.00020331077070699798,\n",
       " (288, 'train'): 0.00014955713844823617,\n",
       " (288, 'val'): 0.00020334866173841334,\n",
       " (289, 'train'): 0.00014743151971035532,\n",
       " (289, 'val'): 0.00020329224773579172,\n",
       " (290, 'train'): 0.00014732900955196883,\n",
       " (290, 'val'): 0.00020327018918814483,\n",
       " (291, 'train'): 0.00014657579810806997,\n",
       " (291, 'val'): 0.00020327882979203154,\n",
       " (292, 'train'): 0.00014628377242910642,\n",
       " (292, 'val'): 0.00020334190102639022,\n",
       " (293, 'train'): 0.00015223354825543033,\n",
       " (293, 'val'): 0.0002033134784411501,\n",
       " (294, 'train'): 0.00015105024687255973,\n",
       " (294, 'val'): 0.00020328638385291452,\n",
       " (295, 'train'): 0.00014752213825920113,\n",
       " (295, 'val'): 0.00020322924548829042,\n",
       " (296, 'train'): 0.00015014827389408042,\n",
       " (296, 'val'): 0.00020330668323569826,\n",
       " (297, 'train'): 0.00015074165141278946,\n",
       " (297, 'val'): 0.00020341678626007505,\n",
       " (298, 'train'): 0.00014830534173934548,\n",
       " (298, 'val'): 0.00020331030504571067,\n",
       " (299, 'train'): 0.00014822876632765487,\n",
       " (299, 'val'): 0.0002032808131641812,\n",
       " (300, 'train'): 0.00014968323780016767,\n",
       " (300, 'val'): 0.00020339771139400977,\n",
       " (301, 'train'): 0.00015101789634812762,\n",
       " (301, 'val'): 0.0002034153375360701,\n",
       " (302, 'train'): 0.00014786215292082893,\n",
       " (302, 'val'): 0.00020342175331380632,\n",
       " (303, 'train'): 0.00015087312742791795,\n",
       " (303, 'val'): 0.00020337541139236204,\n",
       " (304, 'train'): 0.0001469223880795417,\n",
       " (304, 'val'): 0.00020336104487931288,\n",
       " (305, 'train'): 0.00015030363229689774,\n",
       " (305, 'val'): 0.00020335074859084907,\n",
       " (306, 'train'): 0.0001515918238847344,\n",
       " (306, 'val'): 0.0002034450708716004,\n",
       " (307, 'train'): 0.00014751620107778795,\n",
       " (307, 'val'): 0.0002033669949957618,\n",
       " (308, 'train'): 0.0001489353340326084,\n",
       " (308, 'val'): 0.00020328119259189676,\n",
       " (309, 'train'): 0.00014820663879315057,\n",
       " (309, 'val'): 0.00020328148578604063,\n",
       " (310, 'train'): 0.00014713878260442503,\n",
       " (310, 'val'): 0.00020332927643148987,\n",
       " (311, 'train'): 0.0001484003538886706,\n",
       " (311, 'val'): 0.00020326789537513697,\n",
       " (312, 'train'): 0.00014655264870574078,\n",
       " (312, 'val'): 0.00020326542909498567,\n",
       " (313, 'train'): 0.0001480304808528335,\n",
       " (313, 'val'): 0.00020327301764929735,\n",
       " (314, 'train'): 0.00014851648895138943,\n",
       " (314, 'val'): 0.00020325006227250452,\n",
       " (315, 'train'): 0.00014881360672276328,\n",
       " (315, 'val'): 0.0002033270171119107,\n",
       " (316, 'train'): 0.00015001734977381097,\n",
       " (316, 'val'): 0.0002033010263133932,\n",
       " (317, 'train'): 0.00014795385370099985,\n",
       " (317, 'val'): 0.0002033144442571534,\n",
       " (318, 'train'): 0.00014705117791891098,\n",
       " (318, 'val'): 0.00020335016220256134,\n",
       " (319, 'train'): 0.00014709811054032157,\n",
       " (319, 'val'): 0.00020332831061548658,\n",
       " (320, 'train'): 0.00014651106256577704,\n",
       " (320, 'val'): 0.00020331620342201658,\n",
       " (321, 'train'): 0.00014574232907896793,\n",
       " (321, 'val'): 0.000203311322601857,\n",
       " (322, 'train'): 0.00014717856215106117,\n",
       " (322, 'val'): 0.00020331156405585784,\n",
       " (323, 'train'): 0.00014778805241264678,\n",
       " (323, 'val'): 0.00020338953645141037,\n",
       " (324, 'train'): 0.00014681610951407088,\n",
       " (324, 'val'): 0.00020342985926954834,\n",
       " (325, 'train'): 0.00014944329719852517,\n",
       " (325, 'val'): 0.0002033745145632161,\n",
       " (326, 'train'): 0.0001490432769060135,\n",
       " (326, 'val'): 0.0002033394692396676,\n",
       " (327, 'train'): 0.00014892858625562103,\n",
       " (327, 'val'): 0.0002033559226051525,\n",
       " (328, 'train'): 0.0001502888173692756,\n",
       " (328, 'val'): 0.00020332822438191484,\n",
       " (329, 'train'): 0.00015050387958547584,\n",
       " (329, 'val'): 0.0002033217741107499,\n",
       " (330, 'train'): 0.0001492417650297284,\n",
       " (330, 'val'): 0.00020333255330721536,\n",
       " (331, 'train'): 0.00014703641904311048,\n",
       " (331, 'val'): 0.00020342334001152604,\n",
       " (332, 'train'): 0.00014931555509705235,\n",
       " (332, 'val'): 0.00020338979515212554,\n",
       " (333, 'train'): 0.00014658620650017702,\n",
       " (333, 'val'): 0.00020334750620855225,\n",
       " (334, 'train'): 0.0001478684264131718,\n",
       " (334, 'val'): 0.00020333165647806945,\n",
       " (335, 'train'): 0.000150045617138622,\n",
       " (335, 'val'): 0.00020328917782063837,\n",
       " (336, 'train'): 0.0001511673434396033,\n",
       " (336, 'val'): 0.0002032632042688352,\n",
       " (337, 'train'): 0.00014952306756404814,\n",
       " (337, 'val'): 0.00020320753187493042,\n",
       " (338, 'train'): 0.0001475680791945369,\n",
       " (338, 'val'): 0.00020320422050577622,\n",
       " (339, 'train'): 0.00014990408197735196,\n",
       " (339, 'val'): 0.00020326827480285256,\n",
       " (340, 'train'): 0.0001472206182639908,\n",
       " (340, 'val'): 0.00020327563914987777,\n",
       " (341, 'train'): 0.00014505085224906603,\n",
       " (341, 'val'): 0.00020326410109798113,\n",
       " (342, 'train'): 0.00014786943965763958,\n",
       " (342, 'val'): 0.00020328153752618365,\n",
       " (343, 'train'): 0.00014782389539673372,\n",
       " (343, 'val'): 0.00020334954132084494,\n",
       " (344, 'train'): 0.0001487245446898871,\n",
       " (344, 'val'): 0.00020333831370980651,\n",
       " (345, 'train'): 0.00015437361542825346,\n",
       " (345, 'val'): 0.00020369804567760893,\n",
       " (346, 'train'): 0.00014948060615333143,\n",
       " (346, 'val'): 0.00020363287034410018,\n",
       " (347, 'train'): 0.00014911150921963982,\n",
       " (347, 'val'): 0.0002036880080898603,\n",
       " (348, 'train'): 0.00014892617602729134,\n",
       " (348, 'val'): 0.00020353604728976885,\n",
       " (349, 'train'): 0.00014789609014298076,\n",
       " (349, 'val'): 0.00020344534681902992,\n",
       " (350, 'train'): 0.0001489059671897579,\n",
       " (350, 'val'): 0.00020343693042242968,\n",
       " (351, 'train'): 0.0001479135524412548,\n",
       " (351, 'val'): 0.00020334407411239767,\n",
       " (352, 'train'): 0.0001481368844569833,\n",
       " (352, 'val'): 0.00020334129739138816,\n",
       " (353, 'train'): 0.00014622731099802035,\n",
       " (353, 'val'): 0.00020335040365656218,\n",
       " (354, 'train'): 0.00014712898215899864,\n",
       " (354, 'val'): 0.00020330554495255151,\n",
       " (355, 'train'): 0.00014540715212071382,\n",
       " (355, 'val'): 0.00020334114217095904,\n",
       " (356, 'train'): 0.00014932366536447294,\n",
       " (356, 'val'): 0.00020327365577772812,\n",
       " (357, 'train'): 0.0001483064282823492,\n",
       " (357, 'val'): 0.00020334793737641088,\n",
       " (358, 'train'): 0.0001499657303577772,\n",
       " (358, 'val'): 0.00020334397063211159,\n",
       " (359, 'train'): 0.00014908803644141666,\n",
       " (359, 'val'): 0.00020333991765424057,\n",
       " (360, 'train'): 0.00014823658340093162,\n",
       " (360, 'val'): 0.00020331649661616043,\n",
       " (361, 'train'): 0.00014878259281869287,\n",
       " (361, 'val'): 0.0002033922959257055,\n",
       " (362, 'train'): 0.00015092086202154556,\n",
       " (362, 'val'): 0.00020333639932451424,\n",
       " (363, 'train'): 0.00014575253051050282,\n",
       " (363, 'val'): 0.00020332784495419927,\n",
       " (364, 'train'): 0.00014923719033874846,\n",
       " (364, 'val'): 0.00020328327944433249,\n",
       " (365, 'train'): 0.00014752997257919223,\n",
       " (365, 'val'): 0.0002032964731808062,\n",
       " (366, 'train'): 0.00014924584818934954,\n",
       " (366, 'val'): 0.00020331839375473834,\n",
       " (367, 'train'): 0.00014981670581080294,\n",
       " (367, 'val'): 0.00020325818547496088,\n",
       " (368, 'train'): 0.00015404514313020087,\n",
       " (368, 'val'): 0.00020321432708038224,\n",
       " (369, 'train'): 0.00015036241772274175,\n",
       " (369, 'val'): 0.0002032320911961573,\n",
       " (370, 'train'): 0.00014482935269673666,\n",
       " (370, 'val'): 0.00020328424526033578,\n",
       " (371, 'train'): 0.00014801349715088253,\n",
       " (371, 'val'): 0.00020324561262020357,\n",
       " (372, 'train'): 0.00014740464932940626,\n",
       " (372, 'val'): 0.00020328912608049535,\n",
       " (373, 'train'): 0.00014864114820267314,\n",
       " (373, 'val'): 0.00020328221014804311,\n",
       " (374, 'train'): 0.00014545975459946526,\n",
       " (374, 'val'): 0.00020331096042085578,\n",
       " (375, 'train'): 0.00014715217036643513,\n",
       " (375, 'val'): 0.00020332458532518812,\n",
       " (376, 'train'): 0.00014988902128405042,\n",
       " (376, 'val'): 0.0002033196010247425,\n",
       " (377, 'train'): 0.00014707274062352048,\n",
       " (377, 'val'): 0.00020340680041246943,\n",
       " (378, 'train'): 0.00014794583397882957,\n",
       " (378, 'val'): 0.00020336188996831575,\n",
       " (379, 'train'): 0.00014756640626324547,\n",
       " (379, 'val'): 0.00020324854456164217,\n",
       " (380, 'train'): 0.0001505396449593482,\n",
       " (380, 'val'): 0.00020326806784228042,\n",
       " (381, 'train'): 0.000149253751496198,\n",
       " (381, 'val'): 0.00020319200983202017,\n",
       " (382, 'train'): 0.0001474376422939477,\n",
       " (382, 'val'): 0.00020319499351360178,\n",
       " (383, 'train'): 0.00015014903706119016,\n",
       " (383, 'val'): 0.00020323481617702378,\n",
       " (384, 'train'): 0.0001491807634010911,\n",
       " (384, 'val'): 0.0002033574748094435,\n",
       " (385, 'train'): 0.0001487418388326963,\n",
       " (385, 'val'): 0.00020333203590578504,\n",
       " (386, 'train'): 0.00014942523126524907,\n",
       " (386, 'val'): 0.00020331720373144857,\n",
       " (387, 'train'): 0.00015040838452814906,\n",
       " (387, 'val'): 0.00020325496033937843,\n",
       " (388, 'train'): 0.00014685759217374853,\n",
       " (388, 'val'): 0.00020328169274661277,\n",
       " (389, 'train'): 0.00014557269039667316,\n",
       " (389, 'val'): 0.0002033010263133932,\n",
       " (390, 'train'): 0.0001518028977982424,\n",
       " (390, 'val'): 0.0002033405902761,\n",
       " (391, 'train'): 0.00014895008428505175,\n",
       " (391, 'val'): 0.00020332605129590742,\n",
       " (392, 'train'): 0.00014725024380755645,\n",
       " (392, 'val'): 0.00020340666243875467,\n",
       " (393, 'train'): 0.00014788513847937187,\n",
       " (393, 'val'): 0.00020334464325397103,\n",
       " (394, 'train'): 0.00014930662561070035,\n",
       " (394, 'val'): 0.00020326460125269714,\n",
       " (395, 'train'): 0.00014964452323814234,\n",
       " (395, 'val'): 0.0002033686161869102,\n",
       " (396, 'train'): 0.00014831073564925678,\n",
       " (396, 'val'): 0.000203339038071809,\n",
       " (397, 'train'): 0.0001441798328111569,\n",
       " (397, 'val'): 0.00020339388262342523,\n",
       " (398, 'train'): 0.00014779342907584377,\n",
       " (398, 'val'): 0.00020336863343362456,\n",
       " (399, 'train'): 0.00014749686751100753,\n",
       " (399, 'val'): 0.00020329202352850525,\n",
       " (400, 'train'): 0.00014785759547656334,\n",
       " (400, 'val'): 0.0002033317599583555,\n",
       " (401, 'train'): 0.0001499779237848189,\n",
       " (401, 'val'): 0.0002033392277856668,\n",
       " (402, 'train'): 0.0001500936104329648,\n",
       " (402, 'val'): 0.00020327688091331057,\n",
       " (403, 'train'): 0.00014932076360478445,\n",
       " (403, 'val'): 0.00020324545739977448,\n",
       " (404, 'train'): 0.00014821226122202697,\n",
       " (404, 'val'): 0.0002032483548477844,\n",
       " (405, 'train'): 0.000154565105697623,\n",
       " (405, 'val'): 0.00020326020334053923,\n",
       " (406, 'train'): 0.0001481776082612298,\n",
       " (406, 'val'): 0.00020332448184490204,\n",
       " (407, 'train'): 0.00015086125737677017,\n",
       " (407, 'val'): 0.0002033135474280075,\n",
       " (408, 'train'): 0.00015051111889382204,\n",
       " (408, 'val'): 0.00020326206598568846,\n",
       " (409, 'train'): 0.00014964205695799105,\n",
       " (409, 'val'): 0.00020329838756609847,\n",
       " (410, 'train'): 0.00014921965043025989,\n",
       " (410, 'val'): 0.000203345091668544,\n",
       " (411, 'train'): 0.00014826534660878005,\n",
       " (411, 'val'): 0.0002034130264763479,\n",
       " (412, 'train'): 0.00014983272369675064,\n",
       " (412, 'val'): 0.00020358247544478486,\n",
       " (413, 'train'): 0.00014738114205775437,\n",
       " (413, 'val'): 0.00020348415192630555,\n",
       " (414, 'train'): 0.00014814064424071048,\n",
       " (414, 'val'): 0.00020334391889196855,\n",
       " (415, 'train'): 0.0001492147696101003,\n",
       " (415, 'val'): 0.00020334854101141295,\n",
       " (416, 'train'): 0.00014598265773168318,\n",
       " (416, 'val'): 0.00020332610303605043,\n",
       " (417, 'train'): 0.00015025454383619404,\n",
       " (417, 'val'): 0.00020339453799857033,\n",
       " (418, 'train'): 0.00015004079237028404,\n",
       " (418, 'val'): 0.00020328129607218284,\n",
       " (419, 'train'): 0.00014679839282676026,\n",
       " (419, 'val'): 0.00020326175554483025,\n",
       " (420, 'train'): 0.00015057738939369165,\n",
       " (420, 'val'): 0.0002033478683895535,\n",
       " (421, 'train'): 0.0001469968421453679,\n",
       " (421, 'val'): 0.00020335062786384865,\n",
       " (422, 'train'): 0.00014982626480222852,\n",
       " (422, 'val'): 0.0002032974045033808,\n",
       " (423, 'train'): 0.00015104666386765462,\n",
       " (423, 'val'): 0.00020326368717683686,\n",
       " (424, 'train'): 0.0001470075135498687,\n",
       " (424, 'val'): 0.0002032851765829104,\n",
       " (425, 'train'): 0.00015093327965587378,\n",
       " (425, 'val'): 0.00020321391315923797,\n",
       " (426, 'train'): 0.00014913377472785887,\n",
       " (426, 'val'): 0.0002032435430144822,\n",
       " (427, 'train'): 0.00014935227335189229,\n",
       " (427, 'val'): 0.0002032802785160365,\n",
       " (428, 'train'): 0.0001466917822620383,\n",
       " (428, 'val'): 0.0002033757045865059,\n",
       " (429, 'train'): 0.00014395106376873123,\n",
       " (429, 'val'): 0.00020334048679581396,\n",
       " (430, 'train'): 0.00014775748261147074,\n",
       " (430, 'val'): 0.00020337527341864728,\n",
       " (431, 'train'): 0.00014545196770793863,\n",
       " (431, 'val'): 0.0002033413146381025,\n",
       " (432, 'train'): 0.00014768369254414683,\n",
       " (432, 'val'): 0.00020328319321076074,\n",
       " (433, 'train'): 0.00014581337691871104,\n",
       " (433, 'val'): 0.00020328755662948997,\n",
       " (434, 'train'): 0.0001463104875896264,\n",
       " (434, 'val'): 0.0002033473682348375,\n",
       " (435, 'train'): 0.00014872883481008035,\n",
       " (435, 'val'): 0.00020335193861413886,\n",
       " (436, 'train'): 0.00014764336110265167,\n",
       " (436, 'val'): 0.0002033277759673419,\n",
       " (437, 'train'): 0.00015000477260737507,\n",
       " (437, 'val'): 0.00020329398965394056,\n",
       " (438, 'train'): 0.00014811639536034177,\n",
       " (438, 'val'): 0.00020328840171849287,\n",
       " (439, 'train'): 0.00014872668328246586,\n",
       " (439, 'val'): 0.00020324221501747766,\n",
       " (440, 'train'): 0.0001474367799582305,\n",
       " (440, 'val'): 0.00020322108779240538,\n",
       " (441, 'train'): 0.00014918127649084286,\n",
       " (441, 'val'): 0.00020325299421394312,\n",
       " (442, 'train'): 0.0001469735892627526,\n",
       " (442, 'val'): 0.00020327882979203154,\n",
       " (443, 'train'): 0.00014815338525093265,\n",
       " (443, 'val'): 0.0002032652048876992,\n",
       " (444, 'train'): 0.00015173993866752694,\n",
       " (444, 'val'): 0.00020324380171519738,\n",
       " (445, 'train'): 0.0001500238604084761,\n",
       " (445, 'val'): 0.0002033040272416892,\n",
       " (446, 'train'): 0.00014783096654961506,\n",
       " (446, 'val'): 0.00020323172901515608,\n",
       " (447, 'train'): 0.00014691014291235693,\n",
       " (447, 'val'): 0.0002032232953718415,\n",
       " (448, 'train'): 0.0001459809028784986,\n",
       " (448, 'val'): 0.00020325366683580256,\n",
       " (449, 'train'): 0.00014492851699254028,\n",
       " (449, 'val'): 0.00020332303312089708,\n",
       " (450, 'train'): 0.00015343945576912828,\n",
       " (450, 'val'): 0.0002032246061221317,\n",
       " (451, 'train'): 0.00014764879381767027,\n",
       " (451, 'val'): 0.0002032574956063871,\n",
       " (452, 'train'): 0.00015097525384691026,\n",
       " (452, 'val'): 0.0002032229159441259,\n",
       " (453, 'train'): 0.00014598628385337415,\n",
       " (453, 'val'): 0.0002032630835418348,\n",
       " (454, 'train'): 0.00014718390000915087,\n",
       " (454, 'val'): 0.0002032177764232512,\n",
       " (455, 'train'): 0.00015005613332269368,\n",
       " (455, 'val'): 0.00020320261656134217,\n",
       " (456, 'train'): 0.00014803121383819316,\n",
       " (456, 'val'): 0.0002032278829978572,\n",
       " (457, 'train'): 0.00014679027393598248,\n",
       " (457, 'val'): 0.00020325120055565127,\n",
       " (458, 'train'): 0.00015230272051498846,\n",
       " (458, 'val'): 0.00020319637325074937,\n",
       " (459, 'train'): 0.0001462481364055916,\n",
       " (459, 'val'): 0.00020325508106637885,\n",
       " (460, 'train'): 0.00015019710365406894,\n",
       " (460, 'val'): 0.00020320134030448066,\n",
       " (461, 'train'): 0.00014766874826616712,\n",
       " (461, 'val'): 0.00020324926892364466,\n",
       " (462, 'train'): 0.00014829761089964045,\n",
       " (462, 'val'): 0.0002032499415455041,\n",
       " (463, 'train'): 0.0001478340795815543,\n",
       " (463, 'val'): 0.00020327805368988602,\n",
       " (464, 'train'): 0.0001471592932594595,\n",
       " (464, 'val'): 0.00020325173520379595,\n",
       " (465, 'train'): 0.0001495343167334795,\n",
       " (465, 'val'): 0.0002032943518349418,\n",
       " (466, 'train'): 0.00015012628002161228,\n",
       " (466, 'val'): 0.0002034108706370548,\n",
       " (467, 'train'): 0.0001493151886043725,\n",
       " (467, 'val'): 0.00020340776622847275,\n",
       " (468, 'train'): 0.00015026175296279016,\n",
       " (468, 'val'): 0.0002034314977074111,\n",
       " (469, 'train'): 0.00014861717958141257,\n",
       " (469, 'val'): 0.0002033933307285662,\n",
       " (470, 'train'): 0.00014649362613757452,\n",
       " (470, 'val'): 0.00020346505981352594,\n",
       " (471, 'train'): 0.00014749230575506335,\n",
       " (471, 'val'): 0.0002034077144883297,\n",
       " (472, 'train'): 0.000146071702517845,\n",
       " (472, 'val'): 0.000203360751685169,\n",
       " (473, 'train'): 0.00014666902091078186,\n",
       " (473, 'val'): 0.00020337739476451166,\n",
       " (474, 'train'): 0.000148655049054435,\n",
       " (474, 'val'): 0.00020333239808678627,\n",
       " (475, 'train'): 0.00014665562021373598,\n",
       " (475, 'val'): 0.000203312823066005,\n",
       " (476, 'train'): 0.0001472843319384588,\n",
       " (476, 'val'): 0.00020334247016796358,\n",
       " (477, 'train'): 0.00014893032386209125,\n",
       " (477, 'val'): 0.00020345640196292488,\n",
       " (478, 'train'): 0.00015236368765019707,\n",
       " (478, 'val'): 0.00020350093297936297,\n",
       " (479, 'train'): 0.00015078743712769615,\n",
       " (479, 'val'): 0.00020360332672242766,\n",
       " (480, 'train'): 0.00014776134587548399,\n",
       " (480, 'val'): 0.00020345640196292488,\n",
       " (481, 'train'): 0.00014831685823284916,\n",
       " (481, 'val'): 0.0002034660083828149,\n",
       " (482, 'train'): 0.00014796928088698123,\n",
       " (482, 'val'): 0.0002033452813824018,\n",
       " (483, 'train'): 0.00014999457548751875,\n",
       " (483, 'val'): 0.00020335949267502183,\n",
       " (484, 'train'): 0.0001503492627913753,\n",
       " (484, 'val'): 0.00020353442609862045,\n",
       " (485, 'train'): 0.00014959515451833054,\n",
       " (485, 'val'): 0.00020342892794697373,\n",
       " (486, 'train'): 0.00014588050975429791,\n",
       " (486, 'val'): 0.00020332599955576438,\n",
       " (487, 'train'): 0.00014683993584993813,\n",
       " (487, 'val'): 0.00020330890806184875,\n",
       " (488, 'train'): 0.00015026705201577257,\n",
       " (488, 'val'): 0.00020338291371310198,\n",
       " (489, 'train'): 0.0001479610197108101,\n",
       " (489, 'val'): 0.0002033181867941662,\n",
       " (490, 'train'): 0.00014828320126980543,\n",
       " (490, 'val'): 0.00020334121115781641,\n",
       " (491, 'train'): 0.00014940178004541882,\n",
       " (491, 'val'): 0.0002033100463449955,\n",
       " (492, 'train'): 0.0001476929022896069,\n",
       " (492, 'val'): 0.00020331485817829767,\n",
       " (493, 'train'): 0.00014877346068344736,\n",
       " (493, 'val'): 0.00020337506645807512,\n",
       " (494, 'train'): 0.0001506893766216106,\n",
       " (494, 'val'): 0.0002033179108467367,\n",
       " (495, 'train'): 0.00014682186129330485,\n",
       " (495, 'val'): 0.00020323945554318252,\n",
       " (496, 'train'): 0.0001501177946381547,\n",
       " (496, 'val'): 0.0002033109431741414,\n",
       " (497, 'train'): 0.0001493935188692477,\n",
       " (497, 'val'): 0.00020333967620023974,\n",
       " (498, 'train'): 0.00015141436812916287,\n",
       " (498, 'val'): 0.0002033535080651442,\n",
       " (499, 'train'): 0.00014704446032367372,\n",
       " (499, 'val'): 0.00020330235431039774}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old code for autoencoders\n",
    "# for epoch in range(epochs):\n",
    "#     # ä¸éœ€è¦labelï¼Œæ‰€ä»¥ç”¨ä¸€ä¸ªå ä½ç¬¦\"_\"ä»£æ›¿\n",
    "#     for batchidx, (x, _) in enumerate(X_allDataLoader):\n",
    "#         x.requires_grad_(True)\n",
    "#         # encode and decode \n",
    "#         output = model(x)\n",
    "#         # compute loss\n",
    "#         print(output.shape)\n",
    "#         loss = loss_function(output, x)      \n",
    "#         # update\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "           \n",
    "#     loss_train[epoch,0] = loss.item()  \n",
    "#     print('Epoch: %04d, Training loss=%.8f' %\n",
    "#           (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'saved/models/ae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_batch = model(X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0208, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(recon_batch,X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5086, 0.0377, 0.1248,  ..., 0.2024, 0.1449, 0.4762],\n",
       "        [0.5402, 0.0980, 0.1292,  ..., 0.3138, 0.2818, 0.2321],\n",
       "        [0.0515, 0.4554, 0.0792,  ..., 0.1940, 0.4289, 0.1500],\n",
       "        ...,\n",
       "        [0.4740, 0.0804, 0.3108,  ..., 0.1630, 0.2161, 0.1823],\n",
       "        [0.1886, 0.1683, 0.0924,  ..., 0.2058, 0.2563, 0.3026],\n",
       "        [0.6708, 0.0478, 0.1876,  ..., 0.2436, 0.1315, 0.4909]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7359, 0.0868, 0.4233,  ..., 0.2414, 0.1303, 0.6761],\n",
       "        [0.5435, 0.0496, 0.0739,  ..., 0.1036, 0.2022, 0.3589],\n",
       "        [0.1336, 0.4184, 0.0618,  ..., 0.1218, 0.3170, 0.3572],\n",
       "        ...,\n",
       "        [0.3869, 0.0693, 0.4479,  ..., 0.2760, 0.2085, 0.0863],\n",
       "        [0.1415, 0.0232, 0.0663,  ..., 0.2076, 0.3566, 0.3610],\n",
       "        [0.6655, 0.0682, 0.3321,  ..., 0.2687, 0.1723, 0.2549]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_batch.cpu().detach().numpy().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = model.encode(X_trainTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = train_embeddings.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.001)\n",
    "clf.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "RFrg = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "RFrg.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeature = model.encode(X_testTensor)\n",
    "lasso = clf.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfresult = RFrg.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(lasso,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(rfresult,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=\"label\")\n",
    "plt.legend()\n",
    "plt.savefig(\"saved/figures/tsne_ae_gdsc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data type conversion\n",
    "B_feature = torch.FloatTensor(feature).to(device)\n",
    "y = torch.FloatTensor(Y_train.values).to(device)\n",
    "# construct TensorDataset\n",
    "b_data = TensorDataset(B_feature, y)\n",
    "trainDataLoader2 = DataLoader(dataset=b_data, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization DNN model\n",
    "\n",
    "predictor = DNN(128, dim_dnn_out).to(device)\n",
    "optimizer = optim.Adam(predictor.parameters(), lr=1e-3,betas=(0.9,0.99))\n",
    "#loss1-softmax\n",
    "loss_func = nn.MSELoss().to(device)\n",
    "#loss2-sigmoid\n",
    "#loss_func = nn.BCELoss()\n",
    "#loss3-sigmoid\n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = torch.nn.MSELoss(size_average=True)\n",
    "#criterion = torch.nn.BCELoss(size_average=True) # Defined loss function\n",
    "#optimizer = optim.Adm(model.parameters(), lr=0.01) # Defined optimizer\n",
    "loss_train = np.zeros((epochs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ',epoch)\n",
    "    for step,(batch_x,batch_y) in enumerate(trainDataLoader2):\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "        # predict label\n",
    "        output = predictor(b_x)\n",
    "        # b_y=F.sigmoid(b_y) \n",
    "        \n",
    "        #print\n",
    "        #print(output)\n",
    "        #print(b_y)\n",
    "        # compute loss\n",
    "        loss = loss_func(output,b_y)\n",
    "        #loss = criterion(output, b_y)\n",
    "        \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item())) \n",
    "\n",
    "# Save model\n",
    "torch.save(predictor.state_dict(), 'saved/models/DNN_GDSC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict = predictor(testFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
