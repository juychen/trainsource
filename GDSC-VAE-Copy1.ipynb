{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "#import scipy.io as sio\n",
    "\n",
    "from models import VAE\n",
    "from models import DNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "epochs = 500 #200,500,1000  \n",
    "dim_au_in = 20049\n",
    "dim_au_out = 512 #8, 16, 32, 64, 128, 256,512\n",
    "dim_dnn_in = dim_au_out\n",
    "dim_dnn_out=1\n",
    "select_drug = 'Cisplatin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r=pd.read_csv('data/GDSCexpression.csv',index_col=0)\n",
    "label_r=pd.read_csv('data/GDSClabel.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_r=label_r.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Erlotinib</th>\n",
       "      <th>AICAR</th>\n",
       "      <th>Camptothecin</th>\n",
       "      <th>Vinblastine</th>\n",
       "      <th>Cisplatin</th>\n",
       "      <th>Cytarabine</th>\n",
       "      <th>Docetaxel</th>\n",
       "      <th>Methotrexate</th>\n",
       "      <th>ATRA</th>\n",
       "      <th>Gefitinib</th>\n",
       "      <th>...</th>\n",
       "      <th>CMK</th>\n",
       "      <th>Pyrimethamine</th>\n",
       "      <th>JW-7-52-1</th>\n",
       "      <th>A-443654</th>\n",
       "      <th>GW843682X</th>\n",
       "      <th>MS-275</th>\n",
       "      <th>Parthenolide</th>\n",
       "      <th>MG-132</th>\n",
       "      <th>KIN001-135</th>\n",
       "      <th>TGX221</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BxPC-3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159708</td>\n",
       "      <td>0.188393</td>\n",
       "      <td>0.436127</td>\n",
       "      <td>0.154274</td>\n",
       "      <td>0.113764</td>\n",
       "      <td>0.249987</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMOE-2</th>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.318206</td>\n",
       "      <td>0.281740</td>\n",
       "      <td>0.063808</td>\n",
       "      <td>0.140681</td>\n",
       "      <td>0.101530</td>\n",
       "      <td>0.191210</td>\n",
       "      <td>0.031229</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022662</td>\n",
       "      <td>0.025217</td>\n",
       "      <td>0.270791</td>\n",
       "      <td>0.095608</td>\n",
       "      <td>0.139914</td>\n",
       "      <td>0.171747</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.000956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFM-223</th>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.334087</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.361438</td>\n",
       "      <td>0.051983</td>\n",
       "      <td>0.186659</td>\n",
       "      <td>0.228797</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038997</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.002185</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.035206</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUGC-3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095099</td>\n",
       "      <td>0.416792</td>\n",
       "      <td>0.422631</td>\n",
       "      <td>0.266409</td>\n",
       "      <td>0.237811</td>\n",
       "      <td>0.234952</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>0.153002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OC-314</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>0.479915</td>\n",
       "      <td>0.390145</td>\n",
       "      <td>0.067738</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.249668</td>\n",
       "      <td>0.110418</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.020309</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KP-N-S19s</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC-3</th>\n",
       "      <td>0.003515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048227</td>\n",
       "      <td>0.146476</td>\n",
       "      <td>0.400087</td>\n",
       "      <td>0.205247</td>\n",
       "      <td>0.147090</td>\n",
       "      <td>0.298803</td>\n",
       "      <td>0.023796</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.069181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Panc 08.13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.196279</td>\n",
       "      <td>0.180653</td>\n",
       "      <td>0.090963</td>\n",
       "      <td>0.093197</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EKVX</th>\n",
       "      <td>0.177238</td>\n",
       "      <td>0.078395</td>\n",
       "      <td>0.078938</td>\n",
       "      <td>0.072213</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>0.095642</td>\n",
       "      <td>0.111598</td>\n",
       "      <td>0.126364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.243635</td>\n",
       "      <td>0.079375</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.177759</td>\n",
       "      <td>0.014843</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>0.007216</td>\n",
       "      <td>0.063911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DMS-114</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.127677</td>\n",
       "      <td>0.168317</td>\n",
       "      <td>0.281940</td>\n",
       "      <td>0.092074</td>\n",
       "      <td>0.191972</td>\n",
       "      <td>0.185916</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.009329</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.043920</td>\n",
       "      <td>0.285969</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.174841</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.080451</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.028733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows Ã— 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Erlotinib     AICAR  Camptothecin  Vinblastine  Cisplatin  \\\n",
       "BxPC-3       1.000000  0.159708      0.188393     0.436127   0.154274   \n",
       "KMOE-2       0.002108  0.339814      0.318206     0.281740   0.063808   \n",
       "MFM-223      0.002148  0.334087      0.122671     0.361438   0.051983   \n",
       "NUGC-3       1.000000  0.095099      0.416792     0.422631   0.266409   \n",
       "OC-314       1.000000  0.012740      0.479915     0.390145   0.067738   \n",
       "...               ...       ...           ...          ...        ...   \n",
       "KP-N-S19s    1.000000  1.000000      1.000000     1.000000   1.000000   \n",
       "BC-3         0.003515  1.000000      1.000000     1.000000   1.000000   \n",
       "Panc 08.13   1.000000  0.152818      0.196279     0.180653   0.090963   \n",
       "EKVX         0.177238  0.078395      0.078938     0.072213   0.007225   \n",
       "DMS-114      0.000322  0.127677      0.168317     0.281940   0.092074   \n",
       "\n",
       "            Cytarabine  Docetaxel  Methotrexate      ATRA  Gefitinib  ...  \\\n",
       "BxPC-3        0.113764   0.249987      0.015324  0.015324   0.020293  ...   \n",
       "KMOE-2        0.140681   0.101530      0.191210  0.031229   0.007419  ...   \n",
       "MFM-223       0.186659   0.228797      0.003639  0.023043   0.003639  ...   \n",
       "NUGC-3        0.237811   0.234952      0.011558  0.028838   0.153002  ...   \n",
       "OC-314        0.125869   0.249668      0.110418  0.002214   0.020309  ...   \n",
       "...                ...        ...           ...       ...        ...  ...   \n",
       "KP-N-S19s     1.000000   1.000000      1.000000  1.000000   1.000000  ...   \n",
       "BC-3          1.000000   1.000000      1.000000  1.000000   1.000000  ...   \n",
       "Panc 08.13    0.093197   0.097267      0.018269  0.010237   0.009405  ...   \n",
       "EKVX          0.003470   0.129184      0.095642  0.111598   0.126364  ...   \n",
       "DMS-114       0.191972   0.185916      0.007135  0.009329   0.020822  ...   \n",
       "\n",
       "                 CMK  Pyrimethamine  JW-7-52-1  A-443654  GW843682X    MS-275  \\\n",
       "BxPC-3      1.000000       1.000000   1.000000  1.000000   1.000000  1.000000   \n",
       "KMOE-2      0.022662       0.025217   0.270791  0.095608   0.139914  0.171747   \n",
       "MFM-223     0.038997       0.006167   0.002185  0.102592   0.020338  0.010874   \n",
       "NUGC-3      1.000000       1.000000   1.000000  1.000000   1.000000  1.000000   \n",
       "OC-314      1.000000       1.000000   1.000000  1.000000   1.000000  1.000000   \n",
       "...              ...            ...        ...       ...        ...       ...   \n",
       "KP-N-S19s   1.000000       1.000000   1.000000  1.000000   1.000000  1.000000   \n",
       "BC-3        0.048227       0.146476   0.400087  0.205247   0.147090  0.298803   \n",
       "Panc 08.13  1.000000       1.000000   1.000000  1.000000   1.000000  1.000000   \n",
       "EKVX        0.020472       0.044047   0.243635  0.079375   0.019859  0.177759   \n",
       "DMS-114     0.002137       0.043920   0.285969  0.075195   0.174841  0.008383   \n",
       "\n",
       "            Parthenolide    MG-132  KIN001-135    TGX221  \n",
       "BxPC-3          1.000000  1.000000    1.000000  1.000000  \n",
       "KMOE-2          0.026670  0.013904    0.002116  0.000956  \n",
       "MFM-223         0.000970  0.035206    0.007051  0.000323  \n",
       "NUGC-3          1.000000  1.000000    1.000000  1.000000  \n",
       "OC-314          1.000000  1.000000    1.000000  1.000000  \n",
       "...                  ...       ...         ...       ...  \n",
       "KP-N-S19s       1.000000  1.000000    1.000000  1.000000  \n",
       "BC-3            0.023796  0.017219    0.002065  0.069181  \n",
       "Panc 08.13      1.000000  1.000000    1.000000  1.000000  \n",
       "EKVX            0.014843  0.114431    0.007216  0.063911  \n",
       "DMS-114         0.002135  0.080451    0.002135  0.028733  \n",
       "\n",
       "[789 rows x 139 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r.loc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idx = label_r.loc[:,select_drug]!=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your is gene-cell, mine is cell-gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(data_r.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_r[selected_idx]\n",
    "label = label_r.loc[selected_idx,select_drug]\n",
    "scaler = preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "9.829117799766939e-18\n"
     ]
    }
   ],
   "source": [
    "print(np.std(data))\n",
    "print(np.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 11833)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789, 139)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 11833)\n",
      "(675,)\n",
      "(540, 11833) (540,)\n",
      "(135, 11833) (135,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.32064566525343\n",
      "-16.913044973607605\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add all data to AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = torch.FloatTensor(X_train).to(device)\n",
    "testData = torch.FloatTensor(X_test).to(device)\n",
    "y = torch.FloatTensor(Y_train.values).to(device)\n",
    "allData = torch.FloatTensor(data).to(device)\n",
    "\n",
    "# construct TensorDataset\n",
    "train_dataset = TensorDataset(trainData, trainData)\n",
    "test_dataset = TensorDataset(testData, testData)\n",
    "all_dataset = TensorDataset(allData, allData)\n",
    "\n",
    "trainDataLoader1 = DataLoader(dataset=train_dataset, batch_size=200, shuffle=False)\n",
    "trainDataLoaderall = DataLoader(dataset=all_dataset, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = trainDataLoaderall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(dim_au_in=data_r.shape[1],dim_au_out=128)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "reconstruction_function = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Training loss=435.66418457\n",
      "Epoch: 0002, Training loss=227.34948730\n",
      "Epoch: 0003, Training loss=146.82661438\n",
      "Epoch: 0004, Training loss=103.66944885\n",
      "Epoch: 0005, Training loss=76.37157440\n",
      "Epoch: 0006, Training loss=58.99320984\n",
      "Epoch: 0007, Training loss=46.76211929\n",
      "Epoch: 0008, Training loss=37.52946472\n",
      "Epoch: 0009, Training loss=30.51655579\n",
      "Epoch: 0010, Training loss=25.01723671\n",
      "Epoch: 0011, Training loss=21.04143524\n",
      "Epoch: 0012, Training loss=18.05273056\n",
      "Epoch: 0013, Training loss=15.68579006\n",
      "Epoch: 0014, Training loss=13.81253529\n",
      "Epoch: 0015, Training loss=12.39556694\n",
      "Epoch: 0016, Training loss=11.31699657\n",
      "Epoch: 0017, Training loss=10.56795406\n",
      "Epoch: 0018, Training loss=10.18374920\n",
      "Epoch: 0019, Training loss=9.90042686\n",
      "Epoch: 0020, Training loss=9.66532135\n",
      "Epoch: 0021, Training loss=9.46712494\n",
      "Epoch: 0022, Training loss=9.02877998\n",
      "Epoch: 0023, Training loss=8.32121658\n",
      "Epoch: 0024, Training loss=8.07711124\n",
      "Epoch: 0025, Training loss=11.30444431\n",
      "Epoch: 0026, Training loss=21.79204178\n",
      "Epoch: 0027, Training loss=32.33831787\n",
      "Epoch: 0028, Training loss=26.06681061\n",
      "Epoch: 0029, Training loss=13.12881279\n",
      "Epoch: 0030, Training loss=9.71598911\n",
      "Epoch: 0031, Training loss=11.77802753\n",
      "Epoch: 0032, Training loss=9.21528149\n",
      "Epoch: 0033, Training loss=6.62591410\n",
      "Epoch: 0034, Training loss=6.83529139\n",
      "Epoch: 0035, Training loss=6.46852875\n",
      "Epoch: 0036, Training loss=5.77392244\n",
      "Epoch: 0037, Training loss=5.62737751\n",
      "Epoch: 0038, Training loss=5.66911650\n",
      "Epoch: 0039, Training loss=5.47363758\n",
      "Epoch: 0040, Training loss=5.66086674\n",
      "Epoch: 0041, Training loss=6.16232872\n",
      "Epoch: 0042, Training loss=6.77631378\n",
      "Epoch: 0043, Training loss=7.44433832\n",
      "Epoch: 0044, Training loss=8.08696175\n",
      "Epoch: 0045, Training loss=8.53915596\n",
      "Epoch: 0046, Training loss=8.80547714\n",
      "Epoch: 0047, Training loss=8.47091484\n",
      "Epoch: 0048, Training loss=7.96040154\n",
      "Epoch: 0049, Training loss=7.28234959\n",
      "Epoch: 0050, Training loss=6.52744818\n",
      "Epoch: 0051, Training loss=5.87142229\n",
      "Epoch: 0052, Training loss=5.39951754\n",
      "Epoch: 0053, Training loss=4.95928860\n",
      "Epoch: 0054, Training loss=4.72540092\n",
      "Epoch: 0055, Training loss=4.67687464\n",
      "Epoch: 0056, Training loss=5.05550814\n",
      "Epoch: 0057, Training loss=5.76439571\n",
      "Epoch: 0058, Training loss=6.66834784\n",
      "Epoch: 0059, Training loss=7.25626707\n",
      "Epoch: 0060, Training loss=7.47880411\n",
      "Epoch: 0061, Training loss=7.62269163\n",
      "Epoch: 0062, Training loss=7.23071289\n",
      "Epoch: 0063, Training loss=6.46413803\n",
      "Epoch: 0064, Training loss=5.57052612\n",
      "Epoch: 0065, Training loss=5.09119844\n",
      "Epoch: 0066, Training loss=5.09019899\n",
      "Epoch: 0067, Training loss=5.80228710\n",
      "Epoch: 0068, Training loss=7.24597359\n",
      "Epoch: 0069, Training loss=8.50168419\n",
      "Epoch: 0070, Training loss=9.16467667\n",
      "Epoch: 0071, Training loss=9.01824188\n",
      "Epoch: 0072, Training loss=7.96029043\n",
      "Epoch: 0073, Training loss=6.65420198\n",
      "Epoch: 0074, Training loss=5.65156412\n",
      "Epoch: 0075, Training loss=5.04664803\n",
      "Epoch: 0076, Training loss=4.57786655\n",
      "Epoch: 0077, Training loss=4.05293894\n",
      "Epoch: 0078, Training loss=3.49219179\n",
      "Epoch: 0079, Training loss=2.98114014\n",
      "Epoch: 0080, Training loss=2.59682178\n",
      "Epoch: 0081, Training loss=2.33646274\n",
      "Epoch: 0082, Training loss=2.15615988\n",
      "Epoch: 0083, Training loss=2.02559257\n",
      "Epoch: 0084, Training loss=1.94422150\n",
      "Epoch: 0085, Training loss=1.96532512\n",
      "Epoch: 0086, Training loss=2.12140465\n",
      "Epoch: 0087, Training loss=2.42802429\n",
      "Epoch: 0088, Training loss=2.95753551\n",
      "Epoch: 0089, Training loss=3.84381533\n",
      "Epoch: 0090, Training loss=5.12791538\n",
      "Epoch: 0091, Training loss=6.94945478\n",
      "Epoch: 0092, Training loss=9.30353165\n",
      "Epoch: 0093, Training loss=11.65749550\n",
      "Epoch: 0094, Training loss=12.03074741\n",
      "Epoch: 0095, Training loss=10.98400211\n",
      "Epoch: 0096, Training loss=8.12807655\n",
      "Epoch: 0097, Training loss=5.61347246\n",
      "Epoch: 0098, Training loss=4.51106071\n",
      "Epoch: 0099, Training loss=4.48293114\n",
      "Epoch: 0100, Training loss=4.68017626\n",
      "Epoch: 0101, Training loss=4.41856909\n",
      "Epoch: 0102, Training loss=4.13077831\n",
      "Epoch: 0103, Training loss=3.85201645\n",
      "Epoch: 0104, Training loss=3.78661132\n",
      "Epoch: 0105, Training loss=3.88090801\n",
      "Epoch: 0106, Training loss=4.13085890\n",
      "Epoch: 0107, Training loss=4.49708843\n",
      "Epoch: 0108, Training loss=5.07303381\n",
      "Epoch: 0109, Training loss=5.76083899\n",
      "Epoch: 0110, Training loss=6.80031490\n",
      "Epoch: 0111, Training loss=7.86998415\n",
      "Epoch: 0112, Training loss=8.78399849\n",
      "Epoch: 0113, Training loss=9.18906593\n",
      "Epoch: 0114, Training loss=9.13494301\n",
      "Epoch: 0115, Training loss=8.17620468\n",
      "Epoch: 0116, Training loss=7.14894772\n",
      "Epoch: 0117, Training loss=6.05991888\n",
      "Epoch: 0118, Training loss=5.05329609\n",
      "Epoch: 0119, Training loss=4.58319283\n",
      "Epoch: 0120, Training loss=4.33945131\n",
      "Epoch: 0121, Training loss=4.19568634\n",
      "Epoch: 0122, Training loss=4.17467213\n",
      "Epoch: 0123, Training loss=4.04163551\n",
      "Epoch: 0124, Training loss=4.02535152\n",
      "Epoch: 0125, Training loss=3.84606314\n",
      "Epoch: 0126, Training loss=3.72428584\n",
      "Epoch: 0127, Training loss=3.61060166\n",
      "Epoch: 0128, Training loss=3.40047264\n",
      "Epoch: 0129, Training loss=3.05322552\n",
      "Epoch: 0130, Training loss=2.73067236\n",
      "Epoch: 0131, Training loss=2.42059875\n",
      "Epoch: 0132, Training loss=2.19683313\n",
      "Epoch: 0133, Training loss=2.01788187\n",
      "Epoch: 0134, Training loss=1.93675017\n",
      "Epoch: 0135, Training loss=1.92206204\n",
      "Epoch: 0136, Training loss=1.93463707\n",
      "Epoch: 0137, Training loss=2.00641513\n",
      "Epoch: 0138, Training loss=2.16685629\n",
      "Epoch: 0139, Training loss=2.53714871\n",
      "Epoch: 0140, Training loss=3.18899775\n",
      "Epoch: 0141, Training loss=4.37855291\n",
      "Epoch: 0142, Training loss=6.22613621\n",
      "Epoch: 0143, Training loss=7.95301342\n",
      "Epoch: 0144, Training loss=9.20828152\n",
      "Epoch: 0145, Training loss=9.51833439\n",
      "Epoch: 0146, Training loss=8.83745861\n",
      "Epoch: 0147, Training loss=7.57994604\n",
      "Epoch: 0148, Training loss=6.14464664\n",
      "Epoch: 0149, Training loss=4.70953083\n",
      "Epoch: 0150, Training loss=3.70142531\n",
      "Epoch: 0151, Training loss=2.95747066\n",
      "Epoch: 0152, Training loss=2.53749847\n",
      "Epoch: 0153, Training loss=2.23759031\n",
      "Epoch: 0154, Training loss=2.04585910\n",
      "Epoch: 0155, Training loss=1.94248152\n",
      "Epoch: 0156, Training loss=1.84660959\n",
      "Epoch: 0157, Training loss=1.77635074\n",
      "Epoch: 0158, Training loss=1.74920511\n",
      "Epoch: 0159, Training loss=1.72733879\n",
      "Epoch: 0160, Training loss=1.70283401\n",
      "Epoch: 0161, Training loss=1.71216965\n",
      "Epoch: 0162, Training loss=1.71575499\n",
      "Epoch: 0163, Training loss=1.74842989\n",
      "Epoch: 0164, Training loss=1.77767563\n",
      "Epoch: 0165, Training loss=1.82954848\n",
      "Epoch: 0166, Training loss=1.89508927\n",
      "Epoch: 0167, Training loss=2.01031065\n",
      "Epoch: 0168, Training loss=2.17128778\n",
      "Epoch: 0169, Training loss=2.32295251\n",
      "Epoch: 0170, Training loss=2.52817321\n",
      "Epoch: 0171, Training loss=2.76264381\n",
      "Epoch: 0172, Training loss=3.09505773\n",
      "Epoch: 0173, Training loss=3.56394267\n",
      "Epoch: 0174, Training loss=4.31115103\n",
      "Epoch: 0175, Training loss=5.43090820\n",
      "Epoch: 0176, Training loss=6.96570873\n",
      "Epoch: 0177, Training loss=8.68492317\n",
      "Epoch: 0178, Training loss=10.67693520\n",
      "Epoch: 0179, Training loss=12.77394676\n",
      "Epoch: 0180, Training loss=13.00184822\n",
      "Epoch: 0181, Training loss=11.77955914\n",
      "Epoch: 0182, Training loss=9.36779308\n",
      "Epoch: 0183, Training loss=7.44076872\n",
      "Epoch: 0184, Training loss=6.63704634\n",
      "Epoch: 0185, Training loss=6.55214691\n",
      "Epoch: 0186, Training loss=6.73271656\n",
      "Epoch: 0187, Training loss=6.06850386\n",
      "Epoch: 0188, Training loss=4.76604462\n",
      "Epoch: 0189, Training loss=3.83572769\n",
      "Epoch: 0190, Training loss=3.25124121\n",
      "Epoch: 0191, Training loss=2.91504407\n",
      "Epoch: 0192, Training loss=2.60672712\n",
      "Epoch: 0193, Training loss=2.30414200\n",
      "Epoch: 0194, Training loss=2.08932447\n",
      "Epoch: 0195, Training loss=1.98079395\n",
      "Epoch: 0196, Training loss=1.90306735\n",
      "Epoch: 0197, Training loss=1.85813665\n",
      "Epoch: 0198, Training loss=1.83857179\n",
      "Epoch: 0199, Training loss=1.85353231\n",
      "Epoch: 0200, Training loss=1.91331410\n",
      "Epoch: 0201, Training loss=1.99479747\n",
      "Epoch: 0202, Training loss=2.10291553\n",
      "Epoch: 0203, Training loss=2.25692892\n",
      "Epoch: 0204, Training loss=2.48170757\n",
      "Epoch: 0205, Training loss=2.76205921\n",
      "Epoch: 0206, Training loss=3.13065529\n",
      "Epoch: 0207, Training loss=3.54257250\n",
      "Epoch: 0208, Training loss=3.99422073\n",
      "Epoch: 0209, Training loss=4.37381029\n",
      "Epoch: 0210, Training loss=4.65745735\n",
      "Epoch: 0211, Training loss=4.70507717\n",
      "Epoch: 0212, Training loss=4.60062075\n",
      "Epoch: 0213, Training loss=4.24593782\n",
      "Epoch: 0214, Training loss=3.76131654\n",
      "Epoch: 0215, Training loss=3.22231698\n",
      "Epoch: 0216, Training loss=2.82337642\n",
      "Epoch: 0217, Training loss=2.58921838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0218, Training loss=2.49807048\n",
      "Epoch: 0219, Training loss=2.53607154\n",
      "Epoch: 0220, Training loss=2.68219447\n",
      "Epoch: 0221, Training loss=2.86499834\n",
      "Epoch: 0222, Training loss=3.03231812\n",
      "Epoch: 0223, Training loss=3.16516972\n",
      "Epoch: 0224, Training loss=3.21358418\n",
      "Epoch: 0225, Training loss=3.21840143\n",
      "Epoch: 0226, Training loss=3.16964674\n",
      "Epoch: 0227, Training loss=3.03279710\n",
      "Epoch: 0228, Training loss=2.88505983\n",
      "Epoch: 0229, Training loss=2.73652530\n",
      "Epoch: 0230, Training loss=2.58875680\n",
      "Epoch: 0231, Training loss=2.49734855\n",
      "Epoch: 0232, Training loss=2.45696783\n",
      "Epoch: 0233, Training loss=2.50522828\n",
      "Epoch: 0234, Training loss=2.65311718\n",
      "Epoch: 0235, Training loss=2.90040326\n",
      "Epoch: 0236, Training loss=3.22361565\n",
      "Epoch: 0237, Training loss=3.67387938\n",
      "Epoch: 0238, Training loss=4.31257153\n",
      "Epoch: 0239, Training loss=5.11966133\n",
      "Epoch: 0240, Training loss=6.00149584\n",
      "Epoch: 0241, Training loss=6.77064228\n",
      "Epoch: 0242, Training loss=7.15197420\n",
      "Epoch: 0243, Training loss=7.16431236\n",
      "Epoch: 0244, Training loss=6.49495935\n",
      "Epoch: 0245, Training loss=5.47174644\n",
      "Epoch: 0246, Training loss=4.34278154\n",
      "Epoch: 0247, Training loss=3.57875443\n",
      "Epoch: 0248, Training loss=3.28018379\n",
      "Epoch: 0249, Training loss=3.49139142\n",
      "Epoch: 0250, Training loss=3.94937396\n",
      "Epoch: 0251, Training loss=4.42845249\n",
      "Epoch: 0252, Training loss=4.67180777\n",
      "Epoch: 0253, Training loss=4.60729742\n",
      "Epoch: 0254, Training loss=4.41747761\n",
      "Epoch: 0255, Training loss=4.06806326\n",
      "Epoch: 0256, Training loss=3.70780945\n",
      "Epoch: 0257, Training loss=3.44704962\n",
      "Epoch: 0258, Training loss=3.23219776\n",
      "Epoch: 0259, Training loss=3.16065264\n",
      "Epoch: 0260, Training loss=3.24608684\n",
      "Epoch: 0261, Training loss=3.43907547\n",
      "Epoch: 0262, Training loss=3.74582744\n",
      "Epoch: 0263, Training loss=4.06263542\n",
      "Epoch: 0264, Training loss=4.44909430\n",
      "Epoch: 0265, Training loss=4.84465981\n",
      "Epoch: 0266, Training loss=5.21263170\n",
      "Epoch: 0267, Training loss=5.48817730\n",
      "Epoch: 0268, Training loss=5.60767412\n",
      "Epoch: 0269, Training loss=5.64991617\n",
      "Epoch: 0270, Training loss=5.61254168\n",
      "Epoch: 0271, Training loss=5.32970715\n",
      "Epoch: 0272, Training loss=5.02125025\n",
      "Epoch: 0273, Training loss=4.78299141\n",
      "Epoch: 0274, Training loss=4.69185495\n",
      "Epoch: 0275, Training loss=4.67632961\n",
      "Epoch: 0276, Training loss=4.54306221\n",
      "Epoch: 0277, Training loss=4.34720516\n",
      "Epoch: 0278, Training loss=4.02839231\n",
      "Epoch: 0279, Training loss=3.74743462\n",
      "Epoch: 0280, Training loss=3.47320223\n",
      "Epoch: 0281, Training loss=3.28499389\n",
      "Epoch: 0282, Training loss=3.20855927\n",
      "Epoch: 0283, Training loss=3.22642040\n",
      "Epoch: 0284, Training loss=3.32583070\n",
      "Epoch: 0285, Training loss=3.42163444\n",
      "Epoch: 0286, Training loss=3.42953539\n",
      "Epoch: 0287, Training loss=3.36813784\n",
      "Epoch: 0288, Training loss=3.25161099\n",
      "Epoch: 0289, Training loss=3.10065651\n",
      "Epoch: 0290, Training loss=2.89741969\n",
      "Epoch: 0291, Training loss=2.70345998\n",
      "Epoch: 0292, Training loss=2.53645873\n",
      "Epoch: 0293, Training loss=2.42163277\n",
      "Epoch: 0294, Training loss=2.35200548\n",
      "Epoch: 0295, Training loss=2.36756444\n",
      "Epoch: 0296, Training loss=2.47079301\n",
      "Epoch: 0297, Training loss=2.63236499\n",
      "Epoch: 0298, Training loss=2.83942461\n",
      "Epoch: 0299, Training loss=3.12571025\n",
      "Epoch: 0300, Training loss=3.48241210\n",
      "Epoch: 0301, Training loss=3.94471550\n",
      "Epoch: 0302, Training loss=4.41740656\n",
      "Epoch: 0303, Training loss=4.85570478\n",
      "Epoch: 0304, Training loss=5.17951488\n",
      "Epoch: 0305, Training loss=5.44006824\n",
      "Epoch: 0306, Training loss=5.50477648\n",
      "Epoch: 0307, Training loss=5.32567072\n",
      "Epoch: 0308, Training loss=4.96036196\n",
      "Epoch: 0309, Training loss=4.43920422\n",
      "Epoch: 0310, Training loss=3.86498356\n",
      "Epoch: 0311, Training loss=3.41376472\n",
      "Epoch: 0312, Training loss=3.15065289\n",
      "Epoch: 0313, Training loss=3.07159185\n",
      "Epoch: 0314, Training loss=3.20708418\n",
      "Epoch: 0315, Training loss=3.41877866\n",
      "Epoch: 0316, Training loss=3.63185644\n",
      "Epoch: 0317, Training loss=3.72896242\n",
      "Epoch: 0318, Training loss=3.71454072\n",
      "Epoch: 0319, Training loss=3.57688355\n",
      "Epoch: 0320, Training loss=3.37004089\n",
      "Epoch: 0321, Training loss=3.14844847\n",
      "Epoch: 0322, Training loss=2.92662740\n",
      "Epoch: 0323, Training loss=2.75225639\n",
      "Epoch: 0324, Training loss=2.61968279\n",
      "Epoch: 0325, Training loss=2.58257484\n",
      "Epoch: 0326, Training loss=2.62398529\n",
      "Epoch: 0327, Training loss=2.75935507\n",
      "Epoch: 0328, Training loss=2.99345875\n",
      "Epoch: 0329, Training loss=3.30297732\n",
      "Epoch: 0330, Training loss=3.55537367\n",
      "Epoch: 0331, Training loss=3.71836329\n",
      "Epoch: 0332, Training loss=3.85384250\n",
      "Epoch: 0333, Training loss=3.88698459\n",
      "Epoch: 0334, Training loss=3.78463936\n",
      "Epoch: 0335, Training loss=3.58546925\n",
      "Epoch: 0336, Training loss=3.34066415\n",
      "Epoch: 0337, Training loss=3.05195475\n",
      "Epoch: 0338, Training loss=2.87558603\n",
      "Epoch: 0339, Training loss=2.76711035\n",
      "Epoch: 0340, Training loss=2.83728647\n",
      "Epoch: 0341, Training loss=2.99943137\n",
      "Epoch: 0342, Training loss=3.28260946\n",
      "Epoch: 0343, Training loss=3.74548507\n",
      "Epoch: 0344, Training loss=4.04890108\n",
      "Epoch: 0345, Training loss=4.40030813\n",
      "Epoch: 0346, Training loss=4.56308556\n",
      "Epoch: 0347, Training loss=4.57608843\n",
      "Epoch: 0348, Training loss=4.48528481\n",
      "Epoch: 0349, Training loss=4.20784950\n",
      "Epoch: 0350, Training loss=3.81005955\n",
      "Epoch: 0351, Training loss=3.46267271\n",
      "Epoch: 0352, Training loss=3.06700850\n",
      "Epoch: 0353, Training loss=2.78628826\n",
      "Epoch: 0354, Training loss=2.54192853\n",
      "Epoch: 0355, Training loss=2.44874334\n",
      "Epoch: 0356, Training loss=2.40276504\n",
      "Epoch: 0357, Training loss=2.43870974\n",
      "Epoch: 0358, Training loss=2.47278214\n",
      "Epoch: 0359, Training loss=2.50297999\n",
      "Epoch: 0360, Training loss=2.57183886\n",
      "Epoch: 0361, Training loss=2.69403100\n",
      "Epoch: 0362, Training loss=2.90309644\n",
      "Epoch: 0363, Training loss=3.17510009\n",
      "Epoch: 0364, Training loss=3.61778641\n",
      "Epoch: 0365, Training loss=4.07260609\n",
      "Epoch: 0366, Training loss=4.64376783\n",
      "Epoch: 0367, Training loss=5.35953903\n",
      "Epoch: 0368, Training loss=6.15259600\n",
      "Epoch: 0369, Training loss=7.02493238\n",
      "Epoch: 0370, Training loss=7.89986420\n",
      "Epoch: 0371, Training loss=8.52281857\n",
      "Epoch: 0372, Training loss=8.86234283\n",
      "Epoch: 0373, Training loss=8.71754456\n",
      "Epoch: 0374, Training loss=8.56634998\n",
      "Epoch: 0375, Training loss=8.19130516\n",
      "Epoch: 0376, Training loss=7.76462030\n",
      "Epoch: 0377, Training loss=7.93859196\n",
      "Epoch: 0378, Training loss=8.17106342\n",
      "Epoch: 0379, Training loss=8.70282555\n",
      "Epoch: 0380, Training loss=8.74081039\n",
      "Epoch: 0381, Training loss=7.52912378\n",
      "Epoch: 0382, Training loss=6.27645874\n",
      "Epoch: 0383, Training loss=5.19842196\n",
      "Epoch: 0384, Training loss=4.83581686\n",
      "Epoch: 0385, Training loss=4.78704500\n",
      "Epoch: 0386, Training loss=4.43917322\n",
      "Epoch: 0387, Training loss=4.16254187\n",
      "Epoch: 0388, Training loss=4.10410833\n",
      "Epoch: 0389, Training loss=4.37416172\n",
      "Epoch: 0390, Training loss=4.42184067\n",
      "Epoch: 0391, Training loss=4.37677383\n",
      "Epoch: 0392, Training loss=4.26797819\n",
      "Epoch: 0393, Training loss=3.99995637\n",
      "Epoch: 0394, Training loss=3.84689069\n",
      "Epoch: 0395, Training loss=3.68220758\n",
      "Epoch: 0396, Training loss=3.55738878\n",
      "Epoch: 0397, Training loss=3.43324327\n",
      "Epoch: 0398, Training loss=3.34069252\n",
      "Epoch: 0399, Training loss=3.21397591\n",
      "Epoch: 0400, Training loss=3.07460594\n",
      "Epoch: 0401, Training loss=2.96325803\n",
      "Epoch: 0402, Training loss=2.89539623\n",
      "Epoch: 0403, Training loss=2.85913968\n",
      "Epoch: 0404, Training loss=2.84079909\n",
      "Epoch: 0405, Training loss=2.84028029\n",
      "Epoch: 0406, Training loss=2.83605814\n",
      "Epoch: 0407, Training loss=2.87915277\n",
      "Epoch: 0408, Training loss=2.86595750\n",
      "Epoch: 0409, Training loss=2.90611887\n",
      "Epoch: 0410, Training loss=2.86510587\n",
      "Epoch: 0411, Training loss=2.88707852\n",
      "Epoch: 0412, Training loss=2.95532894\n",
      "Epoch: 0413, Training loss=3.13231182\n",
      "Epoch: 0414, Training loss=3.31489229\n",
      "Epoch: 0415, Training loss=3.54839087\n",
      "Epoch: 0416, Training loss=3.89215326\n",
      "Epoch: 0417, Training loss=4.35816669\n",
      "Epoch: 0418, Training loss=4.82556725\n",
      "Epoch: 0419, Training loss=5.10880518\n",
      "Epoch: 0420, Training loss=5.25521183\n",
      "Epoch: 0421, Training loss=5.27264261\n",
      "Epoch: 0422, Training loss=4.98119402\n",
      "Epoch: 0423, Training loss=4.58266830\n",
      "Epoch: 0424, Training loss=3.98566628\n",
      "Epoch: 0425, Training loss=3.42803025\n",
      "Epoch: 0426, Training loss=3.09059334\n",
      "Epoch: 0427, Training loss=2.89964771\n",
      "Epoch: 0428, Training loss=2.88302350\n",
      "Epoch: 0429, Training loss=3.03430438\n",
      "Epoch: 0430, Training loss=3.33832884\n",
      "Epoch: 0431, Training loss=3.67651057\n",
      "Epoch: 0432, Training loss=4.07249022\n",
      "Epoch: 0433, Training loss=4.51835012\n",
      "Epoch: 0434, Training loss=4.88940525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0435, Training loss=5.15181065\n",
      "Epoch: 0436, Training loss=5.20808458\n",
      "Epoch: 0437, Training loss=5.23259497\n",
      "Epoch: 0438, Training loss=4.95903206\n",
      "Epoch: 0439, Training loss=4.64467049\n",
      "Epoch: 0440, Training loss=4.23183107\n",
      "Epoch: 0441, Training loss=3.86394739\n",
      "Epoch: 0442, Training loss=3.56759667\n",
      "Epoch: 0443, Training loss=3.30458188\n",
      "Epoch: 0444, Training loss=3.11683440\n",
      "Epoch: 0445, Training loss=2.97384596\n",
      "Epoch: 0446, Training loss=2.86533976\n",
      "Epoch: 0447, Training loss=2.79909921\n",
      "Epoch: 0448, Training loss=2.80125904\n",
      "Epoch: 0449, Training loss=2.86781669\n",
      "Epoch: 0450, Training loss=2.99241614\n",
      "Epoch: 0451, Training loss=3.14240313\n",
      "Epoch: 0452, Training loss=3.26413107\n",
      "Epoch: 0453, Training loss=3.35593677\n",
      "Epoch: 0454, Training loss=3.44287872\n",
      "Epoch: 0455, Training loss=3.52168751\n",
      "Epoch: 0456, Training loss=3.61771393\n",
      "Epoch: 0457, Training loss=3.69581389\n",
      "Epoch: 0458, Training loss=3.76191902\n",
      "Epoch: 0459, Training loss=3.67772770\n",
      "Epoch: 0460, Training loss=3.63089371\n",
      "Epoch: 0461, Training loss=3.44562030\n",
      "Epoch: 0462, Training loss=3.30371261\n",
      "Epoch: 0463, Training loss=3.20477581\n",
      "Epoch: 0464, Training loss=3.07992816\n",
      "Epoch: 0465, Training loss=3.03130198\n",
      "Epoch: 0466, Training loss=3.08890009\n",
      "Epoch: 0467, Training loss=3.24943972\n",
      "Epoch: 0468, Training loss=3.57429004\n",
      "Epoch: 0469, Training loss=3.90933919\n",
      "Epoch: 0470, Training loss=4.35433865\n",
      "Epoch: 0471, Training loss=4.85122442\n",
      "Epoch: 0472, Training loss=5.17373371\n",
      "Epoch: 0473, Training loss=5.49569607\n",
      "Epoch: 0474, Training loss=5.52236462\n",
      "Epoch: 0475, Training loss=5.52163219\n",
      "Epoch: 0476, Training loss=5.30739975\n",
      "Epoch: 0477, Training loss=5.04579449\n",
      "Epoch: 0478, Training loss=4.71696520\n",
      "Epoch: 0479, Training loss=4.50404739\n",
      "Epoch: 0480, Training loss=4.26528645\n",
      "Epoch: 0481, Training loss=4.26333952\n",
      "Epoch: 0482, Training loss=4.38585234\n",
      "Epoch: 0483, Training loss=4.47786808\n",
      "Epoch: 0484, Training loss=4.36208916\n",
      "Epoch: 0485, Training loss=4.15456915\n",
      "Epoch: 0486, Training loss=3.78557014\n",
      "Epoch: 0487, Training loss=3.53718686\n",
      "Epoch: 0488, Training loss=3.42385530\n",
      "Epoch: 0489, Training loss=3.40407395\n",
      "Epoch: 0490, Training loss=3.37430882\n",
      "Epoch: 0491, Training loss=3.32191348\n",
      "Epoch: 0492, Training loss=3.25433469\n",
      "Epoch: 0493, Training loss=3.11987686\n",
      "Epoch: 0494, Training loss=3.06552052\n",
      "Epoch: 0495, Training loss=3.04737163\n",
      "Epoch: 0496, Training loss=3.07659531\n",
      "Epoch: 0497, Training loss=3.12744045\n",
      "Epoch: 0498, Training loss=3.17951727\n",
      "Epoch: 0499, Training loss=3.27449441\n",
      "Epoch: 0500, Training loss=3.35197544\n"
     ]
    }
   ],
   "source": [
    "loss_train = np.zeros((epochs, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img)\n",
    "        loss = models.vae_loss(recon_batch, img, mu, logvar,reconstruction_function)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "             \n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item()))\n",
    "\n",
    "torch.save(model.state_dict(), 'saved/models/vae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_batch, mu, logvar = model(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = mu.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.001)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.001)\n",
    "clf.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "regr.fit(feature, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "_recon_batch, testFeature, _logvar = model(testData)\n",
    "lasso = clf.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = clf.predict(testFeature.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-95279325571572.25"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(lasso,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-95279325571572.25"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(adaboost,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 540 samples in 0.007s...\n",
      "[t-SNE] Computed neighbors for 540 samples in 0.060s...\n",
      "[t-SNE] Computed conditional probabilities for sample 540 / 540\n",
      "[t-SNE] Mean sigma: 0.035334\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 73.411369\n",
      "[t-SNE] KL divergence after 300 iterations: 0.842990\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44635877, -9.594072  ],\n",
       "       [-1.3699114 , -5.1074886 ],\n",
       "       [10.973329  , -0.38312718],\n",
       "       ...,\n",
       "       [ 0.9975797 , -9.836095  ],\n",
       "       [-9.14996   , -0.3812821 ],\n",
       "       [ 8.176882  , -8.188568  ]], dtype=float32)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], label=\"label\")\n",
    "plt.legend()\n",
    "plt.savefig(\"saved/figures/tsne_vae_gdsc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# data type conversion\n",
    "B_feature = torch.FloatTensor(feature).to(device)\n",
    "y = torch.FloatTensor(Y_train.values).to(device)\n",
    "# construct TensorDataset\n",
    "b_data = TensorDataset(B_feature, y)\n",
    "trainDataLoader2 = DataLoader(dataset=b_data, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization DNN model\n",
    "\n",
    "predictor = DNN(128, dim_dnn_out).to(device)\n",
    "optimizer = optim.Adam(predictor.parameters(), lr=1e-3,betas=(0.9,0.99))\n",
    "#loss1-softmax\n",
    "loss_func = nn.MSELoss().to(device)\n",
    "#loss2-sigmoid\n",
    "#loss_func = nn.BCELoss()\n",
    "#loss3-sigmoid\n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#criterion = torch.nn.MSELoss(size_average=True)\n",
    "#criterion = torch.nn.BCELoss(size_average=True) # Defined loss function\n",
    "#optimizer = optim.Adm(model.parameters(), lr=0.01) # Defined optimizer\n",
    "loss_train = np.zeros((epochs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch: 0001, Training loss=0.15940861\n",
      "Epoch:  1\n",
      "Epoch: 0002, Training loss=0.15287048\n",
      "Epoch:  2\n",
      "Epoch: 0003, Training loss=0.14638048\n",
      "Epoch:  3\n",
      "Epoch: 0004, Training loss=0.13940901\n",
      "Epoch:  4\n",
      "Epoch: 0005, Training loss=0.13189700\n",
      "Epoch:  5\n",
      "Epoch: 0006, Training loss=0.12362639\n",
      "Epoch:  6\n",
      "Epoch: 0007, Training loss=0.11453197\n",
      "Epoch:  7\n",
      "Epoch: 0008, Training loss=0.10461202\n",
      "Epoch:  8\n",
      "Epoch: 0009, Training loss=0.09392172\n",
      "Epoch:  9\n",
      "Epoch: 0010, Training loss=0.08262192\n",
      "Epoch:  10\n",
      "Epoch: 0011, Training loss=0.07096790\n",
      "Epoch:  11\n",
      "Epoch: 0012, Training loss=0.05934255\n",
      "Epoch:  12\n",
      "Epoch: 0013, Training loss=0.04821935\n",
      "Epoch:  13\n",
      "Epoch: 0014, Training loss=0.03807456\n",
      "Epoch:  14\n",
      "Epoch: 0015, Training loss=0.02927183\n",
      "Epoch:  15\n",
      "Epoch: 0016, Training loss=0.02205614\n",
      "Epoch:  16\n",
      "Epoch: 0017, Training loss=0.01654110\n",
      "Epoch:  17\n",
      "Epoch: 0018, Training loss=0.01263995\n",
      "Epoch:  18\n",
      "Epoch: 0019, Training loss=0.01008122\n",
      "Epoch:  19\n",
      "Epoch: 0020, Training loss=0.00853498\n",
      "Epoch:  20\n",
      "Epoch: 0021, Training loss=0.00768575\n",
      "Epoch:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "D:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([140])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0022, Training loss=0.00727784\n",
      "Epoch:  22\n",
      "Epoch: 0023, Training loss=0.00712578\n",
      "Epoch:  23\n",
      "Epoch: 0024, Training loss=0.00710556\n",
      "Epoch:  24\n",
      "Epoch: 0025, Training loss=0.00714042\n",
      "Epoch:  25\n",
      "Epoch: 0026, Training loss=0.00718620\n",
      "Epoch:  26\n",
      "Epoch: 0027, Training loss=0.00722012\n",
      "Epoch:  27\n",
      "Epoch: 0028, Training loss=0.00723318\n",
      "Epoch:  28\n",
      "Epoch: 0029, Training loss=0.00722439\n",
      "Epoch:  29\n",
      "Epoch: 0030, Training loss=0.00719671\n",
      "Epoch:  30\n",
      "Epoch: 0031, Training loss=0.00715492\n",
      "Epoch:  31\n",
      "Epoch: 0032, Training loss=0.00710415\n",
      "Epoch:  32\n",
      "Epoch: 0033, Training loss=0.00704914\n",
      "Epoch:  33\n",
      "Epoch: 0034, Training loss=0.00699378\n",
      "Epoch:  34\n",
      "Epoch: 0035, Training loss=0.00694103\n",
      "Epoch:  35\n",
      "Epoch: 0036, Training loss=0.00689294\n",
      "Epoch:  36\n",
      "Epoch: 0037, Training loss=0.00685051\n",
      "Epoch:  37\n",
      "Epoch: 0038, Training loss=0.00681426\n",
      "Epoch:  38\n",
      "Epoch: 0039, Training loss=0.00678415\n",
      "Epoch:  39\n",
      "Epoch: 0040, Training loss=0.00675960\n",
      "Epoch:  40\n",
      "Epoch: 0041, Training loss=0.00674013\n",
      "Epoch:  41\n",
      "Epoch: 0042, Training loss=0.00672503\n",
      "Epoch:  42\n",
      "Epoch: 0043, Training loss=0.00671350\n",
      "Epoch:  43\n",
      "Epoch: 0044, Training loss=0.00670467\n",
      "Epoch:  44\n",
      "Epoch: 0045, Training loss=0.00669810\n",
      "Epoch:  45\n",
      "Epoch: 0046, Training loss=0.00669339\n",
      "Epoch:  46\n",
      "Epoch: 0047, Training loss=0.00669017\n",
      "Epoch:  47\n",
      "Epoch: 0048, Training loss=0.00668811\n",
      "Epoch:  48\n",
      "Epoch: 0049, Training loss=0.00668691\n",
      "Epoch:  49\n",
      "Epoch: 0050, Training loss=0.00668630\n",
      "Epoch:  50\n",
      "Epoch: 0051, Training loss=0.00668603\n",
      "Epoch:  51\n",
      "Epoch: 0052, Training loss=0.00668589\n",
      "Epoch:  52\n",
      "Epoch: 0053, Training loss=0.00668571\n",
      "Epoch:  53\n",
      "Epoch: 0054, Training loss=0.00668538\n",
      "Epoch:  54\n",
      "Epoch: 0055, Training loss=0.00668481\n",
      "Epoch:  55\n",
      "Epoch: 0056, Training loss=0.00668399\n",
      "Epoch:  56\n",
      "Epoch: 0057, Training loss=0.00668293\n",
      "Epoch:  57\n",
      "Epoch: 0058, Training loss=0.00668165\n",
      "Epoch:  58\n",
      "Epoch: 0059, Training loss=0.00668021\n",
      "Epoch:  59\n",
      "Epoch: 0060, Training loss=0.00667868\n",
      "Epoch:  60\n",
      "Epoch: 0061, Training loss=0.00667709\n",
      "Epoch:  61\n",
      "Epoch: 0062, Training loss=0.00667552\n",
      "Epoch:  62\n",
      "Epoch: 0063, Training loss=0.00667398\n",
      "Epoch:  63\n",
      "Epoch: 0064, Training loss=0.00667252\n",
      "Epoch:  64\n",
      "Epoch: 0065, Training loss=0.00667113\n",
      "Epoch:  65\n",
      "Epoch: 0066, Training loss=0.00666984\n",
      "Epoch:  66\n",
      "Epoch: 0067, Training loss=0.00666862\n",
      "Epoch:  67\n",
      "Epoch: 0068, Training loss=0.00666749\n",
      "Epoch:  68\n",
      "Epoch: 0069, Training loss=0.00666642\n",
      "Epoch:  69\n",
      "Epoch: 0070, Training loss=0.00666539\n",
      "Epoch:  70\n",
      "Epoch: 0071, Training loss=0.00666441\n",
      "Epoch:  71\n",
      "Epoch: 0072, Training loss=0.00666345\n",
      "Epoch:  72\n",
      "Epoch: 0073, Training loss=0.00666250\n",
      "Epoch:  73\n",
      "Epoch: 0074, Training loss=0.00666157\n",
      "Epoch:  74\n",
      "Epoch: 0075, Training loss=0.00666063\n",
      "Epoch:  75\n",
      "Epoch: 0076, Training loss=0.00665970\n",
      "Epoch:  76\n",
      "Epoch: 0077, Training loss=0.00665878\n",
      "Epoch:  77\n",
      "Epoch: 0078, Training loss=0.00665786\n",
      "Epoch:  78\n",
      "Epoch: 0079, Training loss=0.00665695\n",
      "Epoch:  79\n",
      "Epoch: 0080, Training loss=0.00665606\n",
      "Epoch:  80\n",
      "Epoch: 0081, Training loss=0.00665519\n",
      "Epoch:  81\n",
      "Epoch: 0082, Training loss=0.00665434\n",
      "Epoch:  82\n",
      "Epoch: 0083, Training loss=0.00665352\n",
      "Epoch:  83\n",
      "Epoch: 0084, Training loss=0.00665272\n",
      "Epoch:  84\n",
      "Epoch: 0085, Training loss=0.00665195\n",
      "Epoch:  85\n",
      "Epoch: 0086, Training loss=0.00665120\n",
      "Epoch:  86\n",
      "Epoch: 0087, Training loss=0.00665048\n",
      "Epoch:  87\n",
      "Epoch: 0088, Training loss=0.00664979\n",
      "Epoch:  88\n",
      "Epoch: 0089, Training loss=0.00664911\n",
      "Epoch:  89\n",
      "Epoch: 0090, Training loss=0.00664846\n",
      "Epoch:  90\n",
      "Epoch: 0091, Training loss=0.00664783\n",
      "Epoch:  91\n",
      "Epoch: 0092, Training loss=0.00664722\n",
      "Epoch:  92\n",
      "Epoch: 0093, Training loss=0.00664662\n",
      "Epoch:  93\n",
      "Epoch: 0094, Training loss=0.00664604\n",
      "Epoch:  94\n",
      "Epoch: 0095, Training loss=0.00664549\n",
      "Epoch:  95\n",
      "Epoch: 0096, Training loss=0.00664495\n",
      "Epoch:  96\n",
      "Epoch: 0097, Training loss=0.00664442\n",
      "Epoch:  97\n",
      "Epoch: 0098, Training loss=0.00664392\n",
      "Epoch:  98\n",
      "Epoch: 0099, Training loss=0.00664343\n",
      "Epoch:  99\n",
      "Epoch: 0100, Training loss=0.00664296\n",
      "Epoch:  100\n",
      "Epoch: 0101, Training loss=0.00664250\n",
      "Epoch:  101\n",
      "Epoch: 0102, Training loss=0.00664206\n",
      "Epoch:  102\n",
      "Epoch: 0103, Training loss=0.00664163\n",
      "Epoch:  103\n",
      "Epoch: 0104, Training loss=0.00664122\n",
      "Epoch:  104\n",
      "Epoch: 0105, Training loss=0.00664082\n",
      "Epoch:  105\n",
      "Epoch: 0106, Training loss=0.00664043\n",
      "Epoch:  106\n",
      "Epoch: 0107, Training loss=0.00664005\n",
      "Epoch:  107\n",
      "Epoch: 0108, Training loss=0.00663969\n",
      "Epoch:  108\n",
      "Epoch: 0109, Training loss=0.00663934\n",
      "Epoch:  109\n",
      "Epoch: 0110, Training loss=0.00663900\n",
      "Epoch:  110\n",
      "Epoch: 0111, Training loss=0.00663867\n",
      "Epoch:  111\n",
      "Epoch: 0112, Training loss=0.00663835\n",
      "Epoch:  112\n",
      "Epoch: 0113, Training loss=0.00663804\n",
      "Epoch:  113\n",
      "Epoch: 0114, Training loss=0.00663773\n",
      "Epoch:  114\n",
      "Epoch: 0115, Training loss=0.00663743\n",
      "Epoch:  115\n",
      "Epoch: 0116, Training loss=0.00663715\n",
      "Epoch:  116\n",
      "Epoch: 0117, Training loss=0.00663687\n",
      "Epoch:  117\n",
      "Epoch: 0118, Training loss=0.00663661\n",
      "Epoch:  118\n",
      "Epoch: 0119, Training loss=0.00663636\n",
      "Epoch:  119\n",
      "Epoch: 0120, Training loss=0.00663612\n",
      "Epoch:  120\n",
      "Epoch: 0121, Training loss=0.00663589\n",
      "Epoch:  121\n",
      "Epoch: 0122, Training loss=0.00663566\n",
      "Epoch:  122\n",
      "Epoch: 0123, Training loss=0.00663544\n",
      "Epoch:  123\n",
      "Epoch: 0124, Training loss=0.00663523\n",
      "Epoch:  124\n",
      "Epoch: 0125, Training loss=0.00663501\n",
      "Epoch:  125\n",
      "Epoch: 0126, Training loss=0.00663481\n",
      "Epoch:  126\n",
      "Epoch: 0127, Training loss=0.00663461\n",
      "Epoch:  127\n",
      "Epoch: 0128, Training loss=0.00663441\n",
      "Epoch:  128\n",
      "Epoch: 0129, Training loss=0.00663421\n",
      "Epoch:  129\n",
      "Epoch: 0130, Training loss=0.00663402\n",
      "Epoch:  130\n",
      "Epoch: 0131, Training loss=0.00663384\n",
      "Epoch:  131\n",
      "Epoch: 0132, Training loss=0.00663365\n",
      "Epoch:  132\n",
      "Epoch: 0133, Training loss=0.00663347\n",
      "Epoch:  133\n",
      "Epoch: 0134, Training loss=0.00663329\n",
      "Epoch:  134\n",
      "Epoch: 0135, Training loss=0.00663311\n",
      "Epoch:  135\n",
      "Epoch: 0136, Training loss=0.00663293\n",
      "Epoch:  136\n",
      "Epoch: 0137, Training loss=0.00663275\n",
      "Epoch:  137\n",
      "Epoch: 0138, Training loss=0.00663258\n",
      "Epoch:  138\n",
      "Epoch: 0139, Training loss=0.00663241\n",
      "Epoch:  139\n",
      "Epoch: 0140, Training loss=0.00663225\n",
      "Epoch:  140\n",
      "Epoch: 0141, Training loss=0.00663210\n",
      "Epoch:  141\n",
      "Epoch: 0142, Training loss=0.00663195\n",
      "Epoch:  142\n",
      "Epoch: 0143, Training loss=0.00663181\n",
      "Epoch:  143\n",
      "Epoch: 0144, Training loss=0.00663166\n",
      "Epoch:  144\n",
      "Epoch: 0145, Training loss=0.00663152\n",
      "Epoch:  145\n",
      "Epoch: 0146, Training loss=0.00663137\n",
      "Epoch:  146\n",
      "Epoch: 0147, Training loss=0.00663123\n",
      "Epoch:  147\n",
      "Epoch: 0148, Training loss=0.00663108\n",
      "Epoch:  148\n",
      "Epoch: 0149, Training loss=0.00663094\n",
      "Epoch:  149\n",
      "Epoch: 0150, Training loss=0.00663080\n",
      "Epoch:  150\n",
      "Epoch: 0151, Training loss=0.00663066\n",
      "Epoch:  151\n",
      "Epoch: 0152, Training loss=0.00663053\n",
      "Epoch:  152\n",
      "Epoch: 0153, Training loss=0.00663040\n",
      "Epoch:  153\n",
      "Epoch: 0154, Training loss=0.00663027\n",
      "Epoch:  154\n",
      "Epoch: 0155, Training loss=0.00663013\n",
      "Epoch:  155\n",
      "Epoch: 0156, Training loss=0.00663000\n",
      "Epoch:  156\n",
      "Epoch: 0157, Training loss=0.00662987\n",
      "Epoch:  157\n",
      "Epoch: 0158, Training loss=0.00662975\n",
      "Epoch:  158\n",
      "Epoch: 0159, Training loss=0.00662963\n",
      "Epoch:  159\n",
      "Epoch: 0160, Training loss=0.00662952\n",
      "Epoch:  160\n",
      "Epoch: 0161, Training loss=0.00662940\n",
      "Epoch:  161\n",
      "Epoch: 0162, Training loss=0.00662929\n",
      "Epoch:  162\n",
      "Epoch: 0163, Training loss=0.00662918\n",
      "Epoch:  163\n",
      "Epoch: 0164, Training loss=0.00662906\n",
      "Epoch:  164\n",
      "Epoch: 0165, Training loss=0.00662894\n",
      "Epoch:  165\n",
      "Epoch: 0166, Training loss=0.00662883\n",
      "Epoch:  166\n",
      "Epoch: 0167, Training loss=0.00662871\n",
      "Epoch:  167\n",
      "Epoch: 0168, Training loss=0.00662860\n",
      "Epoch:  168\n",
      "Epoch: 0169, Training loss=0.00662849\n",
      "Epoch:  169\n",
      "Epoch: 0170, Training loss=0.00662839\n",
      "Epoch:  170\n",
      "Epoch: 0171, Training loss=0.00662828\n",
      "Epoch:  171\n",
      "Epoch: 0172, Training loss=0.00662818\n",
      "Epoch:  172\n",
      "Epoch: 0173, Training loss=0.00662808\n",
      "Epoch:  173\n",
      "Epoch: 0174, Training loss=0.00662798\n",
      "Epoch:  174\n",
      "Epoch: 0175, Training loss=0.00662787\n",
      "Epoch:  175\n",
      "Epoch: 0176, Training loss=0.00662777\n",
      "Epoch:  176\n",
      "Epoch: 0177, Training loss=0.00662767\n",
      "Epoch:  177\n",
      "Epoch: 0178, Training loss=0.00662757\n",
      "Epoch:  178\n",
      "Epoch: 0179, Training loss=0.00662748\n",
      "Epoch:  179\n",
      "Epoch: 0180, Training loss=0.00662738\n",
      "Epoch:  180\n",
      "Epoch: 0181, Training loss=0.00662728\n",
      "Epoch:  181\n",
      "Epoch: 0182, Training loss=0.00662718\n",
      "Epoch:  182\n",
      "Epoch: 0183, Training loss=0.00662708\n",
      "Epoch:  183\n",
      "Epoch: 0184, Training loss=0.00662699\n",
      "Epoch:  184\n",
      "Epoch: 0185, Training loss=0.00662690\n",
      "Epoch:  185\n",
      "Epoch: 0186, Training loss=0.00662681\n",
      "Epoch:  186\n",
      "Epoch: 0187, Training loss=0.00662673\n",
      "Epoch:  187\n",
      "Epoch: 0188, Training loss=0.00662665\n",
      "Epoch:  188\n",
      "Epoch: 0189, Training loss=0.00662656\n",
      "Epoch:  189\n",
      "Epoch: 0190, Training loss=0.00662648\n",
      "Epoch:  190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0191, Training loss=0.00662639\n",
      "Epoch:  191\n",
      "Epoch: 0192, Training loss=0.00662631\n",
      "Epoch:  192\n",
      "Epoch: 0193, Training loss=0.00662623\n",
      "Epoch:  193\n",
      "Epoch: 0194, Training loss=0.00662615\n",
      "Epoch:  194\n",
      "Epoch: 0195, Training loss=0.00662607\n",
      "Epoch:  195\n",
      "Epoch: 0196, Training loss=0.00662600\n",
      "Epoch:  196\n",
      "Epoch: 0197, Training loss=0.00662592\n",
      "Epoch:  197\n",
      "Epoch: 0198, Training loss=0.00662585\n",
      "Epoch:  198\n",
      "Epoch: 0199, Training loss=0.00662578\n",
      "Epoch:  199\n",
      "Epoch: 0200, Training loss=0.00662570\n",
      "Epoch:  200\n",
      "Epoch: 0201, Training loss=0.00662563\n",
      "Epoch:  201\n",
      "Epoch: 0202, Training loss=0.00662557\n",
      "Epoch:  202\n",
      "Epoch: 0203, Training loss=0.00662550\n",
      "Epoch:  203\n",
      "Epoch: 0204, Training loss=0.00662543\n",
      "Epoch:  204\n",
      "Epoch: 0205, Training loss=0.00662537\n",
      "Epoch:  205\n",
      "Epoch: 0206, Training loss=0.00662530\n",
      "Epoch:  206\n",
      "Epoch: 0207, Training loss=0.00662523\n",
      "Epoch:  207\n",
      "Epoch: 0208, Training loss=0.00662517\n",
      "Epoch:  208\n",
      "Epoch: 0209, Training loss=0.00662511\n",
      "Epoch:  209\n",
      "Epoch: 0210, Training loss=0.00662505\n",
      "Epoch:  210\n",
      "Epoch: 0211, Training loss=0.00662499\n",
      "Epoch:  211\n",
      "Epoch: 0212, Training loss=0.00662493\n",
      "Epoch:  212\n",
      "Epoch: 0213, Training loss=0.00662487\n",
      "Epoch:  213\n",
      "Epoch: 0214, Training loss=0.00662481\n",
      "Epoch:  214\n",
      "Epoch: 0215, Training loss=0.00662475\n",
      "Epoch:  215\n",
      "Epoch: 0216, Training loss=0.00662469\n",
      "Epoch:  216\n",
      "Epoch: 0217, Training loss=0.00662464\n",
      "Epoch:  217\n",
      "Epoch: 0218, Training loss=0.00662459\n",
      "Epoch:  218\n",
      "Epoch: 0219, Training loss=0.00662454\n",
      "Epoch:  219\n",
      "Epoch: 0220, Training loss=0.00662448\n",
      "Epoch:  220\n",
      "Epoch: 0221, Training loss=0.00662443\n",
      "Epoch:  221\n",
      "Epoch: 0222, Training loss=0.00662437\n",
      "Epoch:  222\n",
      "Epoch: 0223, Training loss=0.00662432\n",
      "Epoch:  223\n",
      "Epoch: 0224, Training loss=0.00662427\n",
      "Epoch:  224\n",
      "Epoch: 0225, Training loss=0.00662422\n",
      "Epoch:  225\n",
      "Epoch: 0226, Training loss=0.00662418\n",
      "Epoch:  226\n",
      "Epoch: 0227, Training loss=0.00662413\n",
      "Epoch:  227\n",
      "Epoch: 0228, Training loss=0.00662408\n",
      "Epoch:  228\n",
      "Epoch: 0229, Training loss=0.00662403\n",
      "Epoch:  229\n",
      "Epoch: 0230, Training loss=0.00662399\n",
      "Epoch:  230\n",
      "Epoch: 0231, Training loss=0.00662394\n",
      "Epoch:  231\n",
      "Epoch: 0232, Training loss=0.00662390\n",
      "Epoch:  232\n",
      "Epoch: 0233, Training loss=0.00662385\n",
      "Epoch:  233\n",
      "Epoch: 0234, Training loss=0.00662381\n",
      "Epoch:  234\n",
      "Epoch: 0235, Training loss=0.00662376\n",
      "Epoch:  235\n",
      "Epoch: 0236, Training loss=0.00662372\n",
      "Epoch:  236\n",
      "Epoch: 0237, Training loss=0.00662369\n",
      "Epoch:  237\n",
      "Epoch: 0238, Training loss=0.00662365\n",
      "Epoch:  238\n",
      "Epoch: 0239, Training loss=0.00662361\n",
      "Epoch:  239\n",
      "Epoch: 0240, Training loss=0.00662357\n",
      "Epoch:  240\n",
      "Epoch: 0241, Training loss=0.00662353\n",
      "Epoch:  241\n",
      "Epoch: 0242, Training loss=0.00662348\n",
      "Epoch:  242\n",
      "Epoch: 0243, Training loss=0.00662344\n",
      "Epoch:  243\n",
      "Epoch: 0244, Training loss=0.00662341\n",
      "Epoch:  244\n",
      "Epoch: 0245, Training loss=0.00662339\n",
      "Epoch:  245\n",
      "Epoch: 0246, Training loss=0.00662335\n",
      "Epoch:  246\n",
      "Epoch: 0247, Training loss=0.00662330\n",
      "Epoch:  247\n",
      "Epoch: 0248, Training loss=0.00662326\n",
      "Epoch:  248\n",
      "Epoch: 0249, Training loss=0.00662323\n",
      "Epoch:  249\n",
      "Epoch: 0250, Training loss=0.00662321\n",
      "Epoch:  250\n",
      "Epoch: 0251, Training loss=0.00662318\n",
      "Epoch:  251\n",
      "Epoch: 0252, Training loss=0.00662314\n",
      "Epoch:  252\n",
      "Epoch: 0253, Training loss=0.00662310\n",
      "Epoch:  253\n",
      "Epoch: 0254, Training loss=0.00662307\n",
      "Epoch:  254\n",
      "Epoch: 0255, Training loss=0.00662304\n",
      "Epoch:  255\n",
      "Epoch: 0256, Training loss=0.00662301\n",
      "Epoch:  256\n",
      "Epoch: 0257, Training loss=0.00662298\n",
      "Epoch:  257\n",
      "Epoch: 0258, Training loss=0.00662295\n",
      "Epoch:  258\n",
      "Epoch: 0259, Training loss=0.00662292\n",
      "Epoch:  259\n",
      "Epoch: 0260, Training loss=0.00662289\n",
      "Epoch:  260\n",
      "Epoch: 0261, Training loss=0.00662286\n",
      "Epoch:  261\n",
      "Epoch: 0262, Training loss=0.00662283\n",
      "Epoch:  262\n",
      "Epoch: 0263, Training loss=0.00662281\n",
      "Epoch:  263\n",
      "Epoch: 0264, Training loss=0.00662278\n",
      "Epoch:  264\n",
      "Epoch: 0265, Training loss=0.00662275\n",
      "Epoch:  265\n",
      "Epoch: 0266, Training loss=0.00662272\n",
      "Epoch:  266\n",
      "Epoch: 0267, Training loss=0.00662269\n",
      "Epoch:  267\n",
      "Epoch: 0268, Training loss=0.00662267\n",
      "Epoch:  268\n",
      "Epoch: 0269, Training loss=0.00662264\n",
      "Epoch:  269\n",
      "Epoch: 0270, Training loss=0.00662261\n",
      "Epoch:  270\n",
      "Epoch: 0271, Training loss=0.00662259\n",
      "Epoch:  271\n",
      "Epoch: 0272, Training loss=0.00662257\n",
      "Epoch:  272\n",
      "Epoch: 0273, Training loss=0.00662254\n",
      "Epoch:  273\n",
      "Epoch: 0274, Training loss=0.00662251\n",
      "Epoch:  274\n",
      "Epoch: 0275, Training loss=0.00662249\n",
      "Epoch:  275\n",
      "Epoch: 0276, Training loss=0.00662247\n",
      "Epoch:  276\n",
      "Epoch: 0277, Training loss=0.00662245\n",
      "Epoch:  277\n",
      "Epoch: 0278, Training loss=0.00662243\n",
      "Epoch:  278\n",
      "Epoch: 0279, Training loss=0.00662240\n",
      "Epoch:  279\n",
      "Epoch: 0280, Training loss=0.00662237\n",
      "Epoch:  280\n",
      "Epoch: 0281, Training loss=0.00662235\n",
      "Epoch:  281\n",
      "Epoch: 0282, Training loss=0.00662233\n",
      "Epoch:  282\n",
      "Epoch: 0283, Training loss=0.00662232\n",
      "Epoch:  283\n",
      "Epoch: 0284, Training loss=0.00662230\n",
      "Epoch:  284\n",
      "Epoch: 0285, Training loss=0.00662227\n",
      "Epoch:  285\n",
      "Epoch: 0286, Training loss=0.00662225\n",
      "Epoch:  286\n",
      "Epoch: 0287, Training loss=0.00662222\n",
      "Epoch:  287\n",
      "Epoch: 0288, Training loss=0.00662220\n",
      "Epoch:  288\n",
      "Epoch: 0289, Training loss=0.00662219\n",
      "Epoch:  289\n",
      "Epoch: 0290, Training loss=0.00662218\n",
      "Epoch:  290\n",
      "Epoch: 0291, Training loss=0.00662215\n",
      "Epoch:  291\n",
      "Epoch: 0292, Training loss=0.00662213\n",
      "Epoch:  292\n",
      "Epoch: 0293, Training loss=0.00662211\n",
      "Epoch:  293\n",
      "Epoch: 0294, Training loss=0.00662208\n",
      "Epoch:  294\n",
      "Epoch: 0295, Training loss=0.00662207\n",
      "Epoch:  295\n",
      "Epoch: 0296, Training loss=0.00662206\n",
      "Epoch:  296\n",
      "Epoch: 0297, Training loss=0.00662204\n",
      "Epoch:  297\n",
      "Epoch: 0298, Training loss=0.00662201\n",
      "Epoch:  298\n",
      "Epoch: 0299, Training loss=0.00662199\n",
      "Epoch:  299\n",
      "Epoch: 0300, Training loss=0.00662198\n",
      "Epoch:  300\n",
      "Epoch: 0301, Training loss=0.00662197\n",
      "Epoch:  301\n",
      "Epoch: 0302, Training loss=0.00662195\n",
      "Epoch:  302\n",
      "Epoch: 0303, Training loss=0.00662192\n",
      "Epoch:  303\n",
      "Epoch: 0304, Training loss=0.00662190\n",
      "Epoch:  304\n",
      "Epoch: 0305, Training loss=0.00662189\n",
      "Epoch:  305\n",
      "Epoch: 0306, Training loss=0.00662188\n",
      "Epoch:  306\n",
      "Epoch: 0307, Training loss=0.00662186\n",
      "Epoch:  307\n",
      "Epoch: 0308, Training loss=0.00662183\n",
      "Epoch:  308\n",
      "Epoch: 0309, Training loss=0.00662181\n",
      "Epoch:  309\n",
      "Epoch: 0310, Training loss=0.00662180\n",
      "Epoch:  310\n",
      "Epoch: 0311, Training loss=0.00662179\n",
      "Epoch:  311\n",
      "Epoch: 0312, Training loss=0.00662177\n",
      "Epoch:  312\n",
      "Epoch: 0313, Training loss=0.00662174\n",
      "Epoch:  313\n",
      "Epoch: 0314, Training loss=0.00662173\n",
      "Epoch:  314\n",
      "Epoch: 0315, Training loss=0.00662172\n",
      "Epoch:  315\n",
      "Epoch: 0316, Training loss=0.00662170\n",
      "Epoch:  316\n",
      "Epoch: 0317, Training loss=0.00662168\n",
      "Epoch:  317\n",
      "Epoch: 0318, Training loss=0.00662166\n",
      "Epoch:  318\n",
      "Epoch: 0319, Training loss=0.00662165\n",
      "Epoch:  319\n",
      "Epoch: 0320, Training loss=0.00662164\n",
      "Epoch:  320\n",
      "Epoch: 0321, Training loss=0.00662162\n",
      "Epoch:  321\n",
      "Epoch: 0322, Training loss=0.00662160\n",
      "Epoch:  322\n",
      "Epoch: 0323, Training loss=0.00662159\n",
      "Epoch:  323\n",
      "Epoch: 0324, Training loss=0.00662158\n",
      "Epoch:  324\n",
      "Epoch: 0325, Training loss=0.00662156\n",
      "Epoch:  325\n",
      "Epoch: 0326, Training loss=0.00662153\n",
      "Epoch:  326\n",
      "Epoch: 0327, Training loss=0.00662153\n",
      "Epoch:  327\n",
      "Epoch: 0328, Training loss=0.00662152\n",
      "Epoch:  328\n",
      "Epoch: 0329, Training loss=0.00662149\n",
      "Epoch:  329\n",
      "Epoch: 0330, Training loss=0.00662147\n",
      "Epoch:  330\n",
      "Epoch: 0331, Training loss=0.00662147\n",
      "Epoch:  331\n",
      "Epoch: 0332, Training loss=0.00662147\n",
      "Epoch:  332\n",
      "Epoch: 0333, Training loss=0.00662145\n",
      "Epoch:  333\n",
      "Epoch: 0334, Training loss=0.00662141\n",
      "Epoch:  334\n",
      "Epoch: 0335, Training loss=0.00662140\n",
      "Epoch:  335\n",
      "Epoch: 0336, Training loss=0.00662141\n",
      "Epoch:  336\n",
      "Epoch: 0337, Training loss=0.00662139\n",
      "Epoch:  337\n",
      "Epoch: 0338, Training loss=0.00662136\n",
      "Epoch:  338\n",
      "Epoch: 0339, Training loss=0.00662134\n",
      "Epoch:  339\n",
      "Epoch: 0340, Training loss=0.00662134\n",
      "Epoch:  340\n",
      "Epoch: 0341, Training loss=0.00662133\n",
      "Epoch:  341\n",
      "Epoch: 0342, Training loss=0.00662132\n",
      "Epoch:  342\n",
      "Epoch: 0343, Training loss=0.00662130\n",
      "Epoch:  343\n",
      "Epoch: 0344, Training loss=0.00662129\n",
      "Epoch:  344\n",
      "Epoch: 0345, Training loss=0.00662127\n",
      "Epoch:  345\n",
      "Epoch: 0346, Training loss=0.00662126\n",
      "Epoch:  346\n",
      "Epoch: 0347, Training loss=0.00662125\n",
      "Epoch:  347\n",
      "Epoch: 0348, Training loss=0.00662124\n",
      "Epoch:  348\n",
      "Epoch: 0349, Training loss=0.00662122\n",
      "Epoch:  349\n",
      "Epoch: 0350, Training loss=0.00662121\n",
      "Epoch:  350\n",
      "Epoch: 0351, Training loss=0.00662120\n",
      "Epoch:  351\n",
      "Epoch: 0352, Training loss=0.00662119\n",
      "Epoch:  352\n",
      "Epoch: 0353, Training loss=0.00662116\n",
      "Epoch:  353\n",
      "Epoch: 0354, Training loss=0.00662116\n",
      "Epoch:  354\n",
      "Epoch: 0355, Training loss=0.00662115\n",
      "Epoch:  355\n",
      "Epoch: 0356, Training loss=0.00662114\n",
      "Epoch:  356\n",
      "Epoch: 0357, Training loss=0.00662112\n",
      "Epoch:  357\n",
      "Epoch: 0358, Training loss=0.00662111\n",
      "Epoch:  358\n",
      "Epoch: 0359, Training loss=0.00662110\n",
      "Epoch:  359\n",
      "Epoch: 0360, Training loss=0.00662109\n",
      "Epoch:  360\n",
      "Epoch: 0361, Training loss=0.00662107\n",
      "Epoch:  361\n",
      "Epoch: 0362, Training loss=0.00662106\n",
      "Epoch:  362\n",
      "Epoch: 0363, Training loss=0.00662105\n",
      "Epoch:  363\n",
      "Epoch: 0364, Training loss=0.00662104\n",
      "Epoch:  364\n",
      "Epoch: 0365, Training loss=0.00662103\n",
      "Epoch:  365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0366, Training loss=0.00662101\n",
      "Epoch:  366\n",
      "Epoch: 0367, Training loss=0.00662100\n",
      "Epoch:  367\n",
      "Epoch: 0368, Training loss=0.00662099\n",
      "Epoch:  368\n",
      "Epoch: 0369, Training loss=0.00662098\n",
      "Epoch:  369\n",
      "Epoch: 0370, Training loss=0.00662097\n",
      "Epoch:  370\n",
      "Epoch: 0371, Training loss=0.00662096\n",
      "Epoch:  371\n",
      "Epoch: 0372, Training loss=0.00662094\n",
      "Epoch:  372\n",
      "Epoch: 0373, Training loss=0.00662093\n",
      "Epoch:  373\n",
      "Epoch: 0374, Training loss=0.00662093\n",
      "Epoch:  374\n",
      "Epoch: 0375, Training loss=0.00662091\n",
      "Epoch:  375\n",
      "Epoch: 0376, Training loss=0.00662089\n",
      "Epoch:  376\n",
      "Epoch: 0377, Training loss=0.00662088\n",
      "Epoch:  377\n",
      "Epoch: 0378, Training loss=0.00662088\n",
      "Epoch:  378\n",
      "Epoch: 0379, Training loss=0.00662088\n",
      "Epoch:  379\n",
      "Epoch: 0380, Training loss=0.00662085\n",
      "Epoch:  380\n",
      "Epoch: 0381, Training loss=0.00662083\n",
      "Epoch:  381\n",
      "Epoch: 0382, Training loss=0.00662083\n",
      "Epoch:  382\n",
      "Epoch: 0383, Training loss=0.00662083\n",
      "Epoch:  383\n",
      "Epoch: 0384, Training loss=0.00662081\n",
      "Epoch:  384\n",
      "Epoch: 0385, Training loss=0.00662080\n",
      "Epoch:  385\n",
      "Epoch: 0386, Training loss=0.00662079\n",
      "Epoch:  386\n",
      "Epoch: 0387, Training loss=0.00662077\n",
      "Epoch:  387\n",
      "Epoch: 0388, Training loss=0.00662076\n",
      "Epoch:  388\n",
      "Epoch: 0389, Training loss=0.00662076\n",
      "Epoch:  389\n",
      "Epoch: 0390, Training loss=0.00662075\n",
      "Epoch:  390\n",
      "Epoch: 0391, Training loss=0.00662073\n",
      "Epoch:  391\n",
      "Epoch: 0392, Training loss=0.00662072\n",
      "Epoch:  392\n",
      "Epoch: 0393, Training loss=0.00662071\n",
      "Epoch:  393\n",
      "Epoch: 0394, Training loss=0.00662070\n",
      "Epoch:  394\n",
      "Epoch: 0395, Training loss=0.00662069\n",
      "Epoch:  395\n",
      "Epoch: 0396, Training loss=0.00662067\n",
      "Epoch:  396\n",
      "Epoch: 0397, Training loss=0.00662067\n",
      "Epoch:  397\n",
      "Epoch: 0398, Training loss=0.00662067\n",
      "Epoch:  398\n",
      "Epoch: 0399, Training loss=0.00662065\n",
      "Epoch:  399\n",
      "Epoch: 0400, Training loss=0.00662062\n",
      "Epoch:  400\n",
      "Epoch: 0401, Training loss=0.00662062\n",
      "Epoch:  401\n",
      "Epoch: 0402, Training loss=0.00662062\n",
      "Epoch:  402\n",
      "Epoch: 0403, Training loss=0.00662061\n",
      "Epoch:  403\n",
      "Epoch: 0404, Training loss=0.00662059\n",
      "Epoch:  404\n",
      "Epoch: 0405, Training loss=0.00662057\n",
      "Epoch:  405\n",
      "Epoch: 0406, Training loss=0.00662057\n",
      "Epoch:  406\n",
      "Epoch: 0407, Training loss=0.00662057\n",
      "Epoch:  407\n",
      "Epoch: 0408, Training loss=0.00662055\n",
      "Epoch:  408\n",
      "Epoch: 0409, Training loss=0.00662053\n",
      "Epoch:  409\n",
      "Epoch: 0410, Training loss=0.00662052\n",
      "Epoch:  410\n",
      "Epoch: 0411, Training loss=0.00662053\n",
      "Epoch:  411\n",
      "Epoch: 0412, Training loss=0.00662051\n",
      "Epoch:  412\n",
      "Epoch: 0413, Training loss=0.00662050\n",
      "Epoch:  413\n",
      "Epoch: 0414, Training loss=0.00662048\n",
      "Epoch:  414\n",
      "Epoch: 0415, Training loss=0.00662047\n",
      "Epoch:  415\n",
      "Epoch: 0416, Training loss=0.00662046\n",
      "Epoch:  416\n",
      "Epoch: 0417, Training loss=0.00662046\n",
      "Epoch:  417\n",
      "Epoch: 0418, Training loss=0.00662045\n",
      "Epoch:  418\n",
      "Epoch: 0419, Training loss=0.00662044\n",
      "Epoch:  419\n",
      "Epoch: 0420, Training loss=0.00662042\n",
      "Epoch:  420\n",
      "Epoch: 0421, Training loss=0.00662040\n",
      "Epoch:  421\n",
      "Epoch: 0422, Training loss=0.00662040\n",
      "Epoch:  422\n",
      "Epoch: 0423, Training loss=0.00662039\n",
      "Epoch:  423\n",
      "Epoch: 0424, Training loss=0.00662038\n",
      "Epoch:  424\n",
      "Epoch: 0425, Training loss=0.00662037\n",
      "Epoch:  425\n",
      "Epoch: 0426, Training loss=0.00662035\n",
      "Epoch:  426\n",
      "Epoch: 0427, Training loss=0.00662034\n",
      "Epoch:  427\n",
      "Epoch: 0428, Training loss=0.00662033\n",
      "Epoch:  428\n",
      "Epoch: 0429, Training loss=0.00662033\n",
      "Epoch:  429\n",
      "Epoch: 0430, Training loss=0.00662032\n",
      "Epoch:  430\n",
      "Epoch: 0431, Training loss=0.00662029\n",
      "Epoch:  431\n",
      "Epoch: 0432, Training loss=0.00662028\n",
      "Epoch:  432\n",
      "Epoch: 0433, Training loss=0.00662029\n",
      "Epoch:  433\n",
      "Epoch: 0434, Training loss=0.00662028\n",
      "Epoch:  434\n",
      "Epoch: 0435, Training loss=0.00662025\n",
      "Epoch:  435\n",
      "Epoch: 0436, Training loss=0.00662024\n",
      "Epoch:  436\n",
      "Epoch: 0437, Training loss=0.00662024\n",
      "Epoch:  437\n",
      "Epoch: 0438, Training loss=0.00662023\n",
      "Epoch:  438\n",
      "Epoch: 0439, Training loss=0.00662020\n",
      "Epoch:  439\n",
      "Epoch: 0440, Training loss=0.00662020\n",
      "Epoch:  440\n",
      "Epoch: 0441, Training loss=0.00662020\n",
      "Epoch:  441\n",
      "Epoch: 0442, Training loss=0.00662017\n",
      "Epoch:  442\n",
      "Epoch: 0443, Training loss=0.00662016\n",
      "Epoch:  443\n",
      "Epoch: 0444, Training loss=0.00662016\n",
      "Epoch:  444\n",
      "Epoch: 0445, Training loss=0.00662015\n",
      "Epoch:  445\n",
      "Epoch: 0446, Training loss=0.00662013\n",
      "Epoch:  446\n",
      "Epoch: 0447, Training loss=0.00662011\n",
      "Epoch:  447\n",
      "Epoch: 0448, Training loss=0.00662011\n",
      "Epoch:  448\n",
      "Epoch: 0449, Training loss=0.00662011\n",
      "Epoch:  449\n",
      "Epoch: 0450, Training loss=0.00662009\n",
      "Epoch:  450\n",
      "Epoch: 0451, Training loss=0.00662007\n",
      "Epoch:  451\n",
      "Epoch: 0452, Training loss=0.00662007\n",
      "Epoch:  452\n",
      "Epoch: 0453, Training loss=0.00662006\n",
      "Epoch:  453\n",
      "Epoch: 0454, Training loss=0.00662005\n",
      "Epoch:  454\n",
      "Epoch: 0455, Training loss=0.00662004\n",
      "Epoch:  455\n",
      "Epoch: 0456, Training loss=0.00662001\n",
      "Epoch:  456\n",
      "Epoch: 0457, Training loss=0.00662000\n",
      "Epoch:  457\n",
      "Epoch: 0458, Training loss=0.00662001\n",
      "Epoch:  458\n",
      "Epoch: 0459, Training loss=0.00662000\n",
      "Epoch:  459\n",
      "Epoch: 0460, Training loss=0.00661997\n",
      "Epoch:  460\n",
      "Epoch: 0461, Training loss=0.00661995\n",
      "Epoch:  461\n",
      "Epoch: 0462, Training loss=0.00661996\n",
      "Epoch:  462\n",
      "Epoch: 0463, Training loss=0.00661996\n",
      "Epoch:  463\n",
      "Epoch: 0464, Training loss=0.00661993\n",
      "Epoch:  464\n",
      "Epoch: 0465, Training loss=0.00661991\n",
      "Epoch:  465\n",
      "Epoch: 0466, Training loss=0.00661991\n",
      "Epoch:  466\n",
      "Epoch: 0467, Training loss=0.00661990\n",
      "Epoch:  467\n",
      "Epoch: 0468, Training loss=0.00661989\n",
      "Epoch:  468\n",
      "Epoch: 0469, Training loss=0.00661988\n",
      "Epoch:  469\n",
      "Epoch: 0470, Training loss=0.00661987\n",
      "Epoch:  470\n",
      "Epoch: 0471, Training loss=0.00661985\n",
      "Epoch:  471\n",
      "Epoch: 0472, Training loss=0.00661985\n",
      "Epoch:  472\n",
      "Epoch: 0473, Training loss=0.00661984\n",
      "Epoch:  473\n",
      "Epoch: 0474, Training loss=0.00661984\n",
      "Epoch:  474\n",
      "Epoch: 0475, Training loss=0.00661981\n",
      "Epoch:  475\n",
      "Epoch: 0476, Training loss=0.00661980\n",
      "Epoch:  476\n",
      "Epoch: 0477, Training loss=0.00661981\n",
      "Epoch:  477\n",
      "Epoch: 0478, Training loss=0.00661981\n",
      "Epoch:  478\n",
      "Epoch: 0479, Training loss=0.00661978\n",
      "Epoch:  479\n",
      "Epoch: 0480, Training loss=0.00661976\n",
      "Epoch:  480\n",
      "Epoch: 0481, Training loss=0.00661976\n",
      "Epoch:  481\n",
      "Epoch: 0482, Training loss=0.00661977\n",
      "Epoch:  482\n",
      "Epoch: 0483, Training loss=0.00661975\n",
      "Epoch:  483\n",
      "Epoch: 0484, Training loss=0.00661973\n",
      "Epoch:  484\n",
      "Epoch: 0485, Training loss=0.00661972\n",
      "Epoch:  485\n",
      "Epoch: 0486, Training loss=0.00661972\n",
      "Epoch:  486\n",
      "Epoch: 0487, Training loss=0.00661972\n",
      "Epoch:  487\n",
      "Epoch: 0488, Training loss=0.00661969\n",
      "Epoch:  488\n",
      "Epoch: 0489, Training loss=0.00661968\n",
      "Epoch:  489\n",
      "Epoch: 0490, Training loss=0.00661969\n",
      "Epoch:  490\n",
      "Epoch: 0491, Training loss=0.00661968\n",
      "Epoch:  491\n",
      "Epoch: 0492, Training loss=0.00661966\n",
      "Epoch:  492\n",
      "Epoch: 0493, Training loss=0.00661964\n",
      "Epoch:  493\n",
      "Epoch: 0494, Training loss=0.00661965\n",
      "Epoch:  494\n",
      "Epoch: 0495, Training loss=0.00661964\n",
      "Epoch:  495\n",
      "Epoch: 0496, Training loss=0.00661962\n",
      "Epoch:  496\n",
      "Epoch: 0497, Training loss=0.00661961\n",
      "Epoch:  497\n",
      "Epoch: 0498, Training loss=0.00661961\n",
      "Epoch:  498\n",
      "Epoch: 0499, Training loss=0.00661961\n",
      "Epoch:  499\n",
      "Epoch: 0500, Training loss=0.00661959\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ',epoch)\n",
    "    for step,(batch_x,batch_y) in enumerate(trainDataLoader2):\n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "        # predict label\n",
    "        output = predictor(b_x)\n",
    "        # b_y=F.sigmoid(b_y) \n",
    "        \n",
    "        #print\n",
    "        #print(output)\n",
    "        #print(b_y)\n",
    "        # compute loss\n",
    "        loss = loss_func(output,b_y)\n",
    "        #loss = criterion(output, b_y)\n",
    "        \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train[epoch,0] = loss.item()  \n",
    "    print('Epoch: %04d, Training loss=%.8f' %\n",
    "          (epoch+1, loss.item())) \n",
    "\n",
    "# Save model\n",
    "torch.save(predictor.state_dict(), 'saved/models/DNN_GDSC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredict = predictor(testFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0947],\n",
       "        [0.0909],\n",
       "        [0.0905],\n",
       "        [0.0882],\n",
       "        [0.0971],\n",
       "        [0.0968],\n",
       "        [0.0915],\n",
       "        [0.0890],\n",
       "        [0.0951],\n",
       "        [0.0935],\n",
       "        [0.0942],\n",
       "        [0.0956],\n",
       "        [0.0957],\n",
       "        [0.0958],\n",
       "        [0.0922],\n",
       "        [0.0895],\n",
       "        [0.0947],\n",
       "        [0.0932],\n",
       "        [0.0993],\n",
       "        [0.0928],\n",
       "        [0.0904],\n",
       "        [0.0953],\n",
       "        [0.0948],\n",
       "        [0.0888],\n",
       "        [0.0931],\n",
       "        [0.0930],\n",
       "        [0.0930],\n",
       "        [0.0954],\n",
       "        [0.0939],\n",
       "        [0.0903],\n",
       "        [0.0904],\n",
       "        [0.0931],\n",
       "        [0.0941],\n",
       "        [0.0932],\n",
       "        [0.0948],\n",
       "        [0.0952],\n",
       "        [0.0941],\n",
       "        [0.0974],\n",
       "        [0.0950],\n",
       "        [0.0953],\n",
       "        [0.0947],\n",
       "        [0.0918],\n",
       "        [0.0938],\n",
       "        [0.0913],\n",
       "        [0.0951],\n",
       "        [0.0963],\n",
       "        [0.0942],\n",
       "        [0.0930],\n",
       "        [0.0973],\n",
       "        [0.0970],\n",
       "        [0.0981],\n",
       "        [0.0978],\n",
       "        [0.0933],\n",
       "        [0.0882],\n",
       "        [0.0923],\n",
       "        [0.0873],\n",
       "        [0.0973],\n",
       "        [0.0948],\n",
       "        [0.0940],\n",
       "        [0.0943],\n",
       "        [0.0906],\n",
       "        [0.0907],\n",
       "        [0.0967],\n",
       "        [0.0924],\n",
       "        [0.0968],\n",
       "        [0.0950],\n",
       "        [0.0939],\n",
       "        [0.0949],\n",
       "        [0.0969],\n",
       "        [0.0941],\n",
       "        [0.0922],\n",
       "        [0.0968],\n",
       "        [0.0953],\n",
       "        [0.0922],\n",
       "        [0.0932],\n",
       "        [0.0959],\n",
       "        [0.0927],\n",
       "        [0.0969],\n",
       "        [0.0983],\n",
       "        [0.0970],\n",
       "        [0.0967],\n",
       "        [0.0945],\n",
       "        [0.0937],\n",
       "        [0.0919],\n",
       "        [0.0964],\n",
       "        [0.0939],\n",
       "        [0.0970],\n",
       "        [0.0958],\n",
       "        [0.0983],\n",
       "        [0.0908],\n",
       "        [0.0997],\n",
       "        [0.0944],\n",
       "        [0.0963],\n",
       "        [0.0951],\n",
       "        [0.0996],\n",
       "        [0.0939],\n",
       "        [0.0954],\n",
       "        [0.0951],\n",
       "        [0.0908],\n",
       "        [0.0899],\n",
       "        [0.0904],\n",
       "        [0.0923],\n",
       "        [0.0954],\n",
       "        [0.0943],\n",
       "        [0.0956],\n",
       "        [0.0918],\n",
       "        [0.0966],\n",
       "        [0.0944],\n",
       "        [0.0962],\n",
       "        [0.0881],\n",
       "        [0.0956],\n",
       "        [0.0894],\n",
       "        [0.0914],\n",
       "        [0.0963],\n",
       "        [0.0940],\n",
       "        [0.0922],\n",
       "        [0.0997],\n",
       "        [0.0931],\n",
       "        [0.0951],\n",
       "        [0.0921],\n",
       "        [0.0914],\n",
       "        [0.0940],\n",
       "        [0.0941],\n",
       "        [0.0841],\n",
       "        [0.0898],\n",
       "        [0.0948],\n",
       "        [0.0922],\n",
       "        [0.0965],\n",
       "        [0.0942],\n",
       "        [0.0967],\n",
       "        [0.0932],\n",
       "        [0.0922],\n",
       "        [0.0948],\n",
       "        [0.0909],\n",
       "        [0.0913]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-720.0183535148651"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005350307933550169"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(testpredict.detach().cpu().numpy(),Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
